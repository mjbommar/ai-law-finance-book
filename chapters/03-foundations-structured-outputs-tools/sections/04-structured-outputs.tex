% =============================================================================
% Structured Outputs — Structured Outputs, Tools, Multimodal
% Purpose: JSON/CSV/XML with validation, schema design principles
% Label: sec:llmC-structured
% =============================================================================

\section{Structured Outputs with Validation}
\label{sec:llmC-structured}

Moving from conversational exploration to production applications requires reliable, machine-readable output. Where Chapter~2 (Conversations and Reasoning) focused on natural language interactions, this section addresses how to extract structured data from LLMs consistently and safely. We examine three layers of control—format, value, and semantic constraints—and show how thoughtful schema design transforms LLMs from text generators into reliable components of larger systems.

\subsection{Why Structure Matters: From Freeform to Reliable}

In exploratory or research contexts, natural language responses suffice. But production systems—contract analyzers, compliance monitors, risk assessments—require data that downstream code can parse, validate, and act upon without human intervention. Structured outputs address four critical requirements:

\begin{enumerate}
  \item \textbf{Deterministic parsing}: No guesswork about where fields start or end.
  \item \textbf{Type safety}: Dates are dates, numbers are numbers, enumerations match known values.
  \item \textbf{Automated validation}: Immediate detection of missing fields, out-of-range values, or inconsistent combinations.
  \item \textbf{Auditability}: Machine-readable logs enable compliance review and debugging.
\end{enumerate}

\noindent Consider a legal invoice classifier that must route bills to the correct cost center. A natural language response like ``This appears to be litigation-related, possibly high priority'' leaves too much ambiguity. A structured output with explicit fields—\texttt{category: "litigation"}, \texttt{priority: "high"}, \texttt{confidence: 0.92}—can trigger automated workflows and be logged for audit.

\subsubsection{Three Layers of Output Control}

Reliable structured outputs require coordinated constraints at multiple levels:

\begin{definitionbox}[title={Three Layers of Output Control}]
\begin{enumerate}
  \item \textbf{Format constraints}: Enforce syntactic structure (valid JSON, XML, or CSV). This ensures the output is parseable by standard libraries. Modern LLM APIs offer ``JSON mode'' or grammar-constrained generation to guarantee well-formed output.

  \item \textbf{Value constraints}: Enforce data types, required fields, allowed enumerations, and numeric ranges. These constraints ensure each field contains a value of the correct type and within acceptable bounds (e.g., confidence scores between 0.0 and 1.0).

  \item \textbf{Semantic constraints}: Enforce business rules and cross-field consistency. These application-specific rules ensure the output makes sense in context (e.g., if \texttt{category} is ``litigation'', then \texttt{matter\_id} must be present).
\end{enumerate}
\end{definitionbox}

\noindent All three layers are necessary. Format constraints prevent parsing failures. Value constraints catch invalid data early. Semantic constraints ensure the output satisfies domain-specific requirements. Missing any layer creates vulnerabilities: syntactically valid JSON might contain nonsense values, or type-safe data might violate business logic.

\subsection{Taxonomy Design Principles}

Many structured outputs rely on \keyterm{taxonomies}—controlled vocabularies that classify entities, intents, or relationships. In legal and financial contexts, taxonomies organize case types, transaction categories, regulatory regimes, risk factors, and more. Poor taxonomy design leads to ambiguous classifications, model confusion, and unreliable outputs.

\subsubsection{What Makes a Good Taxonomy}

Effective taxonomies balance coverage, clarity, and practical utility. Four principles guide design:

\begin{enumerate}
  \item \textbf{Orthogonal categories}: Each category represents a distinct concept with minimal overlap. If two categories frequently apply to the same item, they may be redundant or poorly defined.

  \item \textbf{Mutually exclusive options}: Each item should fit clearly into one category. Overlapping categories force arbitrary choices and reduce consistency. When multiple labels are valid, consider a multi-label design rather than forcing mutual exclusivity.

  \item \textbf{Exhaustive coverage}: Every item should fit somewhere. Include an ``other'' or ``unclassified'' category to handle edge cases without breaking the system. Track usage of fallback categories to identify gaps in the taxonomy.

  \item \textbf{Clear descriptions}: Category labels alone are insufficient. Each category needs a description explaining its scope, boundary cases, and distinguishing features. LLMs read these descriptions as guidance.
\end{enumerate}

\begin{keybox}[title={Taxonomy Design Checklist}]
\textbf{Before deploying a taxonomy, verify:}
\begin{itemize}
  \item Each category has a clear, unambiguous description.
  \item Boundary cases are explicitly addressed (``X belongs to A, not B because...'').
  \item Categories do not overlap (if they do, document the priority rule).
  \item An ``other'' or ``unknown'' category exists for uncovered cases.
  \item Examples illustrate each category with typical and edge cases.
\end{itemize}
\end{keybox}

\subsubsection{Labels Need Context: Always Include Descriptions}

A common mistake is defining taxonomies as bare enumerations without explanatory text. Consider an intent classification system for legal intake calls:

\begin{highlightbox}
\textbf{Bad taxonomy (labels only):}
\begin{itemize}
  \item \texttt{NEW\_MATTER}
  \item \texttt{EXISTING\_MATTER}
  \item \texttt{BILLING}
  \item \texttt{GENERAL}
\end{itemize}

\textbf{Problem:} Is a billing question about an existing matter \texttt{EXISTING\_MATTER} or \texttt{BILLING}? The LLM must guess.
\end{highlightbox}

\noindent A better approach includes explicit descriptions:

\begin{highlightbox}
\textbf{Good taxonomy (with descriptions):}
\begin{itemize}
  \item \texttt{NEW\_MATTER}: Caller requesting to open a new legal matter or engagement. No prior matter exists.
  \item \texttt{EXISTING\_MATTER}: Caller has a question or update about an active or closed matter. Use this even if the question is about billing for an existing matter.
  \item \texttt{BILLING}: Caller has a billing question \textbf{not} tied to a specific matter (e.g., general payment policies, account setup).
  \item \texttt{GENERAL}: Caller has a question that does not fit the above categories (e.g., office hours, firm background).
\end{itemize}

\textbf{Improvement:} Clear boundaries, explicit precedence (existing matter takes priority over billing), and guidance on edge cases.
\end{highlightbox}

\noindent The descriptions become part of the prompt or schema documentation. LLMs perform better when they understand the intent behind each category, not just the label.

\subsection{Schemas as LLM Documentation}

A profound shift in thinking occurs when we recognize that \textbf{LLMs read your schema}. In traditional software, schemas primarily serve validation libraries and human developers. With LLMs, the schema itself becomes a form of prompt: field names, descriptions, and examples guide the model's output generation.

\subsubsection{Field Names Are Instructions}

Descriptive field names improve LLM accuracy. Compare:

\begin{itemize}
  \item \textbf{Cryptic}: \texttt{t}, \texttt{p}, \texttt{c}
  \item \textbf{Descriptive}: \texttt{intent}, \texttt{priority}, \texttt{confidence}
\end{itemize}

\noindent The second set communicates intent. Even without additional documentation, the LLM can infer what belongs in each field. Short, cryptic names save a few characters but cost clarity.

\subsubsection{Descriptions Are Prompts}

Most schema languages support field descriptions (docstrings in Python, \texttt{description} in JSON Schema). These are not optional nice-to-haves—they are instructions to the LLM. Write them carefully:

\begin{itemize}
  \item \textbf{Bad}: ``confidence value''
  \item \textbf{Good}: ``Model's confidence in the classification, from 0.0 (no confidence) to 1.0 (complete certainty). Use 0.5 or lower if uncertain.''
\end{itemize}

\noindent The second version clarifies range, semantics, and guidance for uncertain cases.

\subsubsection{Examples Guide Format Expectations}

Including example values in schema definitions or prompts helps the LLM match expected formats. For dates, show \texttt{2025-03-15} rather than just stating ``ISO 8601 date.'' For identifiers, show \texttt{MAT-2025-001234} rather than ``matter ID.'' Concrete examples reduce ambiguity.

\subsubsection{Constraints Communicate Valid Ranges}

Schema constraints (minimum/maximum values, string patterns, array lengths) serve dual purposes: they validate output and signal expectations to the LLM. A field constrained to \texttt{ge=0.0, le=1.0} tells the model the valid range. A field with \texttt{pattern="[A-Z]\{3\}-\textbackslash d\{4\}"} shows the expected identifier format.

\begin{highlightbox}
\textbf{Key Insight:} Every element of your schema—names, descriptions, examples, constraints—is read by the LLM during generation. Design schemas as if they are part of your prompt, because they are.
\end{highlightbox}

\subsection{Good vs. Bad Schema Examples}

To illustrate these principles, consider a classification task for legal documents. We compare a poorly designed schema with a well-designed alternative.

\subsubsection{Bad Schema: Minimal Information}

\begin{lstlisting}[language=Python, caption={Bad Schema Example: Cryptic Names and No Descriptions}]
from pydantic import BaseModel
from enum import Enum

class BadIntent(str, Enum):
    NM = "NM"
    EM = "EM"
    BL = "BL"
    GN = "GN"

class BadPriority(str, Enum):
    H = "H"
    M = "M"
    L = "L"

class BadClassification(BaseModel):
    t: BadIntent
    p: BadPriority
    c: float
\end{lstlisting}

\textbf{Problems:}
\begin{itemize}
  \item Field names (\texttt{t}, \texttt{p}, \texttt{c}) are cryptic.
  \item Enum values (\texttt{NM}, \texttt{H}) are abbreviations without context.
  \item No descriptions explaining what each field represents.
  \item No constraints on \texttt{c}—could be negative, greater than 1, or \texttt{NaN}.
  \item No guidance on when to use each intent or priority.
\end{itemize}

\noindent An LLM faced with this schema must infer semantics from surrounding prompt text, increasing error rates.

\subsubsection{Good Schema: Descriptive and Constrained}

\begin{lstlisting}[language=Python, caption={Good Schema Example: Descriptive Names, Constraints, and Documentation}]
from pydantic import BaseModel, Field
from enum import Enum

class Intent(str, Enum):
    """Classify the caller's intent based on conversation content."""
    NEW_MATTER = "new_matter"
    # Caller wants to open a new legal matter
    EXISTING_MATTER = "existing_matter"
    # Caller has questions about an active or closed matter
    BILLING = "billing"
    # Billing question not tied to a specific matter
    GENERAL = "general"
    # General inquiry not fitting other categories

class Priority(str, Enum):
    """Urgency level for follow-up action."""
    HIGH = "high"
    # Requires response within 24 hours
    MEDIUM = "medium"
    # Requires response within 3 business days
    LOW = "low"
    # Routine inquiry, respond within 1 week

class DocumentClassification(BaseModel):
    """Structured classification output for legal intake calls."""

    intent: Intent = Field(
        description=(
            "Primary intent of the call. Choose the most specific "
            "category. If a call is about billing for an existing "
            "matter, use EXISTING_MATTER."
        )
    )

    priority: Priority = Field(
        description=(
            "Urgency level based on client need and topic. "
            "Use HIGH for time-sensitive legal issues, MEDIUM for "
            "standard requests, LOW for informational queries."
        )
    )

    confidence: float = Field(
        ge=0.0,
        le=1.0,
        description=(
            "Model's confidence in this classification, from 0.0 "
            "(no confidence) to 1.0 (complete certainty). Use 0.5 "
            "or lower if the conversation is ambiguous."
        )
    )

    reasoning: str = Field(
        description=(
            "Brief explanation of why this classification was chosen. "
            "Reference specific statements from the conversation."
        )
    )
\end{lstlisting}

\textbf{Improvements:}
\begin{itemize}
  \item Descriptive field names (\texttt{intent}, \texttt{priority}, \texttt{confidence}, \texttt{reasoning}).
  \item Clear enum values (\texttt{NEW\_MATTER}, \texttt{HIGH}) with inline comments.
  \item Rich field descriptions explaining semantics, boundaries, and edge cases.
  \item Explicit constraints on \texttt{confidence} (\texttt{ge=0.0, le=1.0}).
  \item A \texttt{reasoning} field for transparency and debugging.
\end{itemize}

\noindent This schema serves as both validation specification and LLM guidance. The model receives clear instructions on what each field means and how to choose values.

\subsection{Dynamic Taxonomies}

Static taxonomies work well for stable domains, but legal and financial systems evolve. New matter types emerge, regulatory categories change, and business lines expand. \keyterm{Dynamic taxonomies} adapt to these changes by generating allowed values at runtime.

\subsubsection{When Taxonomies Need to Change at Runtime}

Consider a matter classification system. A law firm's matter types depend on practice areas, which may shift as the firm grows or pivots. Hardcoding matter types in the schema creates maintenance burden and deployment friction. Instead, load matter types from a database or configuration file and generate the enum dynamically:

\begin{lstlisting}[language=Python, caption={Dynamic Taxonomy from Database}]
from pydantic import BaseModel, Field
from enum import Enum
from typing import Type

def create_matter_type_enum(db_conn) -> Type[Enum]:
    """Fetch active matter types from database and create enum."""
    rows = db_conn.execute(
        "SELECT code, description FROM matter_types WHERE active = 1"
    ).fetchall()

    # Build enum dynamically
    enum_dict = {row["code"]: row["code"] for row in rows}
    MatterType = Enum("MatterType", enum_dict)

    # Attach descriptions for LLM context
    for row in rows:
        setattr(
            MatterType[row["code"]],
            "__doc__",
            row["description"]
        )

    return MatterType

# At runtime
db = connect_to_database()
MatterType = create_matter_type_enum(db)

class MatterClassification(BaseModel):
    matter_type: MatterType = Field(
        description="Type of legal matter based on practice area"
    )
\end{lstlisting}

\noindent This approach decouples schema definition from business data. When a new practice area launches, no code changes are required—just update the database.

\subsubsection{Versioning Taxonomies}

Dynamic taxonomies introduce versioning challenges. If the set of allowed values changes between classification and audit review, logged outputs may reference categories that no longer exist. To address this:

\begin{itemize}
  \item \textbf{Record taxonomy version}: Include a \texttt{taxonomy\_version} field in each output, referencing the schema version used.
  \item \textbf{Preserve historical definitions}: Archive old taxonomy definitions so auditors can interpret past classifications.
  \item \textbf{Handle deprecated categories}: When a category is removed, mark it as deprecated rather than deleting it. Map deprecated categories to current equivalents for backward compatibility.
\end{itemize}

\noindent Chapter~5 (Prompt Design, Evaluation, and Optimization) addresses schema versioning and migration strategies in detail, including strategies for A/B testing schema changes and rolling back incompatible updates.

\subsubsection{Handling Taxonomy Drift}

Over time, the distribution of observed categories may drift. A matter type that was rare becomes common, or vice versa. Monitor classification distributions to detect drift:

\begin{lstlisting}[language=Python, caption={Monitoring Taxonomy Distribution}]
from collections import Counter
import datetime

def log_classification(result, db_conn):
    """Log classification result for distribution monitoring."""
    db_conn.execute(
        """
        INSERT INTO classification_log
        (timestamp, matter_type, confidence, taxonomy_version)
        VALUES (?, ?, ?, ?)
        """,
        (
            datetime.datetime.utcnow(),
            result.matter_type.value,
            result.confidence,
            result.taxonomy_version
        )
    )

def check_distribution_drift(db_conn, days=30):
    """Compare recent distribution to historical baseline."""
    recent = db_conn.execute(
        """
        SELECT matter_type, COUNT(*) as cnt
        FROM classification_log
        WHERE timestamp > datetime('now', '-' || ? || ' days')
        GROUP BY matter_type
        """,
        (days,)
    ).fetchall()

    baseline = db_conn.execute(
        """
        SELECT matter_type, COUNT(*) as cnt
        FROM classification_log
        WHERE timestamp <= datetime('now', '-' || ? || ' days')
        GROUP BY matter_type
        """,
        (days,)
    ).fetchall()

    # Compare distributions (simplified)
    recent_dist = {r["matter_type"]: r["cnt"] for r in recent}
    baseline_dist = {r["matter_type"]: r["cnt"] for r in baseline}

    # Flag categories with >50% change in frequency
    for cat in set(recent_dist.keys()) | set(baseline_dist.keys()):
        recent_pct = recent_dist.get(cat, 0) / sum(recent_dist.values())
        baseline_pct = baseline_dist.get(cat, 0) / sum(baseline_dist.values())
        if abs(recent_pct - baseline_pct) > 0.5 * baseline_pct:
            print(f"Drift detected in {cat}: {baseline_pct:.2%} -> {recent_pct:.2%}")
\end{lstlisting}

\noindent Drift may indicate genuine changes in business activity or signal problems with the taxonomy (e.g., a category has become too broad and needs subdivision).

\subsection{JSON vs. XML vs. CSV: Format Tradeoffs}

Most modern LLM applications use JSON for structured output, but legacy systems and specific regulatory requirements may mandate XML or CSV. Understanding the tradeoffs helps you choose appropriately.

\subsubsection{JSON: Concise and Application-Native}

\textbf{Advantages:}
\begin{itemize}
  \item Concise syntax with low overhead.
  \item Native support in JavaScript, Python, and most modern languages.
  \item Flexible nesting for complex hierarchies.
  \item Wide adoption in REST APIs and web applications.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item No standardized schema language (though JSON Schema is widely used).
  \item Limited support for comments (not part of the JSON spec).
  \item No namespace support for merging data from multiple sources.
\end{itemize}

\noindent JSON is the default choice unless external constraints dictate otherwise.

\subsubsection{XML: Legacy Compatibility and Schema Rigor}

\textbf{Advantages:}
\begin{itemize}
  \item Strong schema support (XSD) with validation and documentation.
  \item Namespace support for integrating multiple vocabularies.
  \item Wide adoption in enterprise systems and government data exchange (e.g., XBRL for financial reporting).
  \item Support for comments and metadata.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item Verbose syntax increases token usage and parsing overhead.
  \item More complex for LLMs to generate correctly (closing tags, attribute vs. element choices).
  \item Less natural mapping to modern programming language objects.
\end{itemize}

\noindent Use XML when integrating with legacy systems or when regulatory frameworks require it (e.g., court e-filing systems, financial data standards).

\subsubsection{CSV: Compact but Fragile}

\textbf{Advantages:}
\begin{itemize}
  \item Extremely compact for tabular data.
  \item Universal support (spreadsheets, databases, data science tools).
  \item Low token usage.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item No standard for nested or hierarchical data.
  \item Ambiguous escaping rules (different tools handle quotes and delimiters differently).
  \item No type information (everything is a string; downstream code must parse).
  \item Fragile in the presence of special characters (commas, quotes, newlines).
\end{itemize}

\noindent CSV works well for simple, flat data (e.g., lists of transactions with fixed fields) but becomes unwieldy for complex structures. Prefer JSON or XML for nested data.

\subsection{Prompt Schemas vs. Function Calling}

LLM APIs offer two primary mechanisms for structured output: \keyterm{prompt schemas} (asking the model to produce JSON/XML in its text response) and \keyterm{function calling} (API-level enforcement of structured output). Each has tradeoffs.

\subsubsection{Prompt Schemas: Flexible but Manual}

In this approach, you include the schema definition in the prompt and ask the model to return JSON matching that schema. For example:

\begin{highlightbox}
\textbf{Prompt:}

``Classify the following legal intake call. Return a JSON object with these fields:
\begin{itemize}
  \item \texttt{intent}: one of ``new\_matter'', ``existing\_matter'', ``billing'', ``general''
  \item \texttt{priority}: one of ``high'', ``medium'', ``low''
  \item \texttt{confidence}: float between 0.0 and 1.0
  \item \texttt{reasoning}: string explaining your classification
\end{itemize}

Call transcript: [...]''
\end{highlightbox}

\textbf{Advantages:}
\begin{itemize}
  \item Works with any LLM (no API-specific features required).
  \item Full control over schema representation and explanation.
  \item Easy to prototype and iterate.
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item No guarantee of syntactically valid JSON (the model may produce malformed output).
  \item No automatic validation (you must parse and check manually).
  \item Token overhead (schema is part of the prompt).
  \item Less auditability (harder to log exactly what schema was used).
\end{itemize}

\subsubsection{Function Calling: Enforced Structure}

Modern LLM APIs (OpenAI, Anthropic, Google) support \keyterm{function calling} or \keyterm{tool use}, where you define a schema using JSON Schema or a similar format, and the API guarantees syntactically valid output matching that schema. The LLM generates a function call with arguments conforming to your specification, which the API parses and returns as structured data.

\textbf{Advantages:}
\begin{itemize}
  \item Guaranteed syntactic validity (no parsing errors).
  \item Automatic type checking and constraint enforcement (depending on API).
  \item Clearer separation between schema and prompt.
  \item Better auditability (schema definition is explicit and versioned).
\end{itemize}

\textbf{Disadvantages:}
\begin{itemize}
  \item API-specific (not all models support it; implementations vary).
  \item Less flexibility (you must work within the API's schema format).
  \item Potential for API changes breaking your code.
\end{itemize}

\subsubsection{When to Use Each}

\begin{itemize}
  \item \textbf{Prompt schemas}: Use for quick prototypes, exploratory analysis, or when working with LLMs that do not support function calling. Plan to migrate to function calling for production.

  \item \textbf{Function calling}: Use for production systems where reliability and auditability are critical. Accept the API lock-in as a tradeoff for guaranteed structure.
\end{itemize}

\noindent In practice, many systems start with prompt schemas during development and migrate to function calling once the schema stabilizes. Chapter~5 (Prompt Design, Evaluation, and Optimization) discusses evaluation strategies for validating schema changes before deployment.

\subsection{Validation, Retries, and Error Handling}

Even with well-designed schemas and function calling, LLMs occasionally produce invalid or nonsensical outputs. Robust systems validate outputs, retry on failure, and log errors for analysis.

\subsubsection{Multi-Level Validation}

Validate at all three layers:

\begin{enumerate}
  \item \textbf{Format validation}: Ensure the output is syntactically correct JSON/XML. Most libraries (e.g., \texttt{json.loads} in Python) raise exceptions on malformed input.

  \item \textbf{Schema validation}: Ensure the output matches the expected structure (required fields present, correct types, constraints satisfied). Use validation libraries like Pydantic (Python), Joi (JavaScript), or JSON Schema validators.

  \item \textbf{Semantic validation}: Ensure the output makes sense in context. For example, if \texttt{intent} is \texttt{EXISTING\_MATTER}, verify that \texttt{matter\_id} is present and valid.
\end{enumerate}

\begin{lstlisting}[language=Python, caption={Multi-Level Validation Example}]
import json
from pydantic import ValidationError

def validate_output(raw_output, schema_class, semantic_validator=None):
    """Validate LLM output at all three layers."""
    # Layer 1: Format validation
    try:
        parsed = json.loads(raw_output)
    except json.JSONDecodeError as e:
        return None, f"Invalid JSON: {e}"

    # Layer 2: Schema validation
    try:
        validated = schema_class(**parsed)
    except ValidationError as e:
        return None, f"Schema violation: {e}"

    # Layer 3: Semantic validation (optional)
    if semantic_validator:
        semantic_errors = semantic_validator(validated)
        if semantic_errors:
            return None, f"Semantic errors: {semantic_errors}"

    return validated, None

# Example semantic validator
def check_matter_consistency(classification):
    """Ensure matter_id is present if intent is EXISTING_MATTER."""
    if (classification.intent == Intent.EXISTING_MATTER and
        not hasattr(classification, "matter_id")):
        return ["EXISTING_MATTER requires matter_id field"]
    return []
\end{lstlisting}

\subsubsection{Retry Strategies}

When validation fails, retrying with additional context often succeeds. Effective retry strategies:

\begin{itemize}
  \item \textbf{Include error message}: Tell the LLM what went wrong. ``Your previous output was invalid JSON. Please return a valid JSON object.''

  \item \textbf{Show example}: Provide a valid example matching the schema. ``Expected format: \texttt{\{...\}}''

  \item \textbf{Limit retries}: Retry at most 2-3 times to avoid wasting tokens and time. After repeated failures, escalate to human review or use a fallback.

  \item \textbf{Log failures}: Record all validation failures for analysis. Patterns in failures may reveal schema problems or model limitations.
\end{itemize}

\begin{lstlisting}[language=Python, caption={Retry with Error Feedback}]
def classify_with_retry(prompt, schema_class, max_retries=3):
    """Attempt classification with validation and retry."""
    for attempt in range(max_retries):
        response = call_llm(prompt)
        validated, error = validate_output(
            response,
            schema_class,
            check_matter_consistency
        )

        if validated:
            return validated

        # Retry with error feedback
        prompt = (
            f"{prompt}\n\n"
            f"Your previous attempt failed validation with error: {error}\n"
            f"Please try again, ensuring your output matches the schema exactly."
        )

    # Max retries exhausted
    raise ValueError(f"Classification failed after {max_retries} attempts")
\end{lstlisting}

\subsubsection{Schema Migrations and Versioning}

As schemas evolve, you must handle backward compatibility and migration. Strategies include:

\begin{itemize}
  \item \textbf{Version fields}: Include a \texttt{schema\_version} field in every output. This allows downstream code to handle multiple schema versions gracefully.

  \item \textbf{Deprecation periods}: When removing a field, mark it as deprecated for several releases before removal. Log warnings when deprecated fields are used.

  \item \textbf{Additive changes}: Prefer adding optional fields over removing or renaming required fields. Additive changes are backward compatible.

  \item \textbf{Migration scripts}: Provide scripts to convert old outputs to new formats. Store these scripts alongside schema definitions for auditability.
\end{itemize}

\noindent Chapter~5 (Prompt Design, Evaluation, and Optimization) covers schema versioning in the context of prompt evaluation and A/B testing, including strategies for validating schema changes before rollout.

\subsection{Locale, Time, and Numeric Formatting}

Legal and financial systems operate across jurisdictions with different conventions for dates, times, currencies, and numeric formats. Ambiguity in these formats causes costly errors (e.g., interpreting ``03/04/2025'' as March 4 vs. April 3).

\subsubsection{Normalize Dates and Times}

\textbf{Always use ISO 8601} for dates and timestamps:
\begin{itemize}
  \item Dates: \texttt{2025-03-15}
  \item Timestamps: \texttt{2025-03-15T14:30:00Z} (UTC) or \texttt{2025-03-15T09:30:00-05:00} (with offset)
\end{itemize}

\noindent Include explicit timezone information. Do not rely on local time or implied timezones. For legal deadlines, specify the relevant jurisdiction's timezone (e.g., ``court closes at 5:00 PM Eastern Time'').

\subsubsection{Explicit Currency and Unit Specification}

Do not write ``100'' without units. Write ``USD 100.00'' or ``EUR 100.00''. Use ISO 4217 currency codes. For other units, include the unit explicitly (``10 km'', ``5 kg'', ``3.5 hours'').

\subsubsection{Jurisdiction and Effective Dates in Evidence}

When extracting legal or regulatory information, require:
\begin{itemize}
  \item \textbf{Jurisdiction}: Which legal system or regulatory regime applies? (``Delaware corporate law'', ``SEC Rule 10b-5'', ``GDPR Article 6'')

  \item \textbf{Effective date}: When did this rule or fact become effective? When does it expire? (``Effective 2025-01-01'', ``Amended 2024-07-15'')
\end{itemize}

\noindent Including these fields in structured outputs ensures that downstream systems can apply the correct rules and avoid relying on outdated information.

\subsection{Streaming and Partial Outputs}

Some applications require real-time feedback as the LLM generates output. Streaming APIs return tokens incrementally, allowing you to display partial results or abort generation early. Structured outputs complicate streaming because partial JSON may not be valid.

\subsubsection{Buffering and Incremental Validation}

One approach: buffer tokens until a complete object is available, then validate. For JSON, this means waiting until the closing brace of the root object. Some libraries (e.g., \texttt{ijson} in Python) support incremental JSON parsing, allowing you to process fields as they arrive.

\subsubsection{End-of-Object Markers}

For simple cases, ask the LLM to emit a special marker (e.g., \texttt{<END>}) after each complete object. Parse and validate each object independently:

\begin{lstlisting}[language=Python, caption={Streaming with End-of-Object Markers}]
def stream_classifications(prompt):
    """Stream multiple classifications with <END> markers."""
    buffer = ""
    for token in stream_llm(prompt):
        buffer += token
        if "<END>" in buffer:
            # Extract and validate object
            obj_text = buffer.split("<END>")[0]
            buffer = buffer.split("<END>", 1)[1]

            validated, error = validate_output(obj_text, MatterClassification)
            if validated:
                yield validated
            else:
                print(f"Validation error: {error}")
\end{lstlisting}

\subsubsection{Chunked JSON Strategies}

For more complex cases, consider chunked JSON formats like JSON Lines (JSONL), where each line is a complete JSON object. This format is streaming-friendly and widely supported. The LLM emits one JSON object per line, and you validate each line independently.

\begin{cautionbox}[title={Streaming Complexity}]
Streaming structured outputs adds complexity and may reduce reliability. Use streaming only when real-time feedback is essential. For batch processing, prefer waiting for the complete output before validation.
\end{cautionbox}

\subsection{Forward References}

Schema design interacts with many other topics in this book:

\begin{itemize}
  \item \textbf{Chapter~5 (Prompt Design, Evaluation, and Optimization)}:
  Discusses schema versioning, A/B testing schema changes, and evaluating classification accuracy.

  \item \textbf{Chapters~6--7 (Agents)}:
  The Governance question addresses how schema design enables agentic auditability and transparency---structured outputs become the audit trail for automated decisions.
\end{itemize}

\subsection{Summary}

Structured outputs transform LLMs from conversational assistants into reliable components of production systems. Success requires:

\begin{enumerate}
  \item \textbf{Three-layer validation}: Format, value, and semantic constraints work together to ensure reliable outputs.

  \item \textbf{Thoughtful taxonomy design}: Orthogonal categories, clear descriptions, and explicit boundary handling improve classification accuracy and consistency.

  \item \textbf{Schemas as documentation}: Field names, descriptions, and constraints guide LLM output generation—write them as carefully as you write prompts.

  \item \textbf{Dynamic taxonomies}: Load allowed values from configuration or databases to reduce coupling and support evolving business needs.

  \item \textbf{Validation and retries}: Even well-designed schemas fail occasionally. Validate outputs, retry with error feedback, and log failures for analysis.

  \item \textbf{Normalization and explicitness}: Use ISO 8601 dates, explicit currency codes, and jurisdiction/effective date fields to eliminate ambiguity.
\end{enumerate}

\noindent With these practices, structured outputs become a foundation for automated workflows, compliance monitoring, and auditability in legal and financial applications.

