% =============================================================================
% Reasoning Patterns — Conversations & Reasoning
% Purpose: CoT, self-consistency, ReAct, ToT/GoT, few-shot, reflection
% Label: sec:llmB-reason
% =============================================================================

\section{Reasoning Patterns and When to Use Them}
\label{sec:llmB-reason}

Not every query to an AI is simple. Some questions require step-by-step reasoning, intermediate calculations, or access to external tools and knowledge sources. In this section, we cover different prompting strategies that help the model break down complex tasks or improve its accuracy on challenging problems.

We start with relatively lightweight techniques and progress to more elaborate ones, discussing when each is appropriate. The guiding principle is: \emph{use the simplest strategy that gets the job done reliably}. Overly complex prompting can waste time or even confuse the model if the task didn't need it. On the other hand, a task that requires reasoning or multiple steps will benefit greatly from these patterns.

\Cref{fig:llmB-reasoning-patterns} provides an overview of the reasoning pattern hierarchy, from simple direct prompting to complex graph-based exploration.

\begin{figure}[htbp]
  \centering
  \resizebox{0.9\textwidth}{!}{\input{figures/fig-reasoning-patterns}}
  \caption{The hierarchy of reasoning patterns, from simple (top) to complex (bottom). Each level adds capability at the cost of increased latency and token usage. Select the simplest pattern that meets your accuracy requirements.}
  \label{fig:llmB-reasoning-patterns}
\end{figure}

\subsection{Chain-of-Thought Prompting}
\label{sec:llmB-cot}

One powerful idea that has emerged is to prompt the model to produce a \keyterm{chain of thought}---a series of intermediate reasoning steps---before giving the final answer \parencite{wei2022cot}. Think of this like showing your work in a math problem. Instead of asking directly ``What is the result?'', you ask the model to reason out loud and then conclude.

\subsubsection{Mechanism and Theoretical Basis}

The theoretical underpinning of Chain-of-Thought (CoT) is that it decouples the reasoning process from answer generation. In a standard prompt, the model must compute the logic and the output token simultaneously. In CoT, the intermediate tokens serve as a ``scratchpad,'' allowing the model to dump its working memory into the context window. This effectively expands the computational depth of the model, as the attention mechanism can now attend to the intermediate steps it just generated to inform the next step.

\begin{definitionbox}[title={Chain-of-Thought Prompting}]
\keyterm{Chain-of-Thought (CoT)} prompting encourages the model to generate intermediate reasoning steps before arriving at a final answer. This decomposes complex problems into simpler sub-problems, improving accuracy on tasks requiring multi-step reasoning.
\end{definitionbox}

Research by Wei et al.\ (2022) showed that providing a few examples of this step-by-step approach in the prompt can significantly improve model performance on complex tasks \parencite{wei2022cot}. Simply adding examples where the model was guided to articulate intermediate steps---rather than jumping directly to the answer---made a substantial difference in solving arithmetic word problems, logic puzzles, and commonsense questions.

The improvement was dramatic: a large model (540 billion parameters) with chain-of-thought prompting solved math word problems at state-of-the-art levels, whereas it struggled if only given the question with no reasoning steps. For tasks like arithmetic (GSM8K) or symbolic reasoning, standard prompting yields a relatively flat scaling curve---adding more parameters doesn't significantly help. With CoT, performance scales log-linearly with model size, unlocking ``emergent'' capabilities that smaller models simply cannot perform.

\paragraph{Example: Legal Analysis.} Consider a question about contract liability:

\begin{quote}
\textbf{Direct prompting}: ``Is the seller liable for the defective goods under this contract?'' \\
\textbf{Model response}: ``Yes.'' (potentially incorrect, no reasoning visible)
\end{quote}

\begin{quote}
\textbf{Chain-of-thought prompting}: ``Analyze the contract liability step by step. First, identify the relevant warranty provisions. Second, determine whether the defect was discoverable at delivery. Third, assess whether proper notice was given. Finally, conclude on liability.'' \\
\textbf{Model response}: ``Step 1: The contract contains an express warranty in Section 4.2... Step 2: The defect in the heating element was latent and not discoverable... Step 3: Notice was provided within 10 days as required... Conclusion: The seller is likely liable under the express warranty.''
\end{quote}

The chain-of-thought version forces the model to consider each element systematically, reducing the chance of overlooking a critical factor.

\subsubsection{Private Scratchpads vs.\ User-Facing Rationales}
\label{sec:llmB-scratchpad}

A critical design decision in CoT implementation is the \emph{visibility} of the reasoning trace:

\begin{itemize}
  \item \textbf{User-facing rationales}: The reasoning is displayed to the user. This builds trust (explainability) and allows the user to verify the logic. However, it can be verbose and distracting for simple queries.

  \item \textbf{Private scratchpads}: The reasoning is generated in a hidden block (parsed out by the application before display) or within a strictly internal ``thought'' loop. The user sees only the final answer.
\end{itemize}

\begin{keybox}[title={When to Keep Reasoning Private}]
Keep reasoning traces private when:
\begin{itemize}
  \item The reasoning might explore harmful or sensitive considerations before reaching a safe conclusion
  \item The user interface should remain clean and simple
  \item The reasoning is for system debugging, not user consumption
  \item You want to maintain auditability without cluttering the user experience
\end{itemize}
Make reasoning visible when:
\begin{itemize}
  \item Explainability is a requirement (regulatory, trust-building)
  \item Users need to verify the logic (legal, medical, financial analysis)
  \item The reasoning process itself is educational
\end{itemize}
\end{keybox}

Keeping reasoning traces private offers distinct advantages for safety and simplicity. It allows the model to ``think'' about sensitive topics or explore dead ends without exposing the user to confusing or potentially unsafe intermediate thoughts. Furthermore, it allows for a cleaner user interface where only the concise, final answer is presented, while the detailed log is retained for auditability.

\subsubsection{The Cost of Reasoning: Token Overhead}

The primary trade-off of CoT is latency and cost. Generating a rationale requires significantly more tokens than a direct answer. Analysis suggests that for complex math datasets, CoT can increase token usage by approximately 19--40\% compared to direct answering \parencite{xu2025cod}.

This ``inference compute'' is often a necessary investment; without it, accuracy on complex tasks drops dramatically. However, new techniques attempt to mitigate this overhead:

\begin{itemize}
  \item \keyterm{Chain of Draft (CoD)}: Encourages the model to generate concise, shorthand reasoning (e.g., ``5*5=25, +5=30'') rather than verbose natural language explanations. Research indicates CoD can achieve similar accuracy to full CoT while using up to 40\% fewer tokens \parencite{xu2025cod}.

  \item \keyterm{TokenSkip}: Controllable compression of reasoning traces, pruning redundant tokens while preserving the logical structure \parencite{xu2025tokenskip}.
\end{itemize}

\paragraph{Practical Guidance.} Chain-of-thought is most useful in tasks like:
\begin{itemize}
  \item Multi-step mathematics and calculations
  \item Logical reasoning and puzzle solving
  \item Complex analysis requiring consideration of multiple factors
  \item Any scenario where the model might otherwise ``jump to a conclusion''
\end{itemize}

One caution: CoT tends to help more with larger models. Smaller LLMs (under a few billion parameters) often don't benefit and can even get confused by the additional verbosity. Always verify if it actually improves results for your particular case.

\subsection{Self-Consistency: Taking a Majority Vote}
\label{sec:llmB-selfconsistency}

Chain-of-thought can be taken a step further. What if the model's reasoning process could be run multiple times to see if it arrives at the same answer consistently? This is the idea behind \keyterm{self-consistency} decoding \parencite{wang2023selfconsistency}.

\subsubsection{Operational Mechanics}

Instead of prompting the model once, you prompt it multiple times (say 5 or 10 times) with some randomness injected so it might come up with different reasoning each time. You then examine all the answers it gave. Each chain-of-thought will lead to an answer, and you select the answer that appears most frequently among the outputs via \keyterm{majority voting}.

\begin{definitionbox}[title={Self-Consistency Decoding}]
\keyterm{Self-consistency} generates $k$ independent reasoning chains (samples) for the same query, typically with non-zero temperature ($T \approx 0.7$) to ensure diversity. The final answer is selected via majority voting---marginalizing out the reasoning paths to find the most consistent final answer.
\end{definitionbox}

The intuition is that a complex question might have multiple plausible solution paths, but they should converge to the same correct answer. Incorrect reasoning, on the other hand, might produce a variety of wrong answers. By sampling several reasoning paths, we increase the chance of hitting the correct line of reasoning at least once. Then by taking a majority vote (if one answer appears in 6 out of 10 samples, for example), we eliminate outliers.

\subsubsection{Efficacy vs.\ Compute}

Self-consistency yields dramatic improvements. On the GSM8K benchmark, applying self-consistency can boost accuracy by over 17 percentage points compared to a single CoT path \parencite{wang2023selfconsistency}. For math word problems and commonsense questions, accuracy jumped significantly---in some cases by 10--20 percentage points.

However, the cost is linear with the number of samples ($k$). Running 10 samples increases inference costs and latency by 10$\times$. This creates a sharp accuracy-cost trade-off.

\begin{keybox}[title={Self-Consistency Budgeting}]
\begin{itemize}
  \item \textbf{Sweet spot}: Research suggests 5--10 samples often provide most of the benefit
  \item \textbf{Diminishing returns}: Going from 1 to 5 samples hugely improves reliability; going to 30 samples may not be much better than 10
  \item \textbf{Use selectively}: Reserve for high-stakes queries where the cost is justified
\end{itemize}
\end{keybox}

For a non-technical user, think of it this way: if you're unsure about an answer, you might ``think it through'' multiple times and see if you keep getting the same result. If 4 out of 5 times you arrive at answer A and once at answer B, you'd suspect A is more likely correct. The model can do similarly with self-consistency prompting.

\paragraph{Recent Optimizations.} Techniques like ``Slim-SC'' attempt to prune redundant chains early. If the first 3 chains produce identical answers, the system stops sampling. Alternatively, model confidence scores can determine if more samples are needed. These optimizations can reduce computational overhead by 25--45\% while retaining accuracy benefits.

\subsection{Tool-Augmented Reasoning: The ReAct Framework}
\label{sec:llmB-react}

Sometimes an LLM by itself isn't enough to solve a problem correctly---it might need to look up information or perform a precise calculation. \keyterm{Tool-augmented reasoning} lets the model use external tools (search engines, calculators, databases) as part of its response. A leading approach integrating this is the \keyterm{ReAct} framework (short for Reasoning and Acting) \parencite{yao2023react}.

\subsubsection{The ReAct Loop}

In ReAct, the model interleaves reasoning steps with actions. The pattern structures the interaction as a dynamic loop:

\begin{enumerate}
  \item \textbf{Thought}: The model reasons about what it needs to know
  \item \textbf{Action}: The model emits a call to a tool (e.g., \texttt{Search[query]})
  \item \textbf{Observation}: The system executes the tool and pastes the result back into the context
  \item \textbf{Thought}: The model processes the observation
  \item \textbf{Answer}: The model generates the final response (or loops back to step 2 if more information is needed)
\end{enumerate}

\Cref{fig:llmB-react-loop} illustrates this cycle visually.

\begin{figure}[htbp]
  \centering
  \resizebox{0.75\textwidth}{!}{\input{figures/fig-react-loop}}
  \caption{The ReAct loop: the model alternates between reasoning (Thought), taking action via external tools (Action), and processing results (Observation) until it can produce a final answer. This grounds reasoning in real-world data.}
  \label{fig:llmB-react-loop}
\end{figure}

\begin{highlightbox}[title={ReAct Example: Legal Research}]
\textbf{User}: ``Has the Supreme Court ever overturned a precedent set by Roe v.\ Wade?''

\textbf{Internal ReAct trace} (not shown to user):
\begin{enumerate}
  \item \emph{Thought}: ``The user is asking about overturning a precedent of Roe v.\ Wade. I recall Dobbs v.\ Jackson (2022) might be related. Let me verify.''
  \item \emph{Action}: \texttt{Search[Dobbs Jackson 2022 Roe precedent]}
  \item \emph{Observation}: ``Dobbs v.\ Jackson Women's Health Organization (2022) overturned Roe v.\ Wade.''
  \item \emph{Thought}: ``Confirmed. The precedent was overturned in 2022 by Dobbs.''
\end{enumerate}

\textbf{Final Answer}: ``Yes. In 2022, the Supreme Court's decision in Dobbs v.\ Jackson Women's Health Organization expressly overturned the precedent established by Roe v.\ Wade.''
\end{highlightbox}

The user only sees the final answer. But by reasoning and using a tool in tandem, the model ensured accuracy and could provide a reference to the case.

\subsubsection{Comparison with Chain-of-Thought}

Where CoT is a \emph{closed loop} (internal reasoning only), ReAct is an \emph{open loop} (grounded in external reality). This significantly reduces hallucinations in knowledge-intensive tasks because the model is encouraged to fetch real data for uncertain parts rather than guessing \parencite{yao2023react}.

However, ReAct is inherently slower than CoT due to:
\begin{itemize}
  \item Network latency of tool calls
  \item Verbose ``thought-action-observation'' traces that consume tokens
  \item Potential for multiple tool calls in a single query
\end{itemize}

Benchmarks show ReAct outperforms CoT on fact-checking tasks (HotpotQA) but may struggle with pure logic puzzles where no external information is needed, due to the overhead of the tool-use format.

\paragraph{Use Cases for Legal and Financial Professionals.}
\begin{itemize}
  \item \textbf{Legal research assistant}: The LLM can query legal databases (Westlaw, LexisNexis) for case citations
  \item \textbf{Financial assistant}: The LLM can pull stock prices, economic data, or run calculations via APIs
  \item \textbf{Due diligence}: The LLM can search document repositories for specific provisions or clauses
  \item \textbf{Regulatory compliance}: The LLM can query regulatory databases for current rules
\end{itemize}

In all these cases, the model's answer quality is improved by factual grounding rather than relying on potentially outdated training data.

\subsubsection{Auditability Considerations}

For professional applications, always separate the model's final answer from the reasoning trace. The reasoning + tool usage log is for you (or an expert user) to inspect if needed, but a normal end-user interacting with the AI should just get the polished result. This keeps the experience clean and avoids confusing the user with the model's internal deliberations.

From a governance perspective, the complete ReAct trace provides an audit trail: which tools were called, what information was retrieved, and how the model synthesized its response. This is invaluable for compliance review and debugging.

\begin{highlightbox}[title={ReAct as Proto-Agentic Pattern}]
The ReAct pattern---reasoning interleaved with action---represents a proto-agentic architecture. Chapters~6--7 formalize when such systems qualify as genuine agents and how to architect them. Chapter~6 provides a rigorous framework distinguishing agents from tool-using systems, while Chapter~7's Planning and Action questions address how agents decompose tasks and execute tool calls.
\end{highlightbox}

\subsection{Advanced Topologies: Tree of Thoughts and Graph of Thoughts}
\label{sec:llmB-advanced}

Linear chains (CoT) and loops (ReAct) are sometimes insufficient for problems requiring exploration, backtracking, or combining multiple distinct ideas.

\subsubsection{Tree of Thoughts (ToT)}

\keyterm{Tree of Thoughts} generalizes CoT by framing the reasoning process as a search over a tree \parencite{yao2023tot}. At each step, the model generates multiple possible ``next thoughts'' (branches). It then self-evaluates these branches (using a heuristic or a prompt like ``Is this a promising direction?'') and proceeds only with the best ones, backtracking if a dead end is reached.

\begin{definitionbox}[title={Tree of Thoughts}]
\keyterm{Tree of Thoughts (ToT)} implements deliberate problem solving by:
\begin{enumerate}
  \item Generating multiple candidate next steps at each node
  \item Evaluating candidates for promise (self-evaluation)
  \item Pruning unpromising branches
  \item Backtracking when dead ends are reached
\end{enumerate}
This is analogous to implementing Breadth-First Search (BFS) or Depth-First Search (DFS) via prompting.
\end{definitionbox}

ToT is particularly effective for planning tasks. In the ``Game of 24'' puzzle (form the number 24 using four numbers and basic operations), GPT-4 with standard CoT achieved only a 4\% success rate, while ToT achieved 74\% \parencite{yao2023tot}.

\paragraph{When to Use ToT.}
\begin{itemize}
  \item Creative writing requiring exploration of alternatives
  \item Planning tasks with multiple valid approaches
  \item Puzzles and games requiring search
  \item Design problems with trade-offs
\end{itemize}

\subsubsection{Graph of Thoughts (GoT)}

\keyterm{Graph of Thoughts} further extends this by modeling reasoning as a Directed Acyclic Graph (DAG) \parencite{besta2024got}. It introduces operations that are impossible in trees:

\begin{itemize}
  \item \textbf{Aggregation}: Combining multiple distinct thoughts into a stronger solution
  \item \textbf{Refinement}: Looping back to improve a thought based on later insights
\end{itemize}

\begin{definitionbox}[title={Graph of Thoughts}]
\keyterm{Graph of Thoughts (GoT)} models reasoning as a DAG, enabling:
\begin{itemize}
  \item \textbf{Branching}: Exploring multiple paths simultaneously
  \item \textbf{Aggregation}: Combining insights from different branches
  \item \textbf{Refinement}: Iteratively improving partial solutions
\end{itemize}
This resembles MapReduce or dynamic programming patterns in computation.
\end{definitionbox}

For example, in a sorting task, GoT can break the list into sub-lists, sort them individually (Map), and then merge them (Reduce/Aggregate). While GoT offers the highest ceiling for complex creative or analytical tasks, the orchestration overhead is massive, making it suitable only for offline, high-value tasks where latency is secondary to quality.

\paragraph{Taxonomy of Reasoning Topologies.} Besta et al.\ (2025) provide a comprehensive taxonomy categorizing reasoning patterns from linear chains through branching trees to fully connected graphs, analyzing the computational trade-offs at each level \parencite{besta2025demystifying}.

These exploration patterns inform agent planning. Chapter~7 addresses how agents break complex jobs into steps, manage dependencies, and backtrack when needed---operationalizing ToT/GoT as planning strategies.

\subsection{Self-Reflection and Critique Loops}
\label{sec:llmB-reflection}

Wouldn't it be useful if the AI could check its own work? There is a class of techniques where the model is prompted to critique, verify, or improve its initial answer. This approach mimics human metacognition---thinking about thinking.

\subsubsection{The Critique-Improve Pattern}

One simple approach: after the model provides an answer, you (automatically) ask it, ``Are you sure about that? Is there any mistake in your solution?'' Surprisingly, the model might catch its own error and correct it. This doesn't always work, but it often can for calculation mistakes or logical inconsistencies, especially if the model is high capacity.

A more systematic version is the \keyterm{critique-and-revision loop}:

\begin{enumerate}
  \item \textbf{Generate}: Model produces an initial draft
  \item \textbf{Critique}: Model is prompted to evaluate the draft against criteria (e.g., ``Critique this for errors, bias, or clarity'')
  \item \textbf{Revise}: Model generates a revised version based on its critique
\end{enumerate}

\begin{highlightbox}[title={Self-Reflection Example: Investment Recommendation}]
\textbf{Initial response}: ``You should invest in technology stocks for growth.''

\textbf{Critique prompt}: ``Critique the above advice for any financial risks or missing considerations.''

\textbf{Model critique}: ``The advice doesn't consider the user's age, risk tolerance, existing portfolio allocation, or current market conditions. It also fails to include required disclaimers.''

\textbf{Revision prompt}: ``Given the critique, provide a more comprehensive response.''

\textbf{Revised response}: ``Technology stocks can offer growth potential, but the right allocation depends on your age, investment timeline, and risk tolerance. Consider consulting a licensed financial advisor who can assess your complete financial picture. This is general educational information, not personalized investment advice.''
\end{highlightbox}

Research by Shinn et al.\ (2023) demonstrated that an iterative ``reflective'' strategy enabled agents to significantly improve performance on coding tasks and decision-making games by learning from their own mistakes \parencite{shinn2023reflexion}. Li \& Zhao (2025) showed that self-reflection enhances LLM responses in academic contexts \parencite{li2025selfreflection}.

In agentic contexts, self-consistency informs termination conditions---how agents know when they've reached a reliable answer. Chapter~7's Termination question addresses explicit stopping conditions, goal satisfaction verification, and resource budgets.

\subsubsection{Practical Considerations}

\begin{itemize}
  \item \textbf{Limit iterations}: One round of self-reflection and revision yields most of the benefit. Beyond that, you risk the model oscillating or over-correcting. Two iterations is usually the maximum.

  \item \textbf{Different modes}: The critique prompt forces the model to focus on evaluation rather than generation, engaging a different ``mode'' of thinking (more analytical). This is why it can catch errors it made during generation.

  \item \textbf{Keep critiques private}: The critique may contain sensitive exploration. Show only the final revised answer to users unless explainability requires otherwise.

  \item \textbf{High-stakes applications}: Self-reflection is particularly valuable when the cost of an error is high. In legal or medical contexts, a second opinion (even from the same model acting as a ``critic'') is valuable.
\end{itemize}

\subsection{Few-Shot Examples: Teaching by Demonstration}
\label{sec:llmB-fewshot}

\keyterm{Few-shot prompting} is one of the simplest yet most effective ways to shape an LLM's behavior. It means giving the model a few examples of what you want within the prompt itself before asking it to perform. Think of it as on-the-fly training.

\subsubsection{How Few-Shot Learning Works}

This leverages the fact that large models have learned to continue patterns. The original GPT-3 paper demonstrated that providing a few examples of a task in the prompt allows the model to generalize to similar tasks without any parameter updates \parencite{brown2020gpt3}. In other words, GPT-3 showed that language models are \emph{few-shot learners}. This property means we can guide them with just text examples instead of explicitly programming new rules.

\begin{definitionbox}[title={Few-Shot Prompting}]
\keyterm{Few-shot prompting} provides 1--5 examples of the desired input-output pattern within the prompt. The model generalizes from these examples to handle new inputs in the same format/style.
\end{definitionbox}

\paragraph{Example Structure.}
\begin{quote}
\texttt{Example 1: [Input A] → [Output A]}\\
\texttt{Example 2: [Input B] → [Output B]}\\
\texttt{Example 3: [Input C] → [Output C]}\\
\texttt{Now your turn: [New Input] → ?}
\end{quote}

The model will usually imitate the style and structure of the examples.

\subsubsection{Designing Effective Examples}

\paragraph{Types of Examples.}
\begin{itemize}
  \item \textbf{Positive exemplars}: Show what good outputs look like. If designing a contract analysis bot, provide a mock contract clause and an ideal analysis.

  \item \textbf{Negative exemplars}: Sometimes it helps to show what \emph{not} to do. Use sparingly; heavy use can confuse the model. One or two can clarify boundaries.

  \item \textbf{Boundary cases}: If there are commonly tricky cases in your domain, include an example of how to handle one. For a financial assistant, a boundary case might be a user asking for stock predictions (which you want the assistant to refuse).
\end{itemize}

\begin{keybox}[title={Example Selection Principles}]
\begin{enumerate}
  \item \textbf{Relevance}: Select examples similar to the expected queries
  \item \textbf{Diversity}: Cover different scenarios, not just variations of one
  \item \textbf{Clarity}: Each example should have unambiguous input-output mapping
  \item \textbf{Recency}: Use up-to-date examples reflecting current practices
  \item \textbf{Locality}: Prefer examples from the same domain, jurisdiction, or context
\end{enumerate}
\end{keybox}

\subsubsection{Similarity-Based Selection}

Selecting examples that are relevant to the user's query is crucial. Research shows that retrieving in-context examples that closely match the query's topic or wording can significantly improve performance, more so than random selection \parencite{zebaze2024icl}.

The \keyterm{K-Nearest Neighbors (K-NN)} approach involves:
\begin{enumerate}
  \item Embedding the user's current query into a vector space
  \item Retrieving the $k$ most semantically similar examples from a labeled set
  \item Including those examples in the prompt
\end{enumerate}

For instance, if the user asks a question about mergers and acquisitions law, it's better to include few-shot examples about M\&A legal analyses rather than personal injury examples, even if the logical task is similar.

\subsubsection{Maximal Marginal Relevance for Diversity}

A potential pitfall of pure K-NN is redundancy. If the user asks about ``apples,'' K-NN might retrieve five examples about ``apples.'' This lacks diversity and fails to show the model how to handle variations.

\keyterm{Maximal Marginal Relevance (MMR)} addresses this by optimizing for both relevance and diversity:

\[
\text{MMR} = \lambda \cdot \text{Sim}(d, q) - (1 - \lambda) \cdot \max_{d_j \in S} \text{Sim}(d, d_j)
\]

where $d$ is the candidate example, $q$ is the query, $S$ is the set of already selected examples, and $\lambda$ balances relevance vs.\ diversity (typically $\lambda = 0.5$).

By ensuring few-shot examples cover a diverse range of reasoning patterns while remaining relevant, MMR prevents the model from overfitting to a narrow pattern.

\subsubsection{Bootstrapping Exemplars}
\label{sec:llmB-bootstrap}

When few-shot examples are scarce (the ``cold start'' problem), you can use the model itself to generate candidates:

\begin{enumerate}
  \item Use a strong model to generate candidate chains-of-thought for a set of questions
  \item Filter these candidates: keep only those leading to correct final answers
  \item Use these correct, synthetic chains as few-shot examples
\end{enumerate}

This technique, sometimes called ``Self-Instruct,'' allows rapid creation of high-quality exemplar libraries without manual human annotation \parencite{wang2023selfinstruct}.

\begin{highlightbox}[title={Bootstrapping Best Practices}]
\begin{itemize}
  \item \textbf{Human review}: Always review model-generated examples before deployment; they may contain subtle errors or biases
  \item \textbf{Track provenance}: Note which examples are real vs.\ AI-generated
  \item \textbf{Version control}: Maintain history of your exemplar library as it evolves
  \item \textbf{Domain expertise}: For legal/medical/financial domains, have an expert vet synthetic examples
\end{itemize}
\end{highlightbox}

\subsection{Reasoning in Professional Domains}

The reasoning patterns described above take on specific characteristics when applied to legal and financial contexts. These domains require not only accurate reasoning but also adherence to domain-specific norms of argumentation and evidence.

\subsubsection{Legal Reasoning Patterns}

Legal analysis presents distinctive requirements that influence how we structure reasoning traces:

\paragraph{Issue-Rule-Application-Conclusion (IRAC).} Legal analysis traditionally follows a structured format: identify the legal issue, state the governing rule, apply the rule to the facts, and draw a conclusion. Chain-of-thought prompting can be structured to mirror this pattern:

\begin{itemize}
  \item \textbf{Issue identification}: What legal question does this fact pattern present?
  \item \textbf{Rule statement}: What statutes, regulations, or case holdings govern this issue?
  \item \textbf{Application}: How do the specific facts map onto the legal rule's elements?
  \item \textbf{Conclusion}: What outcome follows from this analysis?
\end{itemize}

Few-shot examples that demonstrate this IRAC structure prime the model to organize its reasoning in a legally recognizable format.

\paragraph{Analogical Reasoning.} Legal reasoning frequently involves arguing by analogy to precedent cases. LLMs can perform analogical reasoning, but benefit from explicit prompting:

\begin{itemize}
  \item ``Compare the facts of this case to [precedent]. What are the key similarities and differences?''
  \item ``Identify the distinguishing facts that might lead to a different outcome.''
  \item ``Rank the cited cases by relevance to the current facts.''
\end{itemize}

Self-consistency is particularly valuable here, as different reasoning chains may identify different analogies, and majority voting can surface the most robust comparisons.

\paragraph{Counter-Argument Generation.} Legal advocacy requires anticipating opposing arguments. LLMs can be prompted to generate counter-arguments:

\begin{itemize}
  \item ``What would opposing counsel argue in response to this analysis?''
  \item ``Identify the weakest points in this legal position.''
  \item ``Steelman the opposing position---what's the strongest case against our conclusion?''
\end{itemize}

This form of self-reflection helps identify vulnerabilities in legal analysis before they are exploited by adversaries.

\paragraph{Authority Verification.} Legal reasoning depends on valid authority. LLMs are prone to hallucinating case citations, statute numbers, and legal rules. ReAct patterns are essential for legal applications:

\begin{itemize}
  \item Never cite a case without retrieval verification
  \item Verify that cited statutes remain current (not repealed or amended)
  \item Confirm that regulatory provisions are still effective
  \item Check for subsequent history that may have overruled a precedent
\end{itemize}

\subsubsection{Financial Reasoning Patterns}

Financial analysis requires its own adaptations of reasoning patterns:

\paragraph{Quantitative Reasoning Chains.} Financial analysis often involves sequential calculations. Chain-of-thought prompting should explicitly trace the computational steps:

\begin{itemize}
  \item State each input value and its source
  \item Show intermediate calculations explicitly
  \item Carry units through the computation (dollars, percentages, shares)
  \item Verify the final result with a reasonableness check
\end{itemize}

For high-stakes calculations, consider using tool calls to perform arithmetic rather than relying on the model's internal computation, which is notoriously unreliable for complex math.

\paragraph{Scenario Analysis.} Financial decision-making often requires analyzing multiple scenarios. Tree of Thoughts is naturally suited to this:

\begin{itemize}
  \item Base case: Expected market conditions
  \item Upside case: Favorable outcomes
  \item Downside case: Adverse developments
  \item Stress case: Extreme but plausible scenarios
\end{itemize}

Each branch can be explored independently, with conclusions synthesized across scenarios to inform risk-aware decisions.

\paragraph{Regulatory Compliance Reasoning.} Financial analysis must consider regulatory constraints. The reasoning trace should explicitly:

\begin{itemize}
  \item Identify applicable regulatory frameworks
  \item Check whether proposed actions trigger regulatory requirements
  \item Verify compliance with disclosure obligations
  \item Flag potential conflicts of interest
\end{itemize}

\paragraph{Market Data Integration.} Financial reasoning requires current market data. ReAct patterns should incorporate:

\begin{itemize}
  \item Real-time price lookups when valuations are needed
  \item Economic indicator retrieval for macroeconomic analysis
  \item Earnings data retrieval for company-specific analysis
  \item Rate lookups for fixed-income and derivatives calculations
\end{itemize}

Unlike legal research, where documents are relatively static, financial data is continuously changing. Systems must be designed to refresh data appropriately and timestamp conclusions.

\subsubsection{Professional Domain Challenges}

Both legal and financial domains share common challenges that influence reasoning pattern selection:

\paragraph{Explainability Requirements.} Both domains may require the ability to explain how conclusions were reached. This favors reasoning patterns that produce interpretable traces:

\begin{itemize}
  \item Chain-of-thought produces linear, readable explanations
  \item Tree of Thoughts shows alternative paths considered
  \item ReAct shows evidence consulted
  \item Self-consistency shows agreement across multiple analyses
\end{itemize}

Patterns that aggregate or compress reasoning (like majority voting without trace preservation) may sacrifice explainability.

\paragraph{Audit Trail Requirements.} Regulated industries often require detailed records of decision processes. Reasoning traces should be:

\begin{itemize}
  \item Timestamped with when the analysis was performed
  \item Associated with the model version used
  \item Linked to the specific data/documents consulted
  \item Preserved for the required retention period
\end{itemize}

\paragraph{Human-in-the-Loop Integration.} High-stakes professional applications typically require human review. Reasoning patterns should facilitate this:

\begin{itemize}
  \item Produce structured outputs that highlight key decision points
  \item Flag areas of uncertainty or low confidence
  \item Present alternative conclusions where appropriate
  \item Provide sufficient detail for expert review without overwhelming verbosity
\end{itemize}

\subsection{Selecting the Right Reasoning Pattern}

With this toolkit of reasoning patterns, how do you decide which to use? The next section provides a decision framework, but here are initial heuristics:

\begin{keybox}[title={Reasoning Pattern Selection Heuristics}]
\begin{itemize}
  \item \textbf{If the task is straightforward}: Use direct prompting (zero-shot or simple few-shot). Don't complicate unnecessarily.

  \item \textbf{If the task requires multi-step logic}: Use Chain-of-Thought. The model needs to ``show its work.''

  \item \textbf{If correctness is paramount}: Use Self-Consistency. Multiple paths reduce the chance of a flawed single chain.

  \item \textbf{If up-to-date information is needed}: Use ReAct with appropriate tools. The model's training data may be outdated.

  \item \textbf{If the problem requires exploration}: Use Tree of Thoughts. Planning and creative tasks benefit from trying multiple approaches.

  \item \textbf{If quality trumps latency}: Use Graph of Thoughts or iterative self-reflection. Reserve for high-value, offline analysis.
\end{itemize}
\end{keybox}

The following section elaborates on these trade-offs and provides a more structured decision framework.

