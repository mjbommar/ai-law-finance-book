\section{Introduction}
\label{sec:intro}

\textbf{Agent.} \textit{Agentic.}

Few terms generate more confusion despite widespread use. While these words appear everywhere—from marketing copy to academic papers—their meanings remain contested and often unclear. Yet despite this definitional chaos, the underlying concepts are deeply intuitive and accessible.

At heart, agents are simply \textbf{``doers'' with a to-do}. As we unpack this accessible starting point, we will discover more explicit conditions for identification. But this four-word formulation captures something essential: agency requires both goals and the capacity to act toward them.

\subsection{Motivation and Approach}

The proliferation of ``agentic AI'' makes definitional clarity urgent. Existing work remains fragmented across purpose and discipline: computer scientists cite Russell and Norvig \parencite{russellnorvig2020aima}, philosophers reference Bratman \parencite{bratman1987intention}, legal scholars consult the Restatement of Agency \parencite{restatement2006agency}, and commercial vendors seem untethered by anything other than sales.

Some of this fragmentation reflects genuinely different perspectives, such as whether we recognize agents by their \textit{internal properties} (mental states, intentions) or \textit{external manifestations} (observable behavior, delegated authority)—a spectrum we explore in Section~\ref{sec:disciplines}.  While theoretical considerations like these can be useful, it is now most critical that we \textbf{establish a practical framework} to guide communication and coordination.

To do so, we organize agency into three levels that correspond to answering the following questions:

\textbf{Level 1}: What makes \textit{anything}—biological or computational—an agent?

\textbf{Level 2}: What makes computational systems agentic?

\textbf{Level 3}: How do traditional and AI-powered agentic systems differ?

Answering these progressive questions establishes a nested hierarchy with three levels, as illustrated in Figure~\ref{fig:hierarchy}.

\input{figures/hierarchy-diagram}

\subsection{Level 1: Minimal Agency}

We begin with the conceptual foundation. What is the absolute minimum required for something to qualify as an agent—whether human, organizational, or computational? Level 1 establishes this baseline, applicable across all domains and technologies.

\begin{definitionbox}[title={\textbf{Level 1: Agent}}]
	An \keyterm{agent} is any system that pursues goals through perception and action, with at least minimal discretion over which action to take in response to what it perceives.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textit{Examples:} A paralegal researching case law. A thermostat maintaining temperature. An organization pursuing objectives.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textbf{Minimal properties (3):}
	\begin{itemize}[nosep,leftmargin=1.5em]
		\item \textbf{Goal}
		\item \textbf{Perception}
		\item \textbf{Action}
	\end{itemize}
\end{definitionbox}

\textbf{What these properties mean:}

\textbf{Goal} refers to a clear objective or task that the system pursues. These goals may be simple or complex, short-term or long-term, and may change over time. They may be provided by a human user, a software system, or even autonomously generated through ``meta-goals.'' They provide direction and purpose for the system's actions.

\textbf{Perception} is awareness of an environment through sensing capabilities. Notably, this environment need not be physical, let alone real; it could be abstract like a contract bargaining space or simulated in an imaginary world.

\textbf{Action} is the ability to influence the environment, such as by exerting physical forces, modifying the value of variables, executing commands, or any other actuation mechanism, i.e., \textit{actuators}. Minimal \textit{discretion} is assumed: the system can select among at least two possible actions or policies contingent on its perceptions.

This trinity forms the conceptual bedrock of agency—equally applicable to humans navigating social contexts, organizations pursuing strategic objectives, biological organisms seeking survival, or computational systems executing tasks. While these three characteristics suffice for theoretical classification, practical deployment demands additional sophistication.

\textbf{Distinguishing agents from non-agents reveals critical boundaries.} Consider systems we encounter daily that, despite their complexity, fail to meet our criteria. A compiler, though it transforms code through sophisticated analysis, operates without autonomous goals—it executes predetermined transformations. Legal databases, while responsive to queries and rich with information, lack independent objectives or environmental adaptation. Even a single ChatGPT response, impressive as it may be, represents a one-shot generation rather than iterative goal pursuit.

This baseline framework illuminates the essence of agency, yet professional applications demand more. The gap between a simple thermostat cycling toward a temperature target and an AI system conducting legal research spans more than technological sophistication—it requires architectural elements that ensure reliability, adaptability, and accountability. Legal research tools, portfolio management systems, and document review platforms operate in environments where stakes are high and errors costly. These operational realities shape our expanded framework for deployable agentic systems.

\subsection{Operational Definition: Agentic Systems}

While Level 1 establishes what makes anything an agent, computational systems in production require three additional properties beyond the minimal three. These six properties together define what we call \keyterm{agentic systems}—the operational standard that bridges conceptual agency and real-world deployment. Both traditional software (Level 2) and AI-powered implementations (Level 3) can achieve this operational standard, though they differ fundamentally in how they realize each property.

\begin{definitionbox}[unbreakable,title={\textbf{Operational Definition: Agentic System}}]
	An \keyterm{agentic system} is a goal-directed agent that repeatedly perceives and acts in its environment, adapting from observations until clear termination conditions are met (explicit or implicit).

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textit{Examples:} Legal research assistants that iteratively refine queries and generate case summaries. Contract analysis systems that identify risks and produce issue reports. Portfolio rebalancing systems that monitor positions and execute trades. Fraud detection systems that analyze transactions and generate alerts.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textbf{Additional properties beyond Level 1 (+3):}
	\begin{itemize}[nosep,leftmargin=1.5em]
		\item \textbf{Iteration} --- Repeat perceive-act cycle
		\item \textbf{Adaptation} --- Adjust based on feedback
		\item \textbf{Termination} --- Stopping conditions (explicit or implicit)
	\end{itemize}

	\textit{Total: 6 operational properties (3 minimal + 3 additional)}
\end{definitionbox}

These six properties emerged from decades of agent research as commonly recognized operational requirements for reliable computational deployment. While not formally proven as minimum necessary, they consistently appear across deployed systems in domains from robotics to enterprise software, reflecting lessons from fielding real-world implementations. Section~\ref{sec:history} traces how each property became recognized as essential through both theoretical development and practical experience.

The three additional properties transform minimal agency into operational systems. \textbf{Iteration} sustains the perceive-act cycle, enabling the system to repeatedly gather information and take action rather than executing a single operation. \textbf{Adaptation} allows the system to modify its strategies based on accumulated observations, learning which approaches succeed and which fail. \textbf{Termination} provides stopping conditions through either implicit goal satisfaction (reaching target state) or explicit control logic (resource limits, time constraints, escalation triggers), ensuring the system recognizes when to cease operation rather than cycling indefinitely.

The relationship between levels clarifies important boundaries. Every agentic system qualifies as an agent (possessing the minimal three properties), but the reverse doesn't hold—many agents lack the operational sophistication of agentic systems. A thermostat exemplifies the minimal baseline: it perceives temperature (sensor readings), acts (heating/cooling), pursues a goal (maintain target), iterates (continuous monitoring), adapts (responds to temperature changes), and terminates through implicit goal satisfaction (stops heating when target reached, stops cooling when target maintained). This implicit termination—where achieving the target state constitutes the stopping condition—differs from the explicit termination logic (resource budgets, escalation triggers, maximum iteration limits) typical of professional agentic systems. Both forms satisfy the termination requirement, though explicit mechanisms provide stronger operational guarantees for high-stakes deployments. Similarly, a human paralegal demonstrates all six properties behaviorally but operates through cognitive processes rather than discrete computational cycles.

This operational framework now raises the implementation question: \textit{How} do computational systems realize these six properties? The answer reveals an architectural distinction. Some systems use traditional programming—rules, algorithms, control logic—to manage planning and orchestration. Others employ AI/ML, particularly large language models, for these functions. We distinguish these as Level 2 (traditional) and Level 3 (AI-powered). Critically, this distinction is architectural, not evaluative: neither approach is inherently superior, and the boundary between them remains fluid and context-dependent.

\subsection{Level 2: Traditional Agentic Software}

\begin{definitionbox}[unbreakable,title={\textbf{Level 2: Traditional Agentic Software}}]
	Traditional agentic software implements the six operational properties using rules, algorithms, or deterministic logic.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textit{Examples:} Conflicts checking systems that scan firm databases and escalate matches. Regulatory compliance monitoring with rule-based alerts. Trading compliance systems that monitor positions and enforce limits. Invoice processing workflows with validation and retry logic.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textbf{Additional properties beyond agentic systems (+0):}
	\begin{itemize}[nosep,leftmargin=1.5em]
		\item \textit{No new properties --- same 6 as agentic systems}
	\end{itemize}
\end{definitionbox}

Traditional agentic software uses human-designed programming to implement all six properties. Planning and orchestration logic is explicitly coded through rules, algorithms, state machines, or control systems. Level 2 systems can be extremely sophisticated: a conflict checking system might employ graph analysis for relationship detection, fuzzy matching for entity resolution, and adaptive threshold tuning—all implemented through traditional programming techniques. The key architectural characteristic is that decision logic is specified by programmers at design time.

A conflicts checking system exemplifies Level 2: it has goals (identify potential conflicts), perception (scans firm databases for client/matter relationships), action (flags matches and escalates to ethics committee), iteration (continuous monitoring as new matters are opened), adaptation (adjusts matching thresholds based on false positive rates), and termination (stops when matter is cleared or rejected). The system achieves these properties through explicitly programmed logic—rules for relationship detection, graph traversal algorithms for indirect conflicts, configurable thresholds for fuzzy name matching.

Level 2 systems achieve all six operational properties through traditional software engineering. Their performance depends on design quality, domain expertise, and implementation rigor—not on whether they employ AI/ML techniques. A well-engineered Level 2 system can substantially outperform a poorly designed Level 3 system in reliability, predictability, and effectiveness for its intended domain.

\subsection{Level 3: AI-Powered Agentic Systems}

Level 3 systems use AI/ML—particularly large language models—to manage planning, orchestration, and adaptation. The architectural distinction from Level 2 is straightforward: where Level 2 systems execute explicitly programmed decision logic, Level 3 systems employ neural networks, language models, or other learned components for these functions. In practice, modern Level 3 systems are typically hybrid: LLMs handle high-level planning and natural language interaction, while traditional code manages structured operations like database queries or API calls. This is what most practitioners mean by ``AI agents.''

\begin{definitionbox}[title={\textbf{Level 3: AI-Powered Agentic Systems}}]
	AI-powered agentic systems implement the six operational properties through strategic integration of artificial intelligence—typically large language models (LLMs), vision-language models (VLMs), or neural networks—with traditional computational components.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textit{Examples:} AI legal research assistants that iteratively search case law and synthesize findings. AI contract risk analyzers that identify problematic clauses and recommend revisions. AI-powered document review systems that classify content and adapt to feedback. AI trading assistants that analyze market data and adjust strategies.

	\textcolor{border-neutral}{\rule{\linewidth}{0.4pt}}

	\textbf{Additional properties beyond agentic systems (+0):}
	\begin{itemize}[nosep,leftmargin=1.5em]
		\item \textit{No new properties --- same 6 as agentic systems}
	\end{itemize}
\end{definitionbox}

The architectural choice to use AI/ML for planning and orchestration has practical implications. LLMs enable natural language interfaces—users can specify goals conversationally rather than through structured formats. Neural networks handle pattern recognition tasks that would require extensive rule engineering. However, this approach trades some predictability for flexibility: LLM outputs can vary across runs, and behavior may be harder to audit than explicit rule chains. The boundary between Level 2 and Level 3 can blur—is a system using gradient boosting for fraud detection Level 2 or Level 3? Today's practical dividing line focuses on whether LLMs manage high-level planning and orchestration.

Consider an AI contract risk analyzer as a Level 3 exemplar. It possesses all six operational properties: goals (identify and assess contract risks), perception (reads contract text and clause context), action (flags problematic provisions, generates risk assessments), iteration (reviews document section by section), adaptation (adjusts risk scoring based on clause combinations and jurisdiction), and termination (stops when complete review is done or high-severity risk triggers immediate escalation). The LLM manages high-level planning—deciding which clauses merit detailed analysis, how to interpret ambiguous language, when to flag issues versus when to request human review—while traditional code handles document parsing, clause extraction, jurisdiction lookup, and risk score calculation. This hybrid architecture is typical of Level 3 systems: AI handles interpretation and strategic decisions, traditional programming handles structured data operations.

Table~\ref{tab:property-levels} maps the progression from minimal agency (3 properties) through operational deployment (6 properties) to implementation approaches.

\begin{table}[!htb]
	\centering
	\small
	\begin{tabular}{@{}lcccc@{}}
		\toprule
		\textbf{Property} & \textbf{Agent}                                   & \multicolumn{2}{c}{\textbf{Agentic Systems}}     & \textbf{Description}                                                              \\
		                  & \textbf{Level 1}                                 & \textbf{Level 2}                                 & \textbf{Level 3}                                  &                               \\
		                  & \textbf{Conceptual}                              & \textbf{Traditional}                             & \textbf{AI}                                       &                               \\
		\midrule
		Goal              & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm);  & Clear objective to pursue     \\
		Perception        & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm);  & Sense environment \& results  \\
		Action            & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm);  & Affect environment            \\
		\cmidrule{1-5}
		Iteration         & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[key-base] (0,0) circle (0.08cm);  & \tikz\fill[key-base] (0,0) circle (0.08cm);   & Repeat perceive-act cycle     \\
		Adaptation        & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[key-base] (0,0) circle (0.08cm);  & \tikz\fill[key-base] (0,0) circle (0.08cm);   & Adjust based on feedback      \\
		Termination       & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[key-base] (0,0) circle (0.08cm);  & \tikz\fill[key-base] (0,0) circle (0.08cm);   & Stopping condition (explicit or implicit)   \\
		\midrule
		AI-Powered        & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[example-base] (0,0) circle (0.08cm); & Uses AI/ML for implementation \\
		\bottomrule
	\end{tabular}
	\caption{Property requirements by level. Filled circles indicate required properties: blue (filled) for minimal agency (Level 1), amber (filled) for additional operational properties (Levels 2 \& 3). Empty circles indicate properties not required. Below the horizontal rule: green (filled) indicates AI-powered implementation (Level 3 only).}
	\label{tab:property-levels}
\end{table}

\subsection{Key Distinctions}

Having traced the progression from minimal agency (3 properties) through operational deployment (6 properties) to implementation approaches (traditional vs. AI), we can now synthesize what this hierarchy reveals. Level 1 establishes conceptual qualification, the operational definition adds production-readiness requirements, and Levels 2--3 distinguish implementation paradigms. This structure clarifies three critical distinctions that cut through definitional confusion:

The critical distinction lies between \textit{properties} and \textit{implementation}. The major jump in capability occurs between Level 1 (3 properties) and operational deployment (6 properties). Levels 2 and 3, by contrast, have \textit{identical property requirements}---they differ only in how those properties are implemented: through explicit rules and logic (Level 2) or through AI/ML models (Level 3).

This hierarchy enables precise terminology:


\begin{keybox}[title={Terminology Precision}]
	\begin{itemize}[nosep]
		\item \textbf{``Agent'' (noun):} Any system meeting Level 1's three minimal properties
		\item \textbf{``Agentic'' (adjective):} At the system level, describes systems meeting all six operational properties. We may also use it descriptively at the feature/behavior level (e.g., ``agentic behavior/properties''); reserve ``agentic system'' for six-of-six conformance.
		\item \textbf{``AI agent'':} An agentic system specifically powered by AI/ML (Level 3)
	\end{itemize}
\end{keybox}

These definitions enable clear exclusions:

\begin{keybox}[title={What Doesn't Qualify}]
	\begin{itemize}[nosep]
		\item Compilers, databases → lack goals (not Level 1)
		\item Single chatbot responses → lack iteration (not operational)
		\item Traditional ML classifiers → lack iteration and goals (not agentic)
		\item Rule-based expert systems → agentic but not AI-powered (Level 2, not Level 3)
	\end{itemize}
\end{keybox}

This framework provides scaffolding for the historical and theoretical analysis that follows. Tracing definitions across decades reveals how each level emerged from evolving technological capabilities and disciplinary concerns. But before diving into that scholarly context, we must address a pressing practical question: Why does definitional precision matter? In professional domains where autonomous systems make consequential decisions—legal research, medical diagnosis, financial advising—the distinction between marketing labels and architectural reality has high stakes.

\subsection{Why This Matters: Stakes and Applications}

With our three-level hierarchy and technical definition established, the practical implications become clear. The stakes extend far beyond academic precision—they shape how professionals evaluate systems, how researchers coordinate findings, how regulators craft policy, and how we hold vendors accountable for their claims.

Using the framework that we develop here, we can:

\textbf{Clearer Evaluation}

Distinguishing genuinely agentic systems from chatbots by assessing whether they meet all six operational properties or merely respond to single prompts (Table~\ref{tab:property-levels}).

\textbf{Better Research Coordination}

Ensuring scholars reference compatible concepts by specifying which properties their systems exhibit rather than relying on ambiguous labels like ``agents'' or ``autonomous systems.''

\textbf{Informed Regulation}

Crafting policies that target measurable capabilities—the six operational properties—rather than vendor marketing claims about ``AI agents.''

\textbf{Vendor Accountability}

Holding companies to falsifiable claims by requiring demonstration of all six operational properties, not just conversational ability or AI-powered text generation.

The stakes are particularly high in professional domains like law, medicine, and finance, where autonomous action by AI systems raises questions of \keyterm{liability}, \keyterm{professional responsibility}, and public safety. For legal professionals, \textit{Mata v. Avianca} sanctions \parencite{avianca-reuters} illustrate the consequences of deploying systems without understanding their architectural properties. Our framework provides the conceptual tools to avoid such failures—not by slowing adoption, but by enabling more precise evaluation of capabilities and limitations.

The remaining sections explain where this definition came from and why it takes this particular form. Section~\ref{sec:practical} provides a practical decision rubric for applying this definition.
