\section{Analytical Framework}
\label{sec:synthesis}

Having traced the historical evolution (Section~\ref{sec:history}), examined disciplinary perspectives (Section~\ref{sec:disciplines}), and identified analytical dimensions (Section~\ref{sec:dimensions}), we now synthesize these insights into a theoretical framework for analyzing agentic systems in legal contexts.

\subsection{Cross-Cutting Patterns}

Despite disciplinary diversity, several patterns emerge consistently:

\begin{keybox}[title={Cross-Cutting Patterns}]
\paragraph{Goal-Directedness}
Nearly every definition involves goals, objectives, or performance measures. Whether expressed as intentions (philosophy), reward functions (AI), mandates (law), or incentive structures (economics), agents are distinguished by oriented behavior toward outcomes.

\paragraph{Perception-Action Coupling}
From \textcite{russellnorvig2020aima}'s sensors and actuators to \textcite{bratman1987intention}'s practical reasoning to \textcite{bandura1989human}'s self-reactiveness, agents observe their environments and take actions. Single-shot transformations (compilers, converters) fall outside this pattern; iterative loops characterize agentic systems.

\paragraph{Selective Autonomy}
While autonomy levels vary (Section~\ref{sec:dimensions}), agency requires some discretion. Pure puppets executing scripted commands without choice do not count. The threshold varies---legal agents have minimal discretion, AI agents have substantial discretion---but the requirement persists.

\paragraph{Stopping Conditions}
Implicit or explicit, operational definitions assume systems eventually complete tasks or reach terminal states. Unbounded processes without convergence criteria—infinite loops, perpetual monitoring without decision points—lack the bounded purposefulness that characterizes agentic systems.
\end{keybox}

\subsection{Theoretical Foundations}

The cross-cutting patterns observed across disciplines build on rich theoretical traditions:

\paragraph{Classical Planning (AI)}
\keyterm{Goal-directedness} and means-end reasoning connect to STRIPS-style planning, where agents decompose goals into subgoals and select actions to achieve preconditions. Legal tasks like due diligence map naturally to hierarchical goal structures.

\paragraph{Reactive Architectures (Robotics)}
The \keyterm{perception-action coupling} loop reflects Brooks' subsumption architecture (layered reactive control) and behavior-based robotics. Agents need not maintain complex world models---tight coupling between sensing and acting suffices for many tasks.

\paragraph{BDI Models (Cognitive Science, Computer Science)}
Beliefs (about the world), Desires (goals to achieve), and Intentions (committed plans) provide one influential formal model. BDI architectures persist in agent-oriented programming and explain how agents manage competing objectives.

\paragraph{Reinforcement Learning (AI)}
The agent-environment interaction framework, where agents select actions, receive observations and rewards, and update policies, implements the iterative loop. RL formalizes learning from experience.

\paragraph{Intentional Stance (Philosophy)}
\textcite{dennett1987intentional}'s pragmatic criterion---treat systems as agents when attributing mental states yields reliable predictions---provides a philosophical justification. We adopt the intentional stance toward systems that exhibit goal-directed behavior.

\subsection{Application to Professional Contexts}

The analytical framework establishes what makes something an agent in general. Professional deployment in legal, medical, or financial contexts requires additional governance safeguards beyond these core properties—attribution mechanisms for tracing claims to sources, provenance tracking for auditable reasoning chains, escalation protocols for recognizing limitations, and confidentiality enforcement for maintaining information barriers. These capability-centric refinements are addressed in a companion chapter on governance and risk management for agentic systems.

\subsection{Limitations and Boundary Cases}

The analytical framework inevitably confronts boundary cases:

\paragraph{Minimal Autonomy}
Thermostats qualify as agents under our definition, yet have trivial autonomy. We accept this inclusion---thermostats exhibit goal-directedness, sensing, acting, and looping. Modern smart thermostats include explicit termination logic (time windows, energy budgets), while basic thermostats rely on implicit goal satisfaction through continuous cycling. We acknowledge that interesting agency questions arise at higher autonomy levels.

\paragraph{Emergent vs Individual Agency}
In multi-agent systems, is agency located in individuals or collectives? Both, potentially. Individual agents exhibit goal-directed behavior; the collective may exhibit emergent properties not reducible to individuals. Legal frameworks struggle with this (corporate vs individual liability), and AI systems inherit these tensions.

\paragraph{Virtual vs Embodied}
\textcite{kauffman2000investigations}'s thermodynamic criteria exclude purely virtual agents. We side with the computational traditions that accept virtual agents, but acknowledge philosophical objections remain valid.

\paragraph{Single-Shot LLM Responses}
As discussed in Section~\ref{sec:practical}, single-shot responses lack the iteration and adaptation required for agentic systems. However, if a response includes internal tool calls, observations, and updated reasoning within a single interaction, it may exhibit agentic properties architecturally even if perceived as one response by the user.

\subsection{Looking Forward}

The analytical framework and theoretical foundations provide stable conceptual tools during rapid technological change. As LLMs grow more capable, as multi-agent systems proliferate, and as AI-human collaboration evolves, these cross-disciplinary patterns help us:
\begin{itemize}
\item Distinguish genuine architectural innovations from repackaged capabilities
\item Maintain cross-disciplinary communication through shared vocabulary
\item Evaluate new systems against consistent criteria
\item Anticipate legal and ethical challenges based on autonomy levels and entity frames
\end{itemize}

A companion chapter examines how these theoretical foundations manifest in contemporary AI systems and frameworks.

\subsection{Definitional Consistency and Formal Verification}

The cross-disciplinary patterns and theoretical foundations we've identified provide valuable insights, but they also demand precision. When legal professionals evaluate AI agents for regulatory compliance, when engineers design multi-agent systems for financial markets, or when researchers assess claims about ``agentic'' capabilities, we need definitions that are not merely descriptive but formally verifiable.

To bridge the gap between conceptual understanding and rigorous application, we now present our framework in formal logical notation. This serves two purposes: it enables precise reasoning about edge cases and boundary conditions, and it provides a foundation for automated verification in computational systems. Each formal statement is paired with an accessible explanation, ensuring that both logically-trained readers and domain practitioners can engage with the material.

\subsubsection{Formal Notation and Predicates}

\begin{definitionbox}[title={Predicates and Relations}]
We define the following predicates for our formal framework:
\begin{itemize}
\item $\mathit{Agent}(x)$ — ``$x$ is an agent''
\item $\mathit{AgenticSystem}(x)$ — ``$x$ is an agentic system''
\item $\mathit{AIAgent}(x)$ — ``$x$ is an AI agent''
\item $\mathit{has}(x, p)$ — ``system $x$ has property $p$''
\item $\mathit{implementedWith}(x, t)$ — ``system $x$ is implemented with technology $t$''
\end{itemize}

Properties are denoted as:
\begin{itemize}
\item $G$ = Goal-directedness
\item $P$ = Perception capability
\item $A$ = Action capability
\item $I$ = Iteration (multi-cycle operation)
\item $D$ = Adaptation (learning/adjustment)
\item $T$ = Termination (explicit or implicit)
\end{itemize}
\end{definitionbox}

\subsubsection{Foundational Axioms}

\begin{theorembox}[title={Axiom A1: Minimal Agency}]
\textbf{Formal statement:}
$$\forall x \: [\mathit{Agent}(x) \leftrightarrow (\mathit{has}(x, G) \land \mathit{has}(x, P) \land \mathit{has}(x, A))]$$

\textbf{Accessible explanation:}
Something is an agent if and only if it has all three core properties: goal-directedness, perception, and action. This bidirectional relationship ($\leftrightarrow$) means these three properties are both necessary and sufficient for minimal agency.
\end{theorembox}

\begin{theorembox}[title={Axiom A2: Agentic Systems}]
\textbf{Formal statement:}
$$\forall x \: [\mathit{AgenticSystem}(x) \leftrightarrow (\mathit{Agent}(x) \land \mathit{has}(x, I) \land \mathit{has}(x, D) \land \mathit{has}(x, T))]$$

\textbf{Accessible explanation:}
An agentic system is precisely an agent that additionally possesses iteration, adaptation, and termination capabilities. The conjunction ($\land$) requires all six properties to be present.
\end{theorembox}

\begin{theorembox}[title={Axiom A3: AI Implementation}]
\textbf{Formal statement:}
$$\forall x \: [\mathit{AIAgent}(x) \leftrightarrow (\mathit{AgenticSystem}(x) \land \mathit{implementedWith}(x, \text{AI/ML}))]$$

\textbf{Accessible explanation:}
An AI agent is defined as an agentic system whose planning or orchestration mechanisms are implemented using artificial intelligence or machine learning technologies. Implementation is orthogonal to the definitional properties.
\end{theorembox}

\subsubsection{Derived Consequences}

\begin{keybox}[title={Theorem C1: Subsumption Hierarchy}]
\textbf{Formal statement:}
$$\forall x \: [\mathit{AgenticSystem}(x) \rightarrow \mathit{Agent}(x)]$$

\textbf{Proof:} Follows immediately from Axiom A2, since $\mathit{AgenticSystem}(x)$ requires $\mathit{Agent}(x)$.

\textbf{Accessible explanation:}
Every agentic system is necessarily an agent (but not vice versa). This creates a proper hierarchy where agentic systems form a subset of agents.
\end{keybox}

\begin{keybox}[title={Theorem C2: Non-Equivalence}]
\textbf{Formal statement:}
$$\exists x \: [\mathit{Agent}(x) \land \neg\mathit{AgenticSystem}(x)]$$

\textbf{Proof:} Consider a single-shot system with $G$, $P$, and $A$ but lacking $I$, $D$, or $T$.

\textbf{Accessible explanation:}
There exist agents that are not agentic systems. For example, a one-shot classification system that perceives input, pursues a goal of accurate classification, and outputs an action (the classification) is an agent but not an agentic system.
\end{keybox}

\begin{keybox}[title={Theorem C3: AI Agent Hierarchy}]
\textbf{Formal statement:}
$$\forall x \: [\mathit{AIAgent}(x) \rightarrow \mathit{AgenticSystem}(x) \rightarrow \mathit{Agent}(x)]$$

\textbf{Proof:} Chain of implications from Axioms A3 and A2.

\textbf{Accessible explanation:}
AI agents form the most specific category in our hierarchy. Every AI agent is an agentic system, and every agentic system is an agent, creating a three-level taxonomy.
\end{keybox}

\begin{keybox}[title={Theorem C4: Property Attribution}]
\textbf{Formal statement:}
$$\forall x, p \in \{G, P, A, I, D, T\} \: [\mathit{has}(x, p) \rightarrow \text{``}x\text{ exhibits agentic behavior w.r.t. } p\text{''}]$$
$$\text{but } \neg[\exists p \subset \{G, P, A, I, D, T\} \: \mathit{has}(x, p) \rightarrow \mathit{AgenticSystem}(x)]$$

\textbf{Accessible explanation:}
We can describe individual features as ``agentic'' without claiming the system is fully agentic. The term ``agentic system'' is reserved for systems exhibiting all six properties.
\end{keybox}

\subsubsection{Clarifying Principles}

\begin{highlightbox}
\textbf{Principle P1: Property-Implementation Distinction}

\textbf{Formal statement:}
$$\forall x, t_1, t_2 \: [(\mathit{Agent}(x) \land \mathit{implementedWith}(x, t_1)) \land (\mathit{Agent}(x) \land \mathit{implementedWith}(x, t_2))]$$
$$\rightarrow \text{Properties of } x \text{ are independent of } t_1 \text{ vs } t_2$$

\textbf{Accessible explanation:}
The properties that make something an agent (goal, perception, action) are distinct from how it's built. An agent implemented with rule-based systems has the same definitional properties as one implemented with neural networks.
\end{highlightbox}

\begin{highlightbox}
\textbf{Principle P2: Single-Shot LLM Constraint}

\textbf{Formal statement:}
$$\forall x \: [\text{SingleShotLLM}(x) \rightarrow (\mathit{Agent}(x) \leftrightarrow \text{InternalLoop}(x))]$$
$$\land \: [\text{SingleShotLLM}(x) \rightarrow \neg\mathit{has}(x, I) \rightarrow \neg\mathit{AgenticSystem}(x)]$$

\textbf{Accessible explanation:}
Single-shot LLM responses can only qualify as agents if they internally instantiate goal-perception-action loops within one interaction. Without iteration across interactions, they cannot be agentic systems.
\end{highlightbox}

\begin{highlightbox}
\textbf{Principle P3: Tool Use Necessity}

\textbf{Formal statement:}
$$\forall x \: [\text{UsesTool}(x) \not\rightarrow \mathit{Agent}(x)]$$
$$\text{but } \forall x \: [(\text{UsesTool}(x) \land \mathit{has}(x, I) \land \mathit{has}(x, D) \land \mathit{has}(x, G)) \rightarrow \mathit{Agent}(x)]$$

\textbf{Accessible explanation:}
Tool use alone doesn't make something an agent. However, iterative and adaptive tool use directed toward goals does contribute to agency. The key is the purposeful, iterative pattern, not the mere presence of tools.
\end{highlightbox}

\subsubsection{Termination Formalization}

\begin{definitionbox}[title={Termination Modes}]
We formalize termination $T$ as:
$$\mathit{has}(x, T) \leftrightarrow [\text{ExplicitTerm}(x) \lor \text{ImplicitTerm}(x)]$$

where:
\begin{itemize}
\item $\text{ExplicitTerm}(x) \leftrightarrow \exists c \: [\text{TermCondition}(c) \land \text{monitors}(x, c)]$
\item $\text{ImplicitTerm}(x) \leftrightarrow \exists g \: [\mathit{has}(x, g) \land \text{GoalSatisfiable}(g)]$
\end{itemize}

\textbf{Accessible explanation:}
A system has termination capability if it either explicitly monitors termination conditions (budgets, timeouts, escalation triggers) or implicitly terminates through goal satisfaction (reaching target state). Continuous control tasks qualify when they use bounded episodes or quiescent states.
\end{definitionbox}

\subsubsection{Practical Application of Formal Framework}

This formal framework serves as a reference point for evaluating systems and resolving definitional disputes. When assessing whether a new AI system qualifies as an ``agent,'' ``agentic system,'' or ``AI agent,'' practitioners can apply these formal criteria systematically. The logical structure also reveals why certain boundary cases—like thermostats or single-shot LLMs—generate debate: they satisfy some but not all definitional requirements.

More importantly, this formalization enables automated verification. Systems can self-certify their capabilities against these predicates, and regulatory frameworks can encode compliance requirements using the same logical structure. As we move toward environments where AI agents interact with legal and financial systems at scale, such formal verification becomes not just useful but essential.

\subsection{Common Questions About Agency}

Having established both the theoretical foundations and formal framework, we can now address common questions that arise when applying these concepts in practice:

\begin{questionbox}[title={\textbf{Is ``Iteration'' just repeated Action?}}]
No. Level~1 allows a single perceive–act cycle. Iteration requires multiple cycles with state carried forward, enabling error correction and pursuit across steps.
\end{questionbox}

\begin{questionbox}[title={\textbf{Do continuous control tasks fail Termination?}}]
No. We accept implicit termination (goal satisfaction) or explicit governance (budgets, episodes, escalation). Engineered deployments typically bound horizons even for continuing tasks.
\end{questionbox}

\begin{questionbox}[title={\textbf{If thermostats are agents, doesn't that make everything an agent?}}]
No. Many systems lack at least one of Goal, Perception, Action (e.g., compilers lack goals; pipelines often lack perception of results). Minimal inclusion does not trivialize the category.
\end{questionbox}

\begin{questionbox}[title={\textbf{Is ``AI agent'' just marketing?}}]
Here it has a falsifiable meaning: an agentic system (six properties) whose planning/orchestration is implemented with AI/ML. Single-shot chat completion is excluded by construction.
\end{questionbox}
