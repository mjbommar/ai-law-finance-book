% =============================================================================
% Bibliography â€” Multimodal Fundamentals
% Chapter 04: PDFs, Layout, Tables/Charts, Images, and Audio
% =============================================================================

% -----------------------------------------------------------------------------
% Document Layout and Structure
% -----------------------------------------------------------------------------

@inproceedings{xu2020layoutlm,
  author    = {Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  title     = {{LayoutLM}: Pre-training of Text and Layout for Document Image Understanding},
  booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year      = {2020},
  pages     = {1192--1200},
  doi       = {10.1145/3394486.3403172},
  url       = {https://arxiv.org/abs/1912.13318},
  urldate   = {2025-12-21},
  note      = {Foundational work combining text and layout for document understanding.}
}

@article{huang2022layoutlmv3,
  author    = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
  title     = {{LayoutLMv3}: Pre-training for Document AI with Unified Text and Image Masking},
  journal   = {arXiv preprint arXiv:2204.08387},
  year      = {2022},
  url       = {https://arxiv.org/abs/2204.08387},
  urldate   = {2025-12-21},
  note      = {Extends LayoutLM with unified text-image pre-training.}
}

@article{scan2025,
  author    = {{SCAN Authors}},
  title     = {{SCAN}: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation},
  journal   = {arXiv preprint},
  year      = {2025},
  url       = {https://arxiv.org/html/2505.14381v2},
  urldate   = {2025-12-21},
  note      = {Recent work on layout analysis for RAG pipelines.}
}

@online{azure-doc-intel,
  author    = {{Microsoft}},
  title     = {Retrieval-Augmented Generation ({RAG}) with Azure Document Intelligence},
  year      = {2024},
  url       = {https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/concept/retrieval-augmented-generation},
  urldate   = {2025-12-21},
  note      = {Practical guidance on layout analysis and table extraction.}
}

% -----------------------------------------------------------------------------
% Table and Chart Understanding
% -----------------------------------------------------------------------------

@inproceedings{wang2024chainoftable,
  author    = {Wang, Zilong and Zhang, Hao and Li, Chao-Hong and Eisenschlos, Julian Martin and Cohan, Arman and Wang, William Yang},
  title     = {Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2024},
  url       = {https://arxiv.org/abs/2401.04398},
  urldate   = {2025-12-21},
  note      = {Framework for table reasoning through iterative operations.}
}

@article{charge2025,
  author    = {{CHARGE Authors}},
  title     = {Benchmarking Multimodal {RAG} through a Chart-based Document Question-Answering Generation Framework},
  journal   = {arXiv preprint},
  year      = {2025},
  url       = {https://arxiv.org/html/2502.14864v1},
  urldate   = {2025-12-21},
  note      = {Framework for chart understanding and QA generation.}
}

@online{elastic-pdf-tables,
  author    = {{Elastic Search Labs}},
  title     = {From {PDF} Tables to Insights: An Alternative Approach for Parsing {PDFs} in {RAG}},
  year      = {2024},
  url       = {https://www.elastic.co/search-labs/blog/alternative-approach-for-parsing-pdfs-in-rag},
  urldate   = {2025-12-21},
  note      = {Practical vision-based approaches for PDF table extraction.}
}

% -----------------------------------------------------------------------------
% Audio and Video RAG
% -----------------------------------------------------------------------------

@article{radford2023whisper,
  author    = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title     = {Robust Speech Recognition via Large-Scale Weak Supervision},
  journal   = {arXiv preprint arXiv:2212.04356},
  year      = {2023},
  url       = {https://arxiv.org/abs/2212.04356},
  urldate   = {2025-12-21},
  note      = {Whisper ASR model with multi-language support and timestamps.}
}

@article{voxrag2025,
  author    = {{VoxRAG Authors}},
  title     = {{VoxRAG}: A Step Toward Transcription-Free {RAG} Systems in Spoken Question Answering},
  journal   = {arXiv preprint},
  year      = {2025},
  url       = {https://arxiv.org/html/2505.17326v1},
  urldate   = {2025-12-21},
  note      = {Research on transcription-free audio RAG.}
}

@article{videorag2025,
  author    = {{VideoRAG Authors}},
  title     = {{VideoRAG}: Retrieval-Augmented Generation with Extreme Long-Context Videos},
  journal   = {arXiv preprint},
  year      = {2025},
  url       = {https://arxiv.org/html/2502.01549v1},
  urldate   = {2025-12-21},
  note      = {Dual-channel architecture for video understanding.}
}

@online{learnopencv-videorag,
  author    = {{LearnOpenCV}},
  title     = {Video-{RAG}: Training-Free Retrieval for Long-Video {LVLMs}},
  year      = {2024},
  url       = {https://learnopencv.com/video-rag-for-long-videos/},
  urldate   = {2025-12-21},
  note      = {Practical tutorial on video RAG implementation.}
}

% -----------------------------------------------------------------------------
% Multimodal Embeddings
% -----------------------------------------------------------------------------

@inproceedings{radford2021clip,
  author    = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  title     = {Learning Transferable Visual Models From Natural Language Supervision},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.00020},
  urldate   = {2025-12-21},
  note      = {CLIP model for cross-modal text-image embeddings.}
}

@online{tds-multimodal-rag,
  author    = {{Towards Data Science}},
  title     = {Building a Multimodal {RAG} That Responds with Text, Images, and Tables from Sources},
  year      = {2024},
  url       = {https://towardsdatascience.com/building-a-multimodal-rag-with-text-images-tables-from-sources-in-response/},
  urldate   = {2025-12-21},
  note      = {Practical guide to multimodal RAG implementation.}
}

@online{medium-rag-images-tables,
  author    = {Ashwin},
  title     = {{RAG} with Images and Tables: Enhancing Retrieval-Augmented Generation for Multimodal Content},
  year      = {2024},
  url       = {https://medium.com/@ashwindevelops/rag-with-images-and-tables-enhancing-retrieval-augmented-generation-for-multimodal-content-a18da39571d5},
  urldate   = {2025-12-21},
  note      = {Comparison of unified embeddings vs late fusion approaches.}
}

% -----------------------------------------------------------------------------
% Privacy and Content Authenticity
% -----------------------------------------------------------------------------

@online{presidio,
  author    = {{Microsoft}},
  title     = {Presidio: An Open-Source Framework for Detecting, Redacting, and Anonymizing Sensitive Data},
  year      = {2024},
  url       = {https://github.com/microsoft/presidio},
  urldate   = {2025-12-21},
  note      = {PII detection and anonymization framework.}
}

@online{c2pa-spec,
  author    = {{Coalition for Content Provenance and Authenticity}},
  title     = {{C2PA} Specification},
  year      = {2024},
  url       = {https://c2pa.org/specifications/specifications/2.0/specs/C2PA_Specification.html},
  urldate   = {2025-12-21},
  note      = {Technical specification for content credentials.}
}

@online{adobe-content-credentials,
  author    = {{Adobe}},
  title     = {Content Credentials Overview},
  year      = {2024},
  url       = {https://helpx.adobe.com/creative-cloud/apps/adobe-content-authenticity/content-credentials/overview.html},
  urldate   = {2025-12-21},
  note      = {Practical guide to implementing content credentials.}
}

@online{cai-wikipedia,
  author    = {{Wikipedia}},
  title     = {Content Authenticity Initiative},
  year      = {2024},
  url       = {https://en.wikipedia.org/wiki/Content_Authenticity_Initiative},
  urldate   = {2025-12-21},
  note      = {Overview of the CAI consortium and standards.}
}
