% ============================================================================
% Design Principles: Dimensional Calibration — Agents Part III
% Purpose: Present risk-based control calibration framework
% Label: sec:agents3-dimensional
% ============================================================================

\section{Design Principles: Dimensional Calibration}
\label{sec:agents3-dimensional}

This section establishes \emph{how much} control intensity is required—how to calibrate governance based on system properties. Section~\ref{sec:agents3-governance-stack} will then demonstrate \emph{what} controls are required across regulatory layers, applying this calibration framework to specific legal and professional obligations. The key insight: governance is not binary (present or absent) but dimensional (scaled to risk). Organizations that apply uniform controls to all agentic systems either over-engineer low-risk deployments (wasting resources) or under-protect high-risk deployments (creating liability exposure). Dimensional calibration matches governance intensity to the operational characteristics that drive risk.

\subsection{The Dimensional Calibration Logic}
\label{sec:agents3-dimensional-logic}

Before applying dimensional calibration, verify the system qualifies as an \emph{agentic system} under Part I's framework:

\begin{keybox}[title={GPA+IAT Validation Requirement}]
Dimensional calibration applies \textbf{only to agentic systems}—those meeting all six operational properties:

\textbf{GPA (Level 1 Agent)}:
\begin{itemize}
\item \textbf{Goal}: Clear objective directing behavior?
\item \textbf{Perception}: Observes environment/inputs?
\item \textbf{Action}: Affects environment/produces outputs?
\end{itemize}

\textbf{+IAT (Agentic System)}:
\begin{itemize}
\item \textbf{Iteration}: Multiple perception-action cycles?
\item \textbf{Adaptation}: Modifies strategy based on feedback?
\item \textbf{Termination}: Explicit or implicit stopping conditions?
\end{itemize}

Systems lacking any of these six properties are \emph{not agentic systems}. A single-shot ML model, batch classifier, or non-iterative chatbot requires different governance approaches beyond this chapter's scope. \textbf{Verify GPA+IAT compliance before proceeding to dimensional calibration.}
\end{keybox}

We calibrate governance intensity across four analytical dimensions introduced in Part I:

\begin{enumerate}
\item \textbf{Autonomy}: The degree of independence the system exercises in decision-making. Ranges from human-in-the-loop (HITL) requiring pre-approval for every significant action, through human-on-the-loop (HOTL) where humans monitor and can intervene, to human-in-command (HIC) where humans set strategic goals and retain emergency stop authority but do not review individual decisions.

\item \textbf{Entity Frame}: How the system presents itself and how users perceive its role. Ranges from \emph{human frame} (agent represents a specific professional), through \emph{hybrid} (collaborative partnership), to \emph{machine frame} (clearly identified as non-human tool), to \emph{institutional frame} (agent acts on behalf of the organization).

\item \textbf{Goal Dynamics}: How the system's objectives change over time. Ranges from \emph{static} (fixed goals validated once), through \emph{adaptive} (system refines goals within predefined boundaries based on feedback), to \emph{negotiated} (system proposes goal changes requiring explicit human approval).

\item \textbf{Persistence}: Whether the system maintains state across interactions. Ranges from \emph{stateless} (each interaction independent) to \emph{stateful} (system accumulates information, builds context, and decisions depend on interaction history).
\end{enumerate}

Risk is \textbf{multidimensional}, not unidimensional. A system's overall risk profile emerges from the \emph{combination} of autonomy, entity frame, goal dynamics, and persistence. Control intensity must respond to this combination. The following subsections calibrate each dimension independently, then Section~\ref{sec:agents3-calibration-integration} demonstrates integration.

\begin{keybox}[title={Why Dimensional Calibration Matters}]
Generic governance frameworks provide one-size-fits-all guidance: ``implement human oversight,'' ``maintain logs,'' ``ensure fairness.'' Dimensional calibration operationalizes these principles: \emph{how much} human oversight (HITL vs. HOTL vs. HIC)? \emph{how detailed} must logs be (decision rationale vs. inputs/outputs only)? \emph{how frequently} must fairness be validated (pre-deployment only vs. continuous monitoring)?

Without calibration, organizations default to either maximum controls (expensive, slow, may not be technically feasible) or minimum controls (cheap, fast, exposes liability). Calibration enables proportionate governance: controls sufficient to manage risk without unnecessary overhead.
\end{keybox}

\subsection{Autonomy Calibration}
\label{sec:agents3-autonomy-calibration}

Autonomy determines the degree of human involvement in decision-making. We distinguish three oversight modes, ordered by increasing system autonomy:

\paragraph{Human-in-the-Loop (HITL): Pre-Approval Required}
In HITL mode, the system recommends actions but a human must approve before execution. HITL is appropriate when:
\begin{itemize}
\item Actions are high-consequence and irreversible (e.g., filing court documents, executing large trades, signing contracts).
\item Professional duties require human judgment (e.g., attorney competence obligations, fiduciary duty).
\item Regulatory requirements mandate human review (e.g., certain medical diagnoses, credit decisions in some jurisdictions).
\end{itemize}

\textbf{Governance implications}: HITL systems require approval workflows, notification mechanisms, and clear assignment of approval authority. Logging must capture both the system's recommendation and the human's decision (approve, reject, modify). Because humans review every significant action, post-action monitoring can be lighter—the human reviewer serves as the primary control. However, organizations must guard against automation bias: humans rubber-stamping AI recommendations without meaningful review.

\textbf{Example}: A legal research assistant that \emph{iteratively} searches case law: it queries legal databases, evaluates result relevance, refines search terms based on findings, and generates progressive case summaries—all before presenting final output to an attorney for review. The attorney verifies citations, assesses legal reasoning, and takes responsibility for the final work product. \textbf{Note}: If the attorney must approve each individual search query before the next query executes, the system lacks autonomous iteration and is not a full agentic system despite having other properties.

\paragraph{Human-on-the-Loop (HOTL): Monitoring with Intervention}
In HOTL mode, the system operates autonomously within defined parameters, but humans monitor performance and can intervene if anomalies, errors, or safety concerns arise. HOTL is appropriate when:
\begin{itemize}
\item Actions are moderate-consequence or reversible (e.g., customer service responses, preliminary data analysis).
\item Real-time human review would create unacceptable latency but oversight remains necessary.
\item The system operates within well-defined boundaries (e.g., credit limits, risk parameters).
\end{itemize}

\textbf{Governance implications}: HOTL systems require monitoring dashboards, anomaly detection, escalation triggers, and intervention protocols. Logging must be sufficiently detailed to enable retrospective review—since humans do not approve every action prospectively, they must be able to audit decisions retrospectively. Escalation triggers define when the system must halt and request human guidance (e.g., low-confidence decisions, outcomes near policy boundaries, user complaints).

\textbf{Example}: A customer service chatbot that handles routine inquiries autonomously but escalates complex questions, complaints, or regulatory issues to human agents. Supervisors monitor conversation logs, error rates, and escalation frequency.

\paragraph{Human-in-Command (HIC): Strategic Oversight and Emergency Stop}
In HIC mode, the system operates with high autonomy. Humans set strategic goals, define constraints, and monitor aggregate performance but do not review individual decisions. Humans retain emergency stop authority to halt the system if safety violations, systemic failures, or regulatory concerns emerge. HIC is appropriate when:
\begin{itemize}
\item The system operates at scale and speed that precludes individual review (e.g., fraud detection processing millions of transactions daily).
\item Actions are individually low-consequence but cumulatively significant.
\item The system operates in a stable, well-understood environment with strong safeguards.
\end{itemize}

\textbf{Governance implications}: HIC systems require exceptionally strong logging, monitoring, and retrospective audit capabilities. Because humans do not review decisions prospectively or monitor continuously at the individual level, post-action auditability becomes critical. Organizations must implement statistical monitoring (e.g., fairness metrics, error rate trends, drift detection) to identify systemic issues. Emergency stop mechanisms must be accessible to authorized personnel and tested regularly.

\textbf{Example}: A fraud detection system that automatically blocks transactions meeting defined risk criteria. Fraud analysts set risk parameters, monitor aggregate block rates and false positive rates, and investigate flagged cases retrospectively. The system can be halted immediately if systemic bias or operational failures are detected.

\paragraph{The Autonomy-Auditability Trade-off}
As autonomy increases, the burden of governance shifts from ex-ante (pre-approval) to ex-post (logging, monitoring, audit). HITL systems rely on human review as the primary control; HIC systems rely on comprehensive logging and statistical monitoring. Organizations must invest in monitoring infrastructure proportionate to autonomy: high-autonomy systems cannot rely on ``we'll review it if someone complains.''

Table~\ref{tab:agents3-autonomy-calibration} summarizes autonomy calibration.

\begin{table}[htbp]
\centering
\caption{Autonomy Calibration: Oversight Modes and Control Requirements}
\label{tab:agents3-autonomy-calibration}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm} >{\raggedright\arraybackslash}p{3.5cm} >{\raggedright\arraybackslash}p{3cm} >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Autonomy Level} & \textbf{Description} & \textbf{Example Use Cases} & \textbf{Control Requirements} \\
\midrule
\textbf{HITL} (Human-in-the-Loop) & Human pre-approves significant actions & Legal research, investment advice, contract review & Approval workflows, automation bias mitigation, competence training \\
\addlinespace
\textbf{HOTL} (Human-on-the-Loop) & System operates autonomously; humans monitor and intervene & Customer service chatbots, preliminary audit analytics & Monitoring dashboards, escalation triggers, intervention protocols \\
\addlinespace
\textbf{HIC} (Human-in-Command) & High autonomy with strategic oversight and emergency stop & Fraud detection, credit pre-screening (within parameters), algorithmic trading & Comprehensive logging, statistical monitoring, emergency stop, fairness metrics \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Entity Frame Calibration}
\label{sec:agents3-entity-frame-calibration}

Entity frame determines how the system presents itself and how users perceive its role. Entity frame affects trust, liability allocation, and user expectations. Mismatches between entity frame and governance create risk.

\paragraph{Human Entity Frame}
The system represents a specific human professional (e.g., ``your attorney,'' ``your financial adviser''). Users may not distinguish between the professional and the AI tool.

\textbf{Governance implications}: Human frame creates the highest accountability expectations. Professional responsibility rules apply in full. The professional represented by the system bears liability for all outputs. Confidentiality, competence, and fiduciary duty obligations are non-delegable. Governance must ensure the professional reviews, validates, and takes ownership of AI-generated outputs.

\textbf{Mismatch risk}: If the system operates with high autonomy (HIC) but presents a human frame, users may assume human oversight that does not exist. This creates misplaced trust and potential liability.

\textbf{Example}: A legal research tool that produces work product under the attorney's name. The attorney must verify citations, assess legal reasoning, and ensure compliance with Rule 1.1 (competence) and Rule 3.3 (candor).

\paragraph{Hybrid Entity Frame}
The system is presented as a collaborative partnership between human and AI (e.g., ``AI-assisted analysis,'' ``our team uses advanced tools'').

\textbf{Governance implications}: Hybrid frame requires clear delineation of responsibilities. Users should understand that AI provides preliminary analysis or recommendations, but humans make final decisions. Transparency about the division of labor reduces misplaced trust. Governance must document which tasks are AI-performed vs. human-performed and ensure human review of AI outputs before client-facing use.

\textbf{Example}: An investment advisory firm that discloses: ``Our financial plans combine AI-driven market analysis with our advisers' professional judgment and knowledge of your personal circumstances.''

\paragraph{Machine Entity Frame}
The system is clearly identified as a non-human tool (e.g., ``AI chatbot,'' ``automated system''). Users understand they are interacting with technology, not a human.

\textbf{Governance implications}: Machine frame sets appropriate expectations. Users are less likely to assume human judgment, empathy, or professional accountability. However, organizations must ensure the system's capabilities match user expectations—a chatbot labeled as ``informational only'' should not provide advice that creates reliance. Governance must include clear disclaimers, capability limitations, and escalation to humans for complex or high-stakes issues.

\textbf{Example}: A customer service chatbot that states: ``I'm an AI assistant. I can help with account questions, but for disputes or complex issues, I'll connect you with a human agent.''

\paragraph{Institutional Entity Frame}
The system acts on behalf of the organization (e.g., ``XYZ Bank's credit decisioning system,'' ``our firm's compliance review tool''). The organization, not an individual, bears accountability.

\textbf{Governance implications}: Institutional frame allocates liability to the organization. This is appropriate for systems used in institutional decision-making (credit underwriting, hiring, fraud detection). Governance must include organizational oversight (board and executive accountability), institutional policies (acceptable use, risk appetite), and enterprise-level monitoring. Professional responsibility considerations (if applicable) must be addressed separately.

\textbf{Mismatch risk}: If an institutional system operates without adequate organizational oversight (e.g., deployed by a rogue team without executive approval), the organization may face liability for decisions it did not authorize.

\textbf{Example}: A bank's credit pre-screening system that evaluates mortgage applications under institutional policies, with oversight by the Chief Risk Officer and compliance with ECOA.

Table~\ref{tab:agents3-entity-frame-calibration} summarizes entity frame calibration.

\begin{table}[htbp]
\centering
\caption{Entity Frame Calibration: Presentation Modes and Accountability Structures}
\label{tab:agents3-entity-frame-calibration}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2.8cm} >{\raggedright\arraybackslash}p{3.2cm} >{\raggedright\arraybackslash}p{3cm} >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Entity Frame} & \textbf{Description} & \textbf{Example Use Cases} & \textbf{Accountability Structure} \\
\midrule
\textbf{Human} & Agent represents specific professional & Legal research under attorney name, personalized financial advice & Professional bears full liability; professional responsibility rules apply; non-delegable duties \\
\addlinespace
\textbf{Hybrid} & Collaborative human-AI partnership & AI-assisted audit analytics, co-drafted documents & Shared responsibility; clear delineation required; human validates AI outputs \\
\addlinespace
\textbf{Machine} & Clearly identified as non-human tool & Customer service chatbot with AI disclosure, informational tools & Organization responsible for tool fitness; clear disclaimers and capability limitations \\
\addlinespace
\textbf{Institutional} & Agent acts on behalf of organization & Credit decisioning, hiring, compliance review & Organizational liability; board/executive oversight; institutional policies and monitoring \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Goal Dynamics Calibration}
\label{sec:agents3-goal-dynamics-calibration}

Goal dynamics determine how the system's objectives change over time. Static goals are easiest to govern; negotiated goals create the highest misalignment risk.

\paragraph{Static Goals}
The system pursues a fixed objective defined at deployment. The goal does not change without explicit redeployment.

\textbf{Governance implications}: Static goals can be validated once during pre-deployment review. Organizations assess whether the goal aligns with organizational objectives, legal requirements, and ethical constraints. Once validated, the goal remains stable. Governance focuses on monitoring whether the system achieves the goal and whether side effects emerge.

\textbf{Example}: A legal research tool with the static goal: ``Identify cases cited in the brief and verify they exist in official reporters.'' The goal does not change; the tool performs the same validation task repeatedly.

\paragraph{Adaptive Goals}
The system refines its objectives within predefined boundaries based on feedback, but cannot change goals fundamentally. For example, a fraud detection system might adjust risk weights based on observed fraud patterns, but cannot change its core objective (detect fraud) or operate outside defined risk parameters.

\textbf{Governance implications}: Adaptive goals require continuous monitoring to ensure the system remains within boundaries. Organizations must define:
\begin{itemize}
\item \textbf{Boundaries}: What aspects of the goal can adapt? What constraints are inviolable?
\item \textbf{Monitoring}: How frequently are adaptations reviewed? What triggers revalidation?
\item \textbf{Rollback}: If adaptation degrades performance or violates constraints, can the system revert to a prior known-good state?
\end{itemize}

\textbf{Example}: A credit scoring model that adapts feature weights based on performance feedback but cannot introduce new features, change fairness constraints, or operate outside regulatory compliance boundaries.

\paragraph{Negotiated Goals}
The system proposes changes to its objectives and requests human approval before implementation. This is the highest governance burden because each goal change requires validation.

\textbf{Governance implications}: Negotiated goals require human-in-the-loop approval for every proposed change. Organizations must establish:
\begin{itemize}
\item \textbf{Approval Authority}: Who can approve goal changes? (Typically requires senior leadership or governance committee.)
\item \textbf{Change Justification}: Why is the system proposing the change? What evidence supports it?
\item \textbf{Impact Assessment}: What are the consequences of the proposed goal change?
\item \textbf{Revalidation}: After goal change, the system must be revalidated for safety, fairness, and compliance.
\end{itemize}

\textbf{Example}: An AI strategic planning assistant that proposes: ``Based on market analysis, I recommend shifting investment focus from Technology to Healthcare.'' This goal change requires executive approval, risk assessment, and fiduciary duty review.

Table~\ref{tab:agents3-goal-dynamics-calibration} summarizes goal dynamics calibration.

\begin{table}[htbp]
\centering
\caption{Goal Dynamics Calibration: Objective Stability and Governance Requirements}
\label{tab:agents3-goal-dynamics-calibration}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm} >{\raggedright\arraybackslash}p{3.5cm} >{\raggedright\arraybackslash}p{3cm} >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Goal Dynamics} & \textbf{Description} & \textbf{Example Use Cases} & \textbf{Governance Requirements} \\
\midrule
\textbf{Static} & Fixed objectives; no goal changes & Citation verification, rule-based compliance checks & One-time goal validation; monitor achievement and side effects \\
\addlinespace
\textbf{Adaptive} & Refinement within boundaries based on feedback & Credit scoring (adjust weights), fraud detection (adapt risk parameters) & Define boundaries, continuous monitoring, rollback capability, revalidation triggers \\
\addlinespace
\textbf{Negotiated} & System proposes goal changes requiring human approval & Strategic planning assistant, adaptive investment strategy & Approval workflows, impact assessment, revalidation after changes, senior leadership involvement \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Persistence Calibration}
\label{sec:agents3-persistence-calibration}

Persistence determines whether the system maintains state across interactions. Stateful systems create compounding error risk and require state integrity controls.

\paragraph{Stateless Systems}
Each interaction is independent. The system does not retain information from prior interactions.

\textbf{Governance implications}: Stateless systems are simpler to govern. Errors do not compound—a mistake in one interaction does not affect subsequent interactions. Logging can be lighter (capture inputs/outputs without state reconstruction). Reproducibility requires only input data, not interaction history.

\textbf{Example}: A legal citation verification tool that checks each citation independently. An error in verifying Citation A does not affect the verification of Citation B.

\paragraph{Stateful Systems}
The system accumulates information across interactions. Decisions depend on prior state.

\textbf{Governance implications}: Stateful systems require state management controls:
\begin{itemize}
\item \textbf{State Integrity}: Protect state from tampering, corruption, or adversarial manipulation.
\item \textbf{State Logging}: Capture state changes to enable decision reconstruction.
\item \textbf{Error Compounding}: Monitor for cases where an initial error propagates through subsequent decisions.
\item \textbf{State Reset}: Define conditions under which state should be reset (e.g., user logout, policy change, detected anomaly).
\end{itemize}

\textbf{Example}: A financial planning chatbot that builds a profile of the client's financial situation across multiple conversations. If the system misunderstands the client's risk tolerance in Session 1, all subsequent recommendations may be inappropriate. Governance must include periodic state validation (``Let me confirm: your risk tolerance is Moderate, correct?'') and state logging to reconstruct how the profile evolved.

Table~\ref{tab:agents3-persistence-calibration} summarizes persistence calibration.

\begin{table}[htbp]
\centering
\caption{Persistence Calibration: State Management and Control Requirements}
\label{tab:agents3-persistence-calibration}
\small
\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm} >{\raggedright\arraybackslash}p{3.5cm} >{\raggedright\arraybackslash}p{3cm} >{\raggedright\arraybackslash}p{4cm}}
\toprule
\textbf{Persistence} & \textbf{Description} & \textbf{Example Use Cases} & \textbf{Governance Requirements} \\
\midrule
\textbf{Stateless} & Each interaction independent; no retained state & Citation verification, single-query research, one-time calculations & Standard logging (inputs/outputs); no state management; errors do not compound \\
\addlinespace
\textbf{Stateful} & System maintains state across interactions; decisions depend on history & Multi-session financial planning, ongoing fraud monitoring, adaptive customer profiles & State integrity protection, state change logging, error compounding monitoring, periodic state validation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Integration: Risk-Calibrated Control Selection}
\label{sec:agents3-calibration-integration}

Dimensional calibration becomes powerful when dimensions are integrated. A system's overall risk profile emerges from the \emph{combination} of autonomy, entity frame, goal dynamics, and persistence. Controls must respond to this multidimensional risk.

\paragraph{Low-Risk Profile Example: Legal Research Assistant (HITL + Human + Static + Stateless)}
\begin{itemize}
\item \textbf{Autonomy}: HITL—attorney must review and verify all outputs before use.
\item \textbf{Entity Frame}: Human—tool operates under attorney's name and professional responsibility.
\item \textbf{Goal Dynamics}: Static—fixed goal (verify citations exist and support arguments).
\item \textbf{Persistence}: Stateless—each citation verification independent.
\end{itemize}

\textbf{Control Calibration}:
\begin{itemize}
\item \textbf{Logging}: Basic inputs/outputs sufficient (attorney review serves as primary control).
\item \textbf{Monitoring}: Minimal (spot-check error rates quarterly).
\item \textbf{Explainability}: Not required (attorney independently verifies legal reasoning).
\item \textbf{Fairness}: Not applicable (legal research is not a protected-class decision).
\item \textbf{Vendor management}: Standard SaaS due diligence (confidentiality, data retention).
\end{itemize}

This is a \emph{low-governance} system because human review (HITL) compensates for other dimensions.

\paragraph{High-Risk Profile Example: Credit Underwriting (HIC + Institutional + Adaptive + Stateful)}
\begin{itemize}
\item \textbf{Autonomy}: HIC—system makes credit decisions autonomously within risk parameters; humans monitor aggregate performance.
\item \textbf{Entity Frame}: Institutional—system acts on behalf of the bank; organizational liability.
\item \textbf{Goal Dynamics}: Adaptive—model adjusts feature weights based on performance feedback.
\item \textbf{Persistence}: Stateful—system maintains applicant history across multiple applications.
\end{itemize}

\textbf{Control Calibration}:
\begin{itemize}
\item \textbf{Logging}: Comprehensive—capture inputs, model version, feature weights, decision rationale, human interventions, state changes. Retention: 25 months (ECOA) + 7 years (litigation hold).
\item \textbf{Monitoring}: Continuous—monthly fairness metrics (80\% rule, disparate impact ratios), data drift detection, approval rate trends by protected class, adverse action reason distributions.
\item \textbf{Explainability}: ECOA-compliant—generate ``principal reasons'' for adverse decisions; validate explanations for faithfulness and completeness.
\item \textbf{Fairness}: Pre-deployment validation + continuous monitoring + revalidation after model updates or detected distribution shift.
\item \textbf{Vendor management}: Enhanced due diligence—model interpretability, fairness validation methodology, update notification, audit rights.
\item \textbf{Human oversight}: Monthly compliance review, quarterly model performance review, board-level annual review.
\item \textbf{Incident response}: Immediate halt if fairness violations detected; root cause analysis; regulator notification if systemic bias found.
\end{itemize}

This is a \emph{high-governance} system because all four dimensions create compounding risk. High autonomy demands strong logging and monitoring. Institutional frame creates organizational liability. Adaptive goals require continuous revalidation. Persistent state creates error compounding risk. The combination necessitates intensive controls.

\begin{keybox}[title={Dimensional Calibration Worksheet}]
When evaluating a new agentic system, assess each dimension:

\begin{enumerate}
\item \textbf{Autonomy}: HITL, HOTL, or HIC?
\item \textbf{Entity Frame}: Human, Hybrid, Machine, or Institutional?
\item \textbf{Goal Dynamics}: Static, Adaptive, or Negotiated?
\item \textbf{Persistence}: Stateless or Stateful?
\end{enumerate}

Use Tables~\ref{tab:agents3-autonomy-calibration} through \ref{tab:agents3-persistence-calibration} to identify baseline controls for each dimension. Then integrate:
\begin{itemize}
\item High autonomy + institutional frame → Strong logging, statistical monitoring, board oversight.
\item Adaptive goals + stateful persistence → Continuous revalidation, state integrity controls.
\item HITL + human frame → Professional responsibility compliance, automation bias mitigation.
\end{itemize}

Dimensional calibration is not a formula—it is a structured reasoning framework that prevents under-protection (``it's just a chatbot'') and over-engineering (``we must apply maximum controls to everything'').
\end{keybox}

Section~\ref{sec:agents3-implementation} operationalizes dimensional calibration through technical architecture and organizational processes. Section~\ref{sec:agents3-examples} demonstrates calibration through worked examples in legal, financial, and audit contexts.

\vspace{0.5em}
\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}
\vspace{0.5em}
