% ============================================================================
% 06-case-studies.tex
% Case Studies: Agents in Practice
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Reference Architectures: Agents in Practice}
\label{sec:agents2-synthesis}

The preceding sections presented agent architecture as components: triggers and channels for how work enters the system (Section~\ref{sec:agents2-triggers}), surfaces for how users interact with agents (Section~\ref{sec:agents2-surfaces}), tools for perception and action, memory for context and learning, planning for strategy and termination, protocols for integration and coordination, and evaluation for quality assurance. This section synthesizes those components through two \textbf{reference architectures}, one legal and one financial, that demonstrate how the pieces fit together.

\begin{highlightbox}[title={Reference Architectures, Not Production Claims}]
The case studies below are \textit{reference architectures}: idealized designs showing how architectural components interconnect. They illustrate target states (what well-designed systems aim to achieve), not claims about what current technology reliably delivers.

As discussed in Section~\ref{sec:agents2-limitations}, agents today achieve under 10\% success on tasks exceeding four hours. Both workflows below describe multi-hour processes. \textbf{Current systems will require substantial human oversight, intervention at failure points, and acceptance of partial automation} rather than the end-to-end execution these architectures describe.

Read these case studies as blueprints for how to structure agent systems, not as descriptions of turnkey solutions available today.
\end{highlightbox}

Each reference architecture walks through a complete agent deployment: the trigger that initiates work, the surface through which users interact, the architecture that processes the task, and the evaluation that validates quality. The goal is not to provide implementation blueprints but to show how architectural choices from Sections~\ref{sec:agents2-triggers}--\ref{sec:agents2-evaluation} manifest in practice.

% ============================================================================
% CASE STUDY 1: CREDIT FACILITY DOCUMENTATION REVIEW
% ============================================================================

\subsection{Case Study: Credit Facility Documentation Review}
\label{sec:agents2-case-legal}

\paragraph{The Scenario}

A mid-market company needs to borrow \$50 million to fund expansion. The lender proposes a floating-rate credit facility, similar to a variable-rate mortgage or credit card, but for a business. The interest rate adjusts periodically based on SOFR (the Secured Overnight Financing Rate) plus a spread. If SOFR rises, the company pays more; if it falls, they pay less.

The borrower's counsel (whether at a law firm or in the company's legal department) must review the 200-page credit agreement and related documents before closing. The stakes are significant: unfavorable terms could cost the company millions over the loan's life, and missed issues could expose counsel to malpractice claims.

\paragraph{Trigger and Surface}

The \textbf{trigger} is a document event: the lender's counsel uploads the draft credit agreement to the deal room. This external feed (Section~\ref{sec:agents2-triggers}) initiates the review workflow automatically; the agent does not wait for someone to remember to start the review.

The \textbf{surface} is document-first (Section~\ref{sec:agents2-surfaces}): the agent produces a structured issues list and summary memo as work products. The associate reviews and edits the document; the partner approves before delivery. Chat interaction is available for follow-up questions, but the primary output is the memo.

\subsubsection{The Task}

The partner assigns the review: ``Go through the credit agreement and flag anything that deviates from market terms or creates unusual risk for the borrower. Pay particular attention to the interest rate mechanics, financial covenants, default triggers, and prepayment provisions. I need a summary memo by Thursday.''

This is a classic legal task: document review requiring both comprehensive coverage (don't miss anything important) and professional judgment (distinguish routine terms from problematic ones). A junior associate might spend 15--20 hours on this review. An agent can accelerate the work while maintaining quality.

\subsubsection{Architecture in Practice}

The agent implements the architectural patterns from Section~\ref{sec:agents2-architecture}. The \textbf{planning system} decomposes the partner's instruction into reviewable components: interest rate provisions, financial covenants, default triggers, prepayment terms, and representations. \textbf{MCP tools} connect the agent to the firm's document management system, precedent database, and legal research platforms, querying Westlaw, iManage, and the deal database through standardized interfaces. \textbf{Memory} maintains context: episodic memory tracks what's been reviewed in this transaction, RAG provides access to prior credit facilities, and semantic memory supplies credit agreement concepts.

The agent follows the \textbf{ReAct loop}: for each section, it reads the provision, compares to precedent and market terms, generates analysis, and records findings. Write actions are controlled: the agent can generate memos and flag issues, but cannot modify the credit agreement or contact opposing counsel. \textbf{Termination} occurs when all sections are reviewed, with escalation to the associate when confidence drops below threshold or provisions fall outside the training distribution.

\subsubsection{Workflow: The Agent Loop in Action}

The review proceeds through the ReAct pattern (Section~\ref{sec:agents2-planning}):

\paragraph{Interest Rate Review} The agent reads the interest rate section: ``Interest accrues at SOFR plus 275 basis points, adjusted quarterly.'' It reasons: this is a standard floating rate structure. It retrieves precedent deals from RAG and finds comparable spreads range from 200--350 basis points for similar credit profiles. It acts: documents that the spread is within market range. It observes an unusual provision: if SOFR becomes unavailable, the lender can select a replacement rate ``in its sole discretion.'' The agent flags this because market-standard fallback provisions typically reference ARRC-recommended replacements, not lender discretion. This is a negotiation point.

\paragraph{Financial Covenant Review} The agent reads the leverage covenant: ``Borrower shall maintain a Total Debt to EBITDA ratio not exceeding 4.0:1.0.'' It retrieves comparable deals and finds this is market for the borrower's credit profile. But it notices the EBITDA definition excludes stock-based compensation and one-time restructuring charges, both borrower-favorable adjustments. It documents these as positive terms. It then reviews the cure provisions and finds the borrower has 30 days to cure covenant violations, shorter than the 45-day standard in the firm's precedent database. It flags this for negotiation.

\paragraph{Default Provisions Review} The agent identifies a cross-default provision: default under any debt instrument exceeding \$1 million triggers default under this facility. It retrieves the borrower's other debt instruments from episodic memory (loaded earlier in the session) and identifies three facilities that could trigger cross-default. It documents the interconnection risk and suggests the threshold should be raised to \$5 million to match market terms.

Throughout, the agent maintains a structured issues list with severity ratings (critical, significant, minor) and recommended responses (negotiate, accept, clarify). When it encounters provisions outside its training distribution, such as an unusual environmental compliance representation, it flags the provision for associate review rather than guessing at analysis.

\subsubsection{Where This Architecture Fails}

Reference architectures should be honest about failure modes. Here are realistic scenarios where the credit review agent falls short:

\textbf{Nuanced definitions escape statistical matching.} The agent flags a ``Change of Control'' provision as standard because it statistically matches the precedent database's patterns. But this deal's definition of ``Control'' excludes the founder's estate and family trusts, a nuance with significant implications for transaction planning that statistical similarity does not capture. The provision matches the pattern but misses the point.

\textbf{Cross-document dependencies break retrieval boundaries.} The credit agreement's EBITDA definition references ``Adjusted EBITDA as defined in the Intercreditor Agreement.'' The agent analyzes the credit agreement's language but doesn't retrieve and parse the separate intercreditor agreement to trace the definition chain. It reports the covenant as ``standard'' when the actual calculation methodology buried in the cross-reference is borrower-unfavorable.

\textbf{Market context requires judgment the agent lacks.} The agent retrieves precedents showing 250--350 basis point spreads for comparable credits. But those precedents are from six months ago; the current credit market has tightened significantly. The 275 basis point spread in this deal is actually aggressive in today's market, a point the agent cannot assess because market conditions are not in the precedent database.

\textbf{Omissions are harder than inclusions.} The agent reviews provisions that exist. But experienced counsel also notice what's \textit{missing}: Where is the equity cure provision that market-standard credit facilities include? The agent finds no precedent for comparison because there's nothing in this document to match against precedents. Missing provisions require a different analytical mode than reviewing existing ones.

These failures illustrate why human review remains essential. The agent accelerates the review but cannot replace professional judgment on nuanced, contextual, or novel issues. The associate validates the agent's work product, catches these limitations, and adds the judgment that turns mechanical analysis into legal advice.

\subsubsection{Protocols: MCP and Human Coordination}

The agent relies on MCP (Section~\ref{sec:agents2-mcp}) to access the firm's legal infrastructure through standardized tool interfaces. To retrieve relevant prior transactions, the agent calls \texttt{search\_precedents(deal\_type="credit facility", size\_range="25M-100M")}, querying the firm's precedent database for comparable deals. When it needs to examine the draft credit agreement itself, \texttt{retrieve\_document(doc\_id="12345")} fetches the document from iManage. For provision-specific analysis, \texttt{compare\_provision(text, provision\_type="leverage covenant")} matches the provision text against market-standard language and returns a deviation analysis highlighting non-standard terms. Finally, to verify current regulatory guidance (particularly important for evolving standards like SOFR transition rules), \texttt{check\_current\_law(topic="SOFR transition", jurisdiction="NY")} searches Westlaw for the latest authority. These MCP-connected tools transform the agent from an isolated reasoning system into one integrated with the firm's document management, precedent knowledge, and legal research capabilities.

For this single-agent deployment, A2A coordination (Section~\ref{sec:agents2-a2a}) is not required because the credit review involves a single specialized agent with a well-defined scope. However, for more complex transactions that span multiple practice areas, A2A enables orchestrated workflows across specialist agents. Consider a leveraged buyout requiring simultaneous review of credit documents, acquisition agreement, and regulatory filings: the orchestrating agent would delegate via A2A to three specialists: a Credit Agent for financing documents, an M\&A Agent for the acquisition agreement, and a Regulatory Agent for HSR filings. Each specialist would use MCP for its domain-specific tool access while A2A coordinates handoffs, tracks dependencies, and assembles the complete analysis. The architecture scales from single-agent tasks to complex multi-agent workflows without requiring fundamental redesign.

Human-in-the-loop integration follows the approval gate pattern (Section~\ref{sec:agents2-hitl}), recognizing that legal work products require professional validation before delivery. The agent produces analysis autonomously, working through the credit agreement sections and generating its issues list and summary memo. But it does not deliver this work product directly to the partner. Instead, the associate receives the agent's draft, reviews the issues list for accuracy and completeness, validates the analysis against their own professional judgment, adds contextual reasoning where the agent flagged uncertainty or encountered provisions outside its training distribution, and refines the memo into client-ready form. High-stakes recommendations, such as advising the client to reject the deal entirely or identifying potential malpractice issues in prior counsel's work, require explicit partner approval before communication to the client.

\subsubsection{Evaluation: Three Layers Applied}

The agent's output is evaluated using the three-layer framework (Section~\ref{sec:agents2-eval-framework}):

\textbf{Layer 1 (Retrieval):} Did the agent find the right precedents? Metrics include retrieval accuracy (percentage of retrieved precedents that are actually comparable), coverage (did it find the firm's most relevant prior deals?), and authority appropriateness (did it prioritize recent deals over outdated ones?). For this review, target: 85\% retrieval accuracy, 90\% coverage of key precedents.

\textbf{Layer 2 (Reasoning):} Did the agent analyze provisions correctly? Metrics include issue identification accuracy (did it flag actual problems?), false positive rate (did it flag routine terms as problematic?), and severity calibration (did ``critical'' issues deserve that rating?). The associate validates by reviewing a sample of flagged and unflagged provisions. Target: 90\% issue identification accuracy, under 20\% false positive rate.

\textbf{Layer 3 (Workflow):} Did the agent complete the review appropriately? Metrics include section coverage (did it review all assigned sections?), deadline compliance (did it finish by Thursday?), escalation appropriateness (did it flag uncertain items rather than guessing?), and output quality (is the memo client-ready after associate review?). Target: 100\% section coverage, all escalations appropriate.

Security evaluation (Section~\ref{sec:agents2-eval-security}) verifies matter isolation (the agent accessed only this client's documents, not other matters), audit trail completeness (all tool calls logged for malpractice defense), and privilege protection (no privileged analysis leaked to unauthorized systems).

\begin{keybox}[title={Credit Deal Review: Architecture Summary}]
\textbf{Task:} Review 200-page credit agreement for borrower-unfavorable terms

\textbf{Tools (MCP):} Document management, precedent database, legal research

\textbf{Memory:} Episodic (this transaction), RAG (prior deals), semantic (credit concepts)

\textbf{Planning:} ReAct for section-by-section review, Plan-Execute for systematic coverage

\textbf{Human-in-the-Loop:} Associate review before partner delivery

\textbf{Evaluation:} L1 (precedent retrieval), L2 (provision analysis), L3 (workflow completion)

\textbf{Target Outcome:} 15-hour task reduced to 3 hours of associate time (agent draft + validation), issues list ready for partner review

\textbf{Current Reality:} Expect 6--8 hours with current technology; agent handles routine provisions while associate focuses on nuanced issues and failure mode catch
\end{keybox}

% ============================================================================
% CASE STUDY 2: EQUITY PORTFOLIO MANAGEMENT
% ============================================================================

\subsection{Case Study: Equity Portfolio Management}
\label{sec:agents2-case-financial}

\paragraph{The Scenario}

A pension fund has entrusted \$500 million in U.S. equities to an asset management firm. Think of it like a 401(k) but at institutional scale; the fund has specific investment objectives, risk constraints, and regulatory requirements that the portfolio manager must honor while seeking returns.

The client's investment policy statement specifies constraints: no single position exceeding 5\% of the portfolio, technology sector limited to 30\%, ESG exclusions (no tobacco, weapons manufacturers, or thermal coal), and tracking error against the S\&P 500 must stay below 3\%. The portfolio manager must continuously monitor compliance, respond to market changes, and rebalance when positions drift outside mandates.

\paragraph{Trigger and Surface}

Unlike the credit review, which is triggered by a discrete document event, portfolio management involves \textbf{multiple trigger types}. Market data feeds provide continuous external triggers (Section~\ref{sec:agents2-external-feeds}): price changes, corporate actions, and news events flow into the system throughout trading hours. Scheduled triggers (Section~\ref{sec:agents2-scheduled}) handle end-of-day reconciliation, weekly drift analysis, and quarterly client reporting. Human prompts arrive when the PM asks ad hoc questions: ``What's our current tech exposure?'' or ``Model the impact of selling half our NVDA position.''

The \textbf{surface} is primarily automation (Section~\ref{sec:agents2-surfaces}): the system monitors continuously and surfaces information only when action is needed. Dashboards show real-time status; alerts appear when positions approach limits; recommendation packages arrive when rebalancing is triggered. Chat interaction is available for PM queries, and quarterly reports use document surfaces. The multi-surface approach matches how the PM actually works: continuous background monitoring with periodic human engagement.

\subsubsection{The Task}

The portfolio manager needs ongoing support: ``Monitor the portfolio for mandate compliance and drift. When positions approach limits or market conditions suggest rebalancing, generate recommendations with supporting analysis. Flag any compliance issues immediately. Prepare quarterly client reports showing performance attribution and risk metrics.''

This is a continuous management task requiring real-time monitoring, periodic rebalancing decisions, and structured reporting, exactly the kind of work where agents can multiply human capacity while humans retain investment judgment.

\subsubsection{Architecture in Practice}

Unlike the discrete credit review, portfolio management involves \textbf{hierarchical planning} with multiple concurrent objectives: compliance monitoring (continuous), drift detection (daily), rebalancing analysis (triggered), and reporting (quarterly). Each workstream has its own termination conditions: compliance monitoring never stops during market hours, while rebalancing terminates when the PM approves or rejects recommendations.

\textbf{MCP tools} connect the agent to Bloomberg for market data, the portfolio management system for holdings and history, compliance databases for restricted lists, and risk systems for VaR calculations. \textbf{Memory} spans market cycles: episodic memory tracks this client's portfolio history and past decisions, RAG provides access to investment research, and learned patterns reflect how the PM has responded to similar situations. When the PM accepted a recommendation six months ago, the agent remembers the reasoning and applies similar logic to current situations.

\textbf{Action} is carefully controlled: agents can generate recommendations, calculate trades, and draft reports, but cannot execute trades directly. All transactions require PM approval, with large trades requiring additional compliance sign-off.

\subsubsection{Workflow: Multi-Agent Coordination}

Portfolio management involves multiple specialized functions. This deployment uses multi-agent orchestration (Section~\ref{sec:agents2-multi-agent}) with A2A coordination:

\paragraph{Monitoring Agent} Runs continuously during market hours. Tracks position sizes against the 5\% single-name limit, calculates sector exposures against the 30\% technology cap, screens holdings against the ESG exclusion list, and monitors tracking error against the benchmark. When any metric approaches its limit (say, a position reaches 4.5\%), the Monitoring Agent creates an A2A Task for the Rebalancing Agent.

\paragraph{Rebalancing Agent} Receives drift alerts from the Monitoring Agent and generates rebalancing recommendations. It retrieves current market conditions via MCP (liquidity, volatility, recent price movements), calculates proposed trades to bring the portfolio within mandates, estimates transaction costs and market impact, and generates a recommendation memo for PM review. The memo includes: current exposure, target exposure, proposed trades, estimated costs, and risk impact.

\paragraph{Compliance Agent} Validates all proposed trades before PM review. It checks the restricted list (no trading in securities where the firm has MNPI), verifies position limits, confirms the trades don't violate client mandates, and ensures regulatory reporting thresholds aren't triggered. If any check fails, the Compliance Agent rejects the recommendation with explanation.

\paragraph{Risk Agent} Calculates portfolio-level risk metrics. Before and after each proposed rebalancing, it computes VaR at 95\% and 99\% confidence, tracking error against benchmark, factor exposures (market, size, value, momentum), and stress test results under various scenarios. The PM uses these metrics to assess whether the rebalancing improves the portfolio's risk profile.

The agents communicate via A2A protocol (Section~\ref{sec:agents2-a2a}). The Monitoring Agent creates a Task describing the drift condition. The Rebalancing Agent returns an Artifact containing the recommendation. The Compliance Agent validates and returns approval or rejection. The Risk Agent provides metrics as supporting Artifacts. The PM reviews the complete package (recommendation, compliance approval, and risk analysis) before authorizing execution.

\subsubsection{Protocols: MCP for Data, A2A for Coordination}

Each specialized agent accesses market and portfolio data through MCP tool interfaces. The foundation is \texttt{get\_positions(portfolio\_id, as\_of\_date)}, which retrieves current holdings from the portfolio management system, providing the agent with the portfolio's composition and historical context. Real-time market information flows through \texttt{get\_market\_data(tickers, fields=["price", "volume", "volatility"])}, connecting to Bloomberg's data feeds for current prices, trading volumes, and volatility metrics that inform rebalancing decisions. Before proposing any trades, the Compliance Agent validates securities using \texttt{check\_restricted\_list(tickers)}, ensuring the firm doesn't trade securities where it possesses material non-public information. Risk analysis depends on \texttt{calculate\_var(portfolio, confidence, horizon)}, which computes Value-at-Risk using the firm's risk engine to quantify downside exposure under various confidence levels and time horizons. Finally, \texttt{get\_esg\_ratings(tickers)} retrieves ESG scores for portfolio securities, enabling the agent to verify compliance with the client's exclusion mandates prohibiting investments in tobacco, weapons manufacturers, or thermal coal producers. Together, these MCP tools provide comprehensive access to the data infrastructure required for continuous portfolio oversight.

A2A protocol coordinates the workflow across the four specialized agents. The process begins with agent discovery: the Monitoring Agent publishes its Agent Card to the system, advertising its capabilities: ``I monitor portfolios for mandate compliance and drift. I produce drift alerts as Tasks for rebalancing analysis.'' This card enables the orchestrating system to route appropriate work to the Monitoring Agent. When the agent detects that technology sector exposure has reached 29\%, approaching the 30\% mandate limit, it creates an A2A Task containing the situation analysis: ``Technology exposure approaching limit. Current: 29\%. Limit: 30\%. Largest tech holdings: AAPL (4.2\%), MSFT (3.8\%), NVDA (2.5\%). Request rebalancing analysis.'' The Rebalancing Agent accepts this Task, conducts its optimization analysis considering current market conditions and transaction costs, and returns an Artifact containing the recommendation memo, in this case proposing to trim the AAPL position by 50 basis points and reallocate the proceeds to healthcare sector holdings. Before this recommendation reaches the portfolio manager, the Compliance Agent receives it for validation, checking restricted lists and mandate compliance, and returns an approval Artifact if all checks pass. Simultaneously, the Risk Agent calculates before-and-after portfolio metrics (VaR, tracking error, factor exposures) and returns its analysis as supporting Artifacts. The orchestrating system assembles all these Artifacts (the rebalancing recommendation, compliance approval, and risk analysis) into a complete decision package for the portfolio manager's review. This A2A coordination enables specialized agents to work in parallel while maintaining proper sequencing and validation gates.

\subsubsection{Multi-Agent Failure Modes}

Multi-agent architectures introduce coordination failures beyond single-agent limitations:

\textbf{Cascading errors across agent boundaries.} The Monitoring Agent incorrectly calculates sector exposure due to a stale price feed; it shows technology at 28\% when actual exposure is 31\%, already over the mandate limit. The Rebalancing Agent receives this incorrect signal and generates recommendations to \textit{increase} technology exposure. The Compliance Agent validates against the same stale data. By the time a human notices, the portfolio has drifted further from mandate compliance. Bad data poisoned the entire chain.

\textbf{Coordination overhead exceeds single-agent simplicity.} The A2A handoffs between the four agents (Monitoring, Rebalancing, Compliance, Risk) introduce latency. Each handoff requires task creation, artifact packaging, and response parsing. For simple rebalancing decisions, a single well-designed agent might outperform the orchestrated specialists because coordination overhead dominates.

\textbf{Debugging complexity when failures span agents.} The PM rejects a recommendation as economically unreasonable. Which agent failed? Was it bad market data (Monitoring's retrieval)? Flawed optimization logic (Rebalancing's reasoning)? Overly conservative risk estimates (Risk's calculations)? Tracing causation across agent boundaries requires sophisticated logging and often manual forensic analysis.

\textbf{Agent disagreement without resolution.} The Rebalancing Agent recommends selling NVDA. The Risk Agent's stress test shows the sale increases portfolio volatility. Neither agent has authority to override the other. The orchestrator presents conflicting recommendations to the PM without synthesis. Multi-agent architectures distribute expertise but may not aggregate it.

\textbf{When to prefer single-agent simplicity:} Multi-agent orchestration suits genuinely parallel, specialized workstreams (M\&A due diligence with distinct legal, financial, and regulatory tracks). For sequential workflows where one agent's output feeds the next, the coordination overhead and failure propagation risks often favor simpler single-agent designs with explicit human checkpoints.

\subsubsection{Evaluation: Continuous Monitoring}

Portfolio management requires continuous evaluation (Section~\ref{sec:agents2-eval-continuous}), not just deployment-time validation:

\textbf{Layer 1 (Data Quality):} Is market data accurate and timely? Metrics include data freshness (latency from exchange to agent), identifier accuracy (correct ticker/CUSIP mapping), and completeness (no missing prices for portfolio securities). Automated monitoring compares agent data against independent feeds. Target: 99.9\% accuracy, sub-second latency during market hours.

\textbf{Layer 2 (Analysis Quality):} Are rebalancing recommendations sound? Metrics include recommendation acceptance rate (what percentage does the PM approve?), post-trade performance (did recommended trades improve the portfolio?), and risk calculation accuracy (do realized volatilities match predictions?). Weekly sampling compares agent analysis to analyst review. Target: 85\% acceptance rate, risk predictions within 10\% of realized.

\textbf{Layer 3 (Mandate Compliance):} Does the portfolio stay within client constraints? Metrics include breach frequency (how often do positions exceed limits?), alert timeliness (how far in advance are approaching limits flagged?), and false alert rate (how many alerts don't require action?). Target: zero mandate breaches, 24-hour advance warning on approaching limits, under 10\% false alerts.

Security evaluation verifies client isolation (this client's portfolio data is not accessible to other client agents), MNPI protection (restricted list checking prevents trading on inside information), and audit completeness (all recommendations and approvals logged for regulatory examination).

The evaluation flywheel (Section~\ref{sec:agents2-eval-flywheel}) operates continuously: recommendations the PM rejects become training cases for improving future recommendations, mandate breaches (if any) trigger root cause analysis and system updates, and quarterly performance reviews compare agent-assisted portfolios against benchmarks.

\begin{keybox}[title={Portfolio Management: Architecture Summary}]
\textbf{Task:} Continuously monitor \$500M equity portfolio for mandate compliance and rebalancing opportunities

\textbf{Tools (MCP):} Market data (Bloomberg), portfolio system, compliance database, risk engine

\textbf{Memory:} Episodic (client history), RAG (investment research), learned (PM preferences)

\textbf{Planning:} Hierarchical coordination of Monitoring, Rebalancing, Compliance, and Risk agents

\textbf{Protocols:} MCP for data access, A2A for multi-agent coordination

\textbf{Human-in-the-Loop:} PM approval for all trades, compliance sign-off for large transactions

\textbf{Evaluation:} Continuous L1 (data), L2 (analysis), L3 (compliance) monitoring with weekly sampling

\textbf{Target Outcome:} Real-time mandate monitoring, proactive rebalancing recommendations, zero compliance breaches

\textbf{Current Reality:} Position monitoring works well; rebalancing recommendations require significant PM judgment; multi-agent coordination remains fragile and requires human oversight at handoff points
\end{keybox}

% ============================================================================
% SYNTHESIS: KEY PRINCIPLES
% ============================================================================

\subsection{Synthesis: Principles Across Domains}
\label{sec:agents2-synthesis-principles}

The two case studies, credit documentation review and portfolio management, differ in domain, time horizon, and complexity. But they share architectural principles that generalize across legal and financial applications:

\textbf{The framework becomes a design checklist.} Both architectures directly implemented the six properties from Part I: planning systems for goals, tools for perception and action, the agent loop for iteration, memory for adaptation, and explicit stopping criteria for termination. Abstract theory became concrete engineering.

\textbf{Tools require appropriate controls.} The credit review agent couldn't modify documents or contact opposing counsel. The portfolio agent couldn't execute trades without PM approval. Tool permissions matched task requirements and risk profiles: read access was permissive, write access was gated.

\textbf{Memory enables context and learning.} Both agents used episodic memory (this transaction, this portfolio), RAG (precedent deals, investment research), and semantic knowledge (legal concepts, financial principles). Memory transformed generic reasoning into domain-competent analysis.

\textbf{Protocols enable integration.} MCP provided standardized tool access in both cases. A2A enabled multi-agent coordination for portfolio management. The protocols from Section~\ref{sec:agents2-protocols} are not optional infrastructure; they are how agents connect to the systems where work actually happens.

\textbf{Humans remain in the loop.} The credit review agent produced recommendations for associate validation. The portfolio agent generated trade proposals for PM approval. Neither agent took consequential action autonomously. Human judgment remained essential for high-stakes decisions.

\textbf{Evaluation is continuous.} Both deployments used three-layer evaluation (retrieval, reasoning, workflow) with metrics appropriate to their domains. Portfolio management added continuous monitoring because the task never ends. Evaluation is not a deployment gate; it is an ongoing quality system.

\begin{highlightbox}[title={From Architecture to Deployment}]
The components from Sections~\ref{sec:agents2-architecture}--\ref{sec:agents2-evaluation} become a deployment checklist:

\textbf{1. Define the work.} What tasks will the agent handle? Credit review? Portfolio monitoring? Research? Drafting? The task determines the architecture.

\textbf{2. Equip with tools.} What systems does the agent need? Legal research, document management, market data, compliance databases? Connect via MCP with appropriate permissions.

\textbf{3. Provide context.} What memory does the agent need? Prior deals, investment research, client history? Build RAG and episodic memory for the domain.

\textbf{4. Design workflows.} How should the agent approach tasks? ReAct for exploration, Plan-Execute for systematic coverage, hierarchical for complex coordination?

\textbf{5. Integrate humans.} Where do humans review and approve? Associate review of legal analysis, PM approval of trades, partner sign-off on client deliverables?

\textbf{6. Measure quality.} How will you know the agent works? Layer 1 retrieval metrics, Layer 2 analysis quality, Layer 3 workflow completion? Build evaluation into the system from day one.

Architecture is the blueprint for systems that work.
\end{highlightbox}

