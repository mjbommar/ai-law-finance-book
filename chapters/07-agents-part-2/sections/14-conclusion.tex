% ============================================================================
% 14-conclusion.tex
% Conclusion: From Architecture to Governance
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Conclusion: From Architecture to Governance}
\label{sec:agents2-conclusion}

% ----------------------------------------------------------------------------
% Opening: Agents as Professional Teams
% ----------------------------------------------------------------------------

AI agents are organized like professional teams. Just as a law firm needs infrastructure---library access, case files, project management, supervision, escalation paths---agents need architecture. The ten questions in Table~\ref{tab:agents2-framework} map these professional structures to technical capabilities: how work arrives (Q1), how instructions become goals (Q2), how information is gathered (Q3) and actions executed (Q4), how context persists (Q5) and work decomposes (Q6), how completion is recognized (Q7) and help requested (Q8), how specialists coordinate (Q9), and how safety is ensured (Q10).

The organizational analogy is a design principle, not merely a metaphor. Agent architecture mirrors professional organization because both solve the same problems: distributing cognitive work, maintaining context, coordinating specialists, ensuring quality, and enabling oversight.

% ----------------------------------------------------------------------------
% What This Lets You Do
% ----------------------------------------------------------------------------

\subsection{What This Lets You Do}
\label{sec:agents2-conclusion-enables}

\textbf{Evaluate vendor claims critically}: When a vendor says their agent ``handles legal research,'' you know to ask: What triggers initiate research? How does it understand the research question? What tools provide information? How does it know when research is complete? What escalation paths exist? The ten questions provide an evaluation framework.

\textbf{Participate in procurement decisions}: You can assess whether a proposed agent system meets your organization's requirements. Does it enforce matter isolation? Does it maintain audit trails? Does it integrate with your approval workflows? You have vocabulary to specify requirements.

\textbf{Design governance that maps to architecture}: You understand that governance is not separate from architecture; it is enabled by architecture. Audit logging is an architectural choice that enables compliance review. Approval gates are architectural choices that enable human oversight. You can design systems where governance is built in, not bolted on.

\textbf{Communicate with technical teams}: You can describe what you need in terms developers understand. ``I need perception tools for these three databases, action tools behind approval gates for these two operations, escalation triggers when confidence drops below threshold.'' Shared vocabulary enables collaboration.

\begin{keybox}[title={Architecture Enables Governance}]
Every architectural choice has governance implications:

\begin{itemize}[nosep]
\item Trigger logging enables audit of what initiated agent action
\item Intent extraction enables review of what the agent understood
\item Perception controls enable data governance
\item Action gates enable approval workflows
\item Memory isolation enables confidentiality protection
\item Planning budgets enable bounded operation
\item Termination criteria enable completion verification
\item Escalation paths enable human oversight
\item Delegation contracts enable accountability
\end{itemize}

\textbf{You cannot govern what you did not architect.}
\end{keybox}

% ----------------------------------------------------------------------------
% Current Limitations
% ----------------------------------------------------------------------------

\subsection{Current Limitations}
\label{sec:agents2-conclusion-limitations}

Honest assessment of current capabilities: agents exhibit a reliability cliff (\Cref{sec:agents2-reliability}) with near-perfect success on tasks under 4 minutes but under 10\% on tasks over 4 hours; they excel at retrieval and systematic execution but struggle with nuanced judgment and novel situations; production systems fail unpredictably due to API changes, authentication expiration, and format variations; and multi-step workflows compound error probabilities at each step. These limitations are not permanent, but they are real today. Design systems that deliver value despite limitations, not systems that assume limitations do not exist.

% ----------------------------------------------------------------------------
% Essential Resources
% ----------------------------------------------------------------------------

\subsection{Essential Resources}
\label{sec:agents2-conclusion-resources}

This chapter introduced ten fundamental questions that every agent designer must answer. Essential resources for moving from concepts to deployment:

\textbf{Security}: Implement the five foundational controls detailed in \Cref{sec:agents2-security-essentials} (input separation, output validation, least privilege, audit logging, matter/client isolation) before any production deployment. The OWASP LLM Top 10 provides vulnerability taxonomy for language model applications; the NIST AI Risk Management Framework offers lifecycle guidance organized into four functions (Govern, Map, Measure, Manage) that align with enterprise risk management practices.

\textbf{Protocols and Standards}: The Model Context Protocol (MCP) standardizes agent-to-tool communication (\Cref{sec:agents2-perception}, \Cref{sec:agents2-action}) and is production-ready as of late 2025 with thousands of available servers. The Agent-to-Agent Protocol (A2A) standardizes agent-to-agent coordination (\Cref{sec:agents2-delegation}) and is maturing under the Linux Foundation; suitable for internal multi-agent coordination, though cross-vendor interoperability is still emerging.

\textbf{Research Foundations}: For deeper theoretical grounding, see Xi et al. (2023) on agent architecture and design patterns \parencite{xi2023rise}, Yao et al. (2022) on the ReAct reasoning-action loop \parencite{yao2022react}, and Park et al. (2023) on memory architecture \parencite{park2023generative}. For evaluation, use LegalBench (162 legal reasoning tasks) \parencite{guha2023legalbench} and VLAIR (legal AI performance against lawyer baselines) \parencite{bommarito2025vlair}.

\textbf{Learning Paths}: \textit{Legal professionals} should start with narrowly scoped pilots focused on evaluation criteria (accuracy, audit trails, fail-safe behaviors) and validate outputs against their own analysis. \textit{Financial professionals} should begin with read-only monitoring tasks, integrating with existing systems (Bloomberg terminals, portfolio management, compliance databases) before introducing advisory workflows. \textit{Technical practitioners} should build a simple research agent using a framework (LangChain, LlamaIndex, CrewAI), add memory and evaluation, then build an MCP server for a real data source with monitoring and audit logging. \textit{For everyone}: implement security controls from the beginning---retrofitting is expensive.

\textbf{Staying Current}: Technology advances quickly, regulation is emerging, and security risks evolve continuously. Monitor protocol specifications on GitHub, follow research venues (NeurIPS, ICML, ACL), track framework release notes. Legal practitioners should monitor ABA ethics opinions (particularly Formal Opinion 512 on supervision \parencite{aba-formal-opinion-512}); financial practitioners should monitor SEC guidance, FINRA communications, and prudential regulators' guidance on model risk management. Subscribe to OWASP LLM Top 10 updates and follow security researchers specializing in language model vulnerabilities.

\begin{highlightbox}[title={Temporal Warning}]
Resources accurate as of late 2025 may not reflect subsequent developments. Protocol specifications evolve, regulatory frameworks develop, security vulnerabilities emerge. Verify currency of all technical and regulatory references before relying on them for production deployment decisions.
\end{highlightbox}

% ----------------------------------------------------------------------------
% From Architecture to Governance
% ----------------------------------------------------------------------------

\subsection{From Architecture to Governance}
\label{sec:agents2-conclusion-bridge}

This chapter answered: \textit{How do you build an agent?}

The ten questions provide architectural foundations. Each question maps to implementation choices. Each implementation choice enables---or forecloses---governance options.

The next chapter answers: \textit{How do you govern an agent?}

Where this chapter focused on \textit{capability}---what agents can do---the next focuses on \textit{control}---ensuring agents do what they should, and only what they should. The five-layer governance stack (legal, model, system, process, culture) provides the framework. Dimensional controls (autonomy, persistence, goal dynamics) calibrate oversight. Implementation patterns translate principles into practice.

You understand agent architecture. Now you can govern it.

\vspace{1em}
\begin{center}
\textit{You cannot govern what you do not understand.}

\textit{This chapter has provided that understanding.}
\end{center}
