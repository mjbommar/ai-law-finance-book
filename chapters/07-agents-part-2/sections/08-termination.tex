% ============================================================================
% 08-termination.tex
% Q7: How Does an Agent Know When It's Done?
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{How Does an Agent Know When It's Done?}
\label{sec:agents2-termination}

% ----------------------------------------------------------------------------
% Opening: Q7 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

Every professional learns to recognize completion. The research memo is done when you've found sufficient authority and synthesized it coherently. The due diligence is done when you've reviewed all material documents and reported findings. The trade is done when the order executes and settles. Knowing when work is complete---and when it isn't---distinguishes effective professionals from those who over-research or under-deliver.

Agents face the same challenge. Without explicit termination conditions, agents can run indefinitely: searching one more database, trying one more approach, refining one more time. We call this the ``runaway associate'' problem: you asked for two relevant cases, the associate gives you fifty because they did not know when enough was enough.

\begin{definitionbox}[title={Termination}]
\keyterm{Termination} conditions define when an agent should stop executing. Three outcomes are possible:

\textbf{Success}: The goal is achieved. Deliver the result.

\textbf{Failure}: The goal cannot be achieved. Report why and stop.

\textbf{Escalation}: The agent cannot determine success or failure. Transfer to human judgment.

Termination implements the ``T'' in the GPA+IAT framework from Part I. Without termination, agents lack the sixth property that distinguishes agentic systems from runaway processes.
\end{definitionbox}

% ----------------------------------------------------------------------------
% Termination Condition Categories
% ----------------------------------------------------------------------------

\subsection{Termination Condition Categories}
\label{sec:agents2-termination-categories}

Five categories of termination conditions bound agent execution:

\subsubsection{Success Conditions}

The most obvious termination: the goal is achieved, return the result.

\textbf{Completeness criteria}: Have all required elements been produced? For document review: all provisions on the checklist have been analyzed. For research: the legal question has been answered with supporting authority. For portfolio rebalancing: allocations match targets within tolerance.

\textbf{Quality thresholds}: Is the output good enough? For a research memo: are conclusions supported by binding authority? For a risk assessment: have material risks been identified and analyzed? Quality thresholds often require human judgment---the agent can check completeness but may not assess quality reliably.

\textbf{Convergence criteria}: Has the agent stopped learning new information? If the last three searches returned no new relevant authority, the research may be saturated. If the last five portfolio adjustments produced diminishing improvement, optimization may have converged.

\subsubsection{Resource Budgets}

Hard limits prevent runaway execution by capping consumption across multiple dimensions. Token budgets stop execution after a specified threshold---say, 50,000 tokens---preventing expensive reasoning loops that would otherwise continue indefinitely. Time budgets enforce deadlines by terminating after a fixed duration, ensuring that research tasks do not consume an entire day when an hour was expected. Iteration budgets cap tool calls, stopping after twenty searches to prevent infinite loops where the agent keeps trying slightly different queries without progress. Cost budgets provide the most direct control, halting execution after spending a dollar amount (perhaps \$5 in API calls) to limit financial exposure.

These budgets cascade and interact: a task might hit its time limit before exhausting its token budget, or vice versa. Budget exhaustion does not mean failure; partial results may still be valuable. But the agent must stop and report rather than continuing indefinitely.

\subsubsection{Confidence Thresholds}

Confidence thresholds gate actions on certainty, creating a decision boundary between autonomous execution and human review. When confidence is high, the agent delivers its answer. When confidence drops below a calibrated threshold---perhaps 80\%---the agent stops and escalates rather than proceeding with uncertain information. This mirrors how associates should work: ``I'm not confident this is right. Let me ask the partner before proceeding.''

Calibrating these thresholds is challenging. Agents may be overconfident, proceeding when they should escalate, or underconfident, escalating unnecessarily and providing no value. Effective calibration requires testing against known outcomes, comparing agent confidence to actual accuracy, and adjusting thresholds until the agent escalates at appropriate uncertainty levels.

\subsubsection{Error Conditions}

Agents must recognize when things are going wrong and terminate rather than compounding errors. Repeated tool failures signal infrastructure problems: if Westlaw times out three times consecutively, the agent should stop rather than retrying indefinitely while consuming budget. Inconsistent data---a revenue figure in the 10-K that does not match the earnings release---requires human investigation, not agent guesswork about which source is correct. Constraint violations demand immediate termination: if the planned action would exceed position limits or breach confidentiality, the agent must stop before acting, not after.

Some tasks prove impossible as specified. Analysis may reveal conflicting requirements, missing prerequisites, or logical impossibilities. In these cases, the agent should report the impossibility honestly rather than proceeding with a compromised approach that satisfies the letter of the instruction while violating its spirit.

\subsubsection{Escalation Triggers}

Some situations require human judgment regardless of whether the task has succeeded, failed, or consumed its budget. Novel situations that do not match training patterns need human expertise to navigate. High-stakes decisions warrant human approval even when the agent is confident, because the consequences of error justify the overhead of review. Authority boundaries define what the agent can do autonomously; actions beyond those boundaries require escalation by design, not because something went wrong.

\Cref{sec:agents2-escalation} provides comprehensive treatment of when and how to escalate.

% ----------------------------------------------------------------------------
% Explicit Success Criteria
% ----------------------------------------------------------------------------

\subsection{Defining Success Criteria}
\label{sec:agents2-success-criteria}

Vague goals produce unclear termination. ``Research the statute of limitations'' could mean finding one relevant case or exhaustively surveying all circuits. Effective success criteria take several forms, each providing a different signal that work is complete.

Completeness checklists enumerate what must be delivered. For credit agreement review, the checklist might require identifying all financial covenants, comparing them to market terms, flagging provisions that differ from the firm's template, and summarizing material risks. The agent terminates when all checklist items are complete. Sufficiency thresholds define ``enough'' without requiring exhaustive coverage. For case research, sufficiency might mean finding at least three on-point circuit opinions, or if circuits conflict, identifying the leading case from each side. The agent knows when sufficiency is reached without searching every database.

Convergence criteria recognize diminishing returns. If three consecutive searches return no new relevant authority, the research may be saturated; further searching is unlikely to yield value. Deliverable specifications define the output format---a two-page memo with executive summary, analysis, and recommendation---so the agent knows what success looks like, not just what success requires.

Consider how experienced attorneys instruct associates: ``If you find clear Ninth Circuit authority, you're done. If the circuits are split, map the split and recommend which approach applies to our facts. If you can't find binding authority after two hours, come talk to me.'' Agents need the same clarity.

% ----------------------------------------------------------------------------
% Failure Recognition
% ----------------------------------------------------------------------------

\subsection{Recognizing Failure}
\label{sec:agents2-failure-recognition}

Not every task succeeds, and agents must recognize failure and report it honestly. Negative results are still results: ``I searched all major databases and found no authority on point'' is a valid finding that the attorney needs. The absence of authority is information. Agents should report negative results explicitly rather than continuing to search indefinitely in hope of finding something.

When failure occurs, diagnostic reporting explains what was attempted and why it failed. A useful failure report might state: ``I searched Westlaw, Lexis, and Bloomberg Law using [specific queries]. Zero results suggest either the issue is novel or the search terms are wrong. Recommend manual review of secondary sources or consultation with practice group expert.'' This gives the human actionable information rather than a bare ``task failed'' message.

Partial completion should be acknowledged honestly. If the agent completed analysis of Articles 1-4 before a tool failure, it should report what was accomplished and what remains: ``Articles 5-8 remain unanalyzed.'' Partial results may still be valuable, and preserving them prevents wasted effort. Where possible, root cause identification helps humans decide next steps: Was this a tool failure that will resolve itself? Impossible requirements that need rethinking? Insufficient information that requires additional input? The agent's diagnosis informs the human's response.

% ----------------------------------------------------------------------------
% Guardrails and Loop Detection
% ----------------------------------------------------------------------------

\subsection{Guardrails and Loop Detection}
\label{sec:agents2-loop-detection}

Even with well-defined termination conditions, agents can get stuck in unproductive loops---searching repeatedly without progress, rephrasing queries slightly, finding nothing, rephrasing again. Multiple mechanisms detect and prevent these loops.

Step limits provide the simplest guardrail: after N steps, stop and require human approval to continue. This prevents unbounded execution regardless of what the agent thinks it is accomplishing. Progress detection monitors whether recent actions produced value: if the last five actions yielded no new information, the agent may be stuck and should trigger reflection or escalation. Reflection steps build self-assessment into the workflow, periodically asking meta-questions: ``Am I making progress toward the goal? Have my recent actions been productive? Should I try a different approach or escalate?''

External watchdogs monitor agent behavior from outside the agent's own reasoning. If the same tool is called repeatedly with nearly identical parameters, an external system can recognize the loop pattern and intervene. Meta-policies encode loop detection rules directly: calling the same tool with the same parameters more than three times is probably a loop, so stop and escalate.

Without loop detection, agents will eventually get stuck in production. The question is not whether it will happen, but whether you will detect it when it does.

% ----------------------------------------------------------------------------
% The Reliability Cliff
% ----------------------------------------------------------------------------

\subsection{The Reliability Cliff}
\label{sec:agents2-reliability}

Independent benchmarking reveals a sharp reliability boundary. METR (Model Evaluation and Threat Research) tested agents across standardized task suites varying in duration and complexity. The results:

\begin{keybox}[title={The Four-Minute Cliff}]
METR's 2025 study found that agents achieve \textbf{near-perfect success on tasks under 4 minutes}, but \textbf{under 10\% success on tasks over 4 hours} \parencite{metr2024autonomy}.

This gap---from near-100\% to under-10\%---defines the current boundary between reliable and unreliable agent deployment.

\textbf{Implication}: Decompose tasks aggressively. Keep individual agent tasks short. Insert human checkpoints between phases. Don't expect autonomous completion of multi-hour workflows.
\end{keybox}

\input{figures/fig-reliability-cliff}

The reliability cliff has several causes, each contributing to the dramatic drop in success rates as task duration increases. Compounding errors are perhaps the most fundamental: each step introduces error probability, and these probabilities multiply. A 95\%-accurate retrieval step followed by 90\%-accurate reasoning followed by 85\%-accurate action yields roughly 73\% end-to-end accuracy, before accounting for sequencing decisions. Over a four-hour task with dozens of steps, these compounding errors accumulate into near-certain failure.

Planning fragility compounds the problem. Agents frequently select suboptimal tool sequences, get stuck in loops, or fail to recognize when their approach is not working. A human would step back and reconsider; agents often persist with failing strategies. Integration brittleness adds another failure mode: tool APIs return unexpected formats, authentication tokens expire, rate limits trigger. Each integration point is a potential failure mode, and complex tasks touch many integration points.

Design for this reality. Decompose aggressively. Validate at checkpoints. Assume agents will fail and design for graceful degradation.

% ----------------------------------------------------------------------------
% Graceful Degradation
% ----------------------------------------------------------------------------

\subsection{Graceful Degradation}
\label{sec:agents2-graceful-degradation}

When termination occurs before full completion, agents should degrade gracefully rather than failing completely. Tiered outputs provide value at every budget level: at 20\% budget, deliver the controlling statute with citation; at 60\% budget, add key holdings; at 100\% budget, deliver comprehensive analysis. Partial results are better than nothing, and users can decide whether partial results suffice or warrant additional investment.

Progress preservation saves intermediate state so humans can resume where the agent stopped. If the agent analyzed 30 of 50 contracts before budget exhaustion, that work should not be lost when execution terminates. Clear status reporting communicates exactly where things stand: ``Completed 60\% of task. Remaining: Articles 5-8 unreviewed due to budget exhaustion. Findings so far: [summary].'' The human knows what was accomplished and what remains.

Beyond reporting status, agents should recommend next steps when possible. ``Recommend allocating additional 30 minutes to complete review'' gives the human a concrete decision to make. ``Remaining work is routine; recommend proceeding with partial findings'' helps humans assess whether additional effort is worthwhile. The goal is a handoff that enables informed human decision-making, not a handoff that forces the human to start over.

% ----------------------------------------------------------------------------
% Evaluating Termination
% ----------------------------------------------------------------------------

\subsection{Evaluating Termination Capabilities}
\label{sec:agents2-termination-eval}

When evaluating agent systems, assess termination capabilities against six criteria that distinguish robust systems from fragile ones.

Success criteria clarity determines whether termination is predictable. Are termination conditions explicit? Can you predict when the agent will stop? Systems with vague or implicit termination conditions produce unpredictable behavior. Budget enforcement determines whether limits actually constrain execution. Test by setting tight budgets and verifying the agent actually stops; some systems log budget exhaustion but continue anyway. Loop detection determines whether the agent recognizes when it is stuck. Test with impossible tasks or unavailable tools; a system without loop detection will spin indefinitely.

Failure reporting determines whether failures are actionable. When tasks fail, does the agent explain why? A bare ``task failed'' message forces the human to investigate; a detailed explanation enables informed response. Graceful degradation determines whether early termination preserves value. When stopped early, does the agent preserve partial results? Is status clearly reported? Escalation integration determines whether handoffs work smoothly. When termination requires human judgment, does the human receive sufficient context to decide? A handoff that requires the human to start from scratch is a handoff that failed.

% ----------------------------------------------------------------------------
% Connection to Other Questions
% ----------------------------------------------------------------------------

\subsection{From Termination to Escalation}
\label{sec:agents2-termination-escalation}

Termination defines when agents stop, but not all stopping is the same. Success termination means the task is complete; deliver the results. Failure termination means the task is impossible; report why. Escalation termination means the agent cannot determine success or failure on its own; human judgment is required.

The third category is critical. An agent might complete its search and find conflicting authority, leaving it unable to determine whether the research question has been answered. It might approach a decision that exceeds its authorization. It might recognize that the situation is novel in ways that make its confidence unreliable. In each case, the right response is not to terminate with a result or a failure, but to terminate with a request for human input.

\Cref{sec:agents2-escalation} examines this closely related question: when should an agent stop autonomous operation and ask for human help? Termination and escalation together define the boundaries of autonomous execution. Without termination, agents run forever. Without escalation, agents exceed authority. These boundaries make agent deployment safe---or at least safer.
