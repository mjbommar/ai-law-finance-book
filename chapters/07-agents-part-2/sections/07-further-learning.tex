% ============================================================================
% 07-further-learning.tex
% Further Learning
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Further Learning}
\label{sec:agents2-further}

\subsection{Research Foundations}

For readers seeking deeper engagement with agent systems, several resources provide essential foundations. Xi et al.'s ``Rise and Potential of LLM Based Agents'' (2023) \parencite{xi2023rise} offers the most comprehensive architecture survey, systematically covering design patterns, memory, planning, and tool use. The credit facility review case study demonstrated ReAct in action: perceive the provision, reason about market terms, act by generating analysis, observe results, repeat. Yao et al.'s ``ReAct'' paper (2022) \parencite{yao2022react} introduced this reasoning-action loop. The portfolio management case study implemented memory patterns from Park et al.'s ``Generative Agents'' (2023) \parencite{park2023generative}: episodic memory of client decisions, RAG access to investment research, learned preferences from PM feedback.

For domain-specific evaluation, LegalBench provides 162 legal reasoning tasks \parencite{legalbench2023}, FinQA tests financial question answering \parencite{finqa2021}, and VLAIR compares legal AI against lawyer baselines \parencite{vlair2025}. These benchmarks help measure whether agents perform at professional standards.

\subsection{Security Essentials}

Security is not optional. Both case studies required security controls: matter isolation and privilege protection for credit review, client isolation and MNPI protection for portfolio management.

\begin{keybox}[title={Security Controls for Regulated Practice}]
\begin{itemize}[nosep]
\item \textbf{Input separation}: Isolate user inputs from system prompts
\item \textbf{Output validation}: Verify agent outputs before execution
\item \textbf{Least privilege}: Grant minimum necessary tool access
\item \textbf{Audit logging}: Maintain comprehensive action logs
\item \textbf{Matter/client isolation}: Enforce confidentiality boundaries
\end{itemize}
\end{keybox}

The OWASP LLM Top 10 provides vulnerability taxonomy for LLM applications. The NIST AI Risk Management Framework offers lifecycle guidance for identifying and mitigating AI risks.

\subsection{Protocols and Standards}

The Model Context Protocol (MCP) is production-ready. If you are building agents that integrate with multiple data sources---the pattern in both case studies---implementing MCP servers makes your architecture modular. You can swap implementations without changing agent code, which matters when data sources or regulatory requirements change.

The Agent-to-Agent Protocol (A2A) is maturing under the Linux Foundation. It standardizes how agents exchange tasks and artifacts, as shown in the portfolio management workflow. As of November 2025, A2A is suitable for systems where you control all agents; cross-vendor interoperability is still emerging. Use A2A for internal multi-agent coordination, but monitor the standard's evolution before depending on it for external integration.

\subsection{Learning Paths}

\paragraph{For Legal Professionals} Focus on evaluation criteria: accuracy on domain tasks, audit trail completeness, fail-safe behaviors. Start with narrowly scoped pilots where quality can be validated---contract review like the credit facility case study is ideal. Your domain expertise makes you well-positioned to define acceptable performance thresholds. Key question: Would you accept this output from a third-year associate?

\paragraph{For Financial Professionals} Focus on integration with existing workflows---Bloomberg, portfolio systems, compliance databases. Validate agent outputs against your own analysis before relying on them. The portfolio management case study illustrates the pattern: agents monitor and recommend, humans approve and execute. Key question: Does the agent's recommendation match what you would conclude given the same data?

\paragraph{For Technical Practitioners} Start with a framework tutorial, then build a simple research agent. Add memory, implement evaluation, then build an MCP server for a real data source. This progression takes you from concepts to production-ready skills. For deeper expertise, study the ReAct paper, build custom orchestration logic, and implement comprehensive observability.

\paragraph{For Everyone} Build agents regularly to internalize patterns. Study production code from open-source projects. Implement security controls from the beginning. The field evolves rapidly; sustained engagement is essential for responsible adoption.

\subsection{Staying Current}

Technology advances quickly: new model capabilities, improved reasoning techniques, evolving protocol specifications. Regulation is emerging: the EU AI Act phases in through 2027, US frameworks continue developing. Security risks emerge as researchers discover new vulnerabilities.

Before deploying any agent system, verify current protocol specifications, review recent security advisories, check applicable regulatory requirements, and consult professional ethics guidance. Resources accurate as of November 2025 may not reflect subsequent developments.

