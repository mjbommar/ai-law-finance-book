% ============================================================================
% 12-synthesis.tex
% Synthesis: Reference Architectures
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Synthesis: Reference Architectures}
\label{sec:agents2-synthesis}

% ----------------------------------------------------------------------------
% Opening
% ----------------------------------------------------------------------------

The previous sections examined each of the ten questions in isolation. This section shows how they work together in complete deployments. Two reference architectures demonstrate the full framework while honestly acknowledging current limitations: one legal, one financial.

\begin{keybox}[title={Reference Architectures, Not Production Claims}]
These architectures illustrate how components fit together as \textit{design targets}, not claims about current reliability. The reliability cliff (\Cref{sec:agents2-reliability}) constrains what agentic systems achieve today; these multi-hour workflows require extensive human oversight. Read as ``how you would design it'' not ``how it works today.''
\end{keybox}

% ----------------------------------------------------------------------------
% Case Study 1: Credit Facility Review
% ----------------------------------------------------------------------------

\subsection{Case Study: Credit Facility Documentation Review}
\label{sec:agents2-case-legal}

A corporate client is borrowing \$500 million under a senior secured revolving credit facility. The law firm represents the borrower. The partner assigns the matter: ``Review the draft credit agreement and identify provisions that differ materially from market terms or our standard positions.''

This is a document review task that would traditionally require 8--12 hours of senior associate time. The goal is not to replace the associate but to accelerate the review and ensure comprehensive coverage. The ten-question framework guides every design choice.

\textbf{Q1 (Triggers):} Work enters via document upload to the deal room and partner assignment. The trigger is explicit: new document plus assignment.

\textbf{Q2 (Intent):} The agent extracts intent: document review task, borrower perspective, focus on material deviations. Implicit constraints include confidentiality (privileged work product) and deadline (closing in two weeks).

\textbf{Q3 (Perception):} The agent uses MCP Resources to access the draft agreement (document management), the firm's template (precedent database), and market terms data (external database). Access is read-only.

\textbf{Q4 (Action):} Action tools are limited to document annotation (internal markup) and memo generation (work product creation). No external actions like filing or communication are permitted.

\textbf{Q5 (Memory):} Episodic memory tracks analysis progress (sections reviewed, issues identified). RAG provides access to prior credit agreement memos. Matter isolation ensures this work doesn't access unrelated client information.

\textbf{Q6 (Planning):} Plan-Execute pattern. The agent creates a section-by-section review plan based on the table of contents. It executes systematically through financial covenants, events of default, and representations.

\textbf{Q7 (Termination):} Success criteria: all material sections reviewed, issues identified, draft memo produced. Budget: token limit, time limit, iteration limit. Checkpoint after initial scan to confirm scope.

\textbf{Q8 (Escalation):} Escalate on ambiguous provisions requiring legal judgment, potential conflicts, or issues affecting deal viability. Use the Human-as-Tool pattern for partner input on materiality thresholds.

\textbf{Q9 (Delegation):} Single-agent architecture. Multi-agent would be appropriate if combined with separate research or financial modeling workstreams.

\textbf{Q10 (Governance):} Audit logging of all document access and analysis steps. Privilege protection enforced. Work product clearly marked as AI-assisted for attorney review.

The agent proceeds systematically:
\begin{enumerate}[nosep]
    \item Partner assigns matter; agent receives trigger.
    \item Agent retrieves credit agreement, template, market terms.
    \item Agent creates review plan: 15 sections.
    \item Agent analyzes Section 1 (Definitions): identifies unusual defined terms.
    \item Agent continues through financial covenants: flags leverage ratio differing from market.
    \item ... [continues through all sections]
    \item Agent compiles findings into issues list and draft memo.
    \item Agent presents to associate for review.
    \item Associate reviews, adds context, and escalates significant issues to partner.
\end{enumerate}

Even well-designed agents fail. The agent may miss nuanced definitions where a term has been subtly modified. Human review catches these subtleties. Cross-document dependencies present another risk; if the agent does not analyze referenced schedules, issues may be missed. The most dangerous failures are omissions. Mitigation requires checkpoint review and partner validation.

% ----------------------------------------------------------------------------
% Case Study 2: Equity Portfolio Management
% ----------------------------------------------------------------------------

\subsection{Case Study: Equity Portfolio Management}
\label{sec:agents2-case-financial}

An investment adviser manages a \$200 million equity portfolio for institutional clients. The portfolio manager (PM) wants continuous monitoring with agent assistance for rebalancing analysis, compliance checking, and research synthesis.

This is a continuous monitoring task, not a one-time analysis. The goal is augmentation, not autonomous trading. The ten-question framework shapes this multi-agent architecture.

\textbf{Q1 (Triggers):} Multiple triggers: market data feeds (price movements), scheduled jobs (daily compliance check), human prompts (PM requests), and escalation events (limit breaches).

\textbf{Q2 (Intent):} Varies by trigger. Price alert: assess significance. Scheduled rebalance: generate trade list if needed. PM query: answer specific question.

\textbf{Q3 (Perception):} MCP Resources access market data, portfolio data, research, and compliance data. Read-only access to trading systems.

\textbf{Q4 (Action):} The agent generates recommendations and reports. Trade execution requires PM approval (approval gate). No autonomous trading.

\textbf{Q5 (Memory):} Episodic memory tracks recent analysis and PM decisions. RAG accesses research archive. Client isolation ensures segregation.

\textbf{Q6 (Planning):} ReAct pattern for ad hoc analysis. Plan-Execute for scheduled tasks. Hierarchical for comprehensive reviews.

\textbf{Q7 (Termination):} Monitoring is continuous. Analysis completes when the question is answered. Rebalancing completes when the trade list is generated and approved.

\textbf{Q8 (Escalation):} Escalate on positions approaching limits, unusual market conditions, conflicting signals, or any trade recommendation.

\textbf{Q9 (Delegation):} Multi-agent architecture. Monitoring Agent watches market data. Research Agent synthesizes reports. Compliance Agent checks guidelines. Rebalancing Agent generates recommendations. PM Agent orchestrates.

\textbf{Q10 (Governance):} Comprehensive audit trail. Fiduciary duty documentation. Compliance monitoring. MNPI controls.

Multiple agents coordinate:
\begin{enumerate}[nosep]
    \item Monitoring Agent detects: ``Tech sector up 3\%; allocation 35\% vs 25\% target.''
    \item Monitoring Agent triggers Rebalancing Agent.
    \item Rebalancing Agent queries Compliance Agent: ``Check proposed sales against guidelines.''
    \item Compliance Agent confirms: trades within limits.
    \item Rebalancing Agent queries Research Agent: ``Any negative research?''
    \item Research Agent returns: ``One position has earnings next week.''
    \item Rebalancing Agent adjusts: defer that sale.
    \item Rebalancing Agent presents recommendation to PM Agent.
    \item PM Agent formats for human review.
    \item Human PM reviews, approves, and authorizes execution.
    \item Execution Agent places orders via OMS.
\end{enumerate}

Coordination introduces failure vectors. Cascading errors occur if the Monitoring Agent misinterprets data. Debugging complexity increases with multiple agents. Mitigation requires comprehensive logging and clear escalation protocols.

% ----------------------------------------------------------------------------
% Synthesis Principles
% ----------------------------------------------------------------------------

\subsection{Synthesis: Principles Across Domains}
\label{sec:agents2-synthesis-principles}

Both case studies illustrate common principles:

\textbf{Decomposition is essential.} Neither workflow attempts end-to-end autonomous completion. Tasks are broken into manageable steps with human checkpoints.

\textbf{Human-in-the-loop oversight is the norm.} Attorney review and PM approval are structural requirements. Agents augment professional judgment; they do not replace it.

\textbf{Memory enables continuity.} Both rely on episodic memory and RAG. Without memory, every interaction starts fresh.

\textbf{Isolation is non-negotiable.} Matter isolation (legal) and client isolation (financial) are architectural requirements.

\textbf{Failure modes are predictable.} Nuanced judgment failures, omissions, and cascading errors appear in both domains. Design for these failures explicitly.

\textbf{Governance is pervasive.} Every architectural choice has governance implications. Audit trails, approval gates, and escalation paths are foundational.

\textbf{Expectation setting is part of the work.} In regulated professional services, it is not enough for an agent to be helpful; stakeholders must understand what it did, what it did not do, and what remains uncertain. Treat this as a deliverable requirement: outputs should clearly state scope, as-of dates, sources accessed, and any confidence caveats. Where outputs may be relied on externally (clients, counterparties, regulators), require explicit human review and use the system's logs to support disclosure and defensible documentation.

\textbf{The reliability cliff remains binding.} Even well-designed systems degrade sharply as tasks extend from minutes to hours (\Cref{sec:agents2-reliability}). Architect for short, check-pointed work with graceful partial completion and escalation, not for uninterrupted autonomy.

Chapter~8 (Agents Part III: Governing Agents) takes these themes from principles to controls: how to calibrate control intensity, implement audit and retention, and define accountability and disclosure practices that match professional obligations.

% ----------------------------------------------------------------------------
% Framework Completion
% ----------------------------------------------------------------------------

\subsection{Framework Completion Checklist}
\label{sec:agents2-checklist}

Before deploying any agentic system, verify that all ten questions have been answered:

\begin{keybox}[title={Ten-Question Deployment Checklist}, breakable=false]
\begin{enumerate}[nosep]
\item[$\square$] \textbf{Triggers:} How does work enter? Are all trigger types covered? Is there audit logging?
\item[$\square$] \textbf{Intent:} How is intent extracted? What happens with ambiguity? Are constraints identified?
\item[$\square$] \textbf{Perception:} What tools provide information? Is access controlled? Is provenance tracked?
\item[$\square$] \textbf{Action:} What can the agent do? Are irreversible actions gated? Is rollback possible?
\item[$\square$] \textbf{Memory:} What persists? Is isolation enforced? What are retention policies?
\item[$\square$] \textbf{Planning:} What pattern applies? Are budgets enforced? Is there loop detection?
\item[$\square$] \textbf{Termination:} How does it know when it's done? What are success criteria?
\item[$\square$] \textbf{Escalation:} When does it ask for help? Who receives escalations? Is context sufficient?
\item[$\square$] \textbf{Delegation:} Does it coordinate? Are protocols standardized? Are barriers enforced?
\item[$\square$] \textbf{Governance:} Are security controls implemented? Is there audit capability?
\end{enumerate}
\end{keybox}

Any question left unanswered represents a gap in the architecture that will manifest as a failure in production.
