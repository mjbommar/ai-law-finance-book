% ============================================================================
% 03-intent.tex
% Q2: How Does an Agent Understand What's Being Asked?
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{How Does an Agent Understand What's Being Asked?}
\label{sec:agents2-intent}

% ----------------------------------------------------------------------------
% Opening: Q2 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

When a partner walks into your office and says ``look into the Johnson matter,'' your first job is understanding what that actually means. You must determine whether this is a quick status check or a request for deep analysis, whether you should answer a specific question or identify all issues, and whether this is urgent work for today's call or background work for next week's meeting. The words you hear are the \textbf{instruction}; the underlying purpose that those words point toward is the \textbf{intent}.

Every professional develops this skill over time: reading the assignment memo, clarifying ambiguous instructions, and understanding not just what was said but what was meant. Junior associates tend to over-clarify; senior associates internalize firm norms and client expectations and infer appropriately. The best professionals know when to ask and when to proceed.

Agent systems face the same challenge. The user provides an instruction: natural language, often ambiguous, sometimes contradictory. The agent must extract intent: what goal is being pursued, what constraints apply, what success looks like. This is the second fundamental question: \textit{How does an agent understand what's being asked?}

\begin{definitionbox}[title={Instruction, Intent, Goal, and Task}]
\keyterm{Instruction}: The words the user provides (e.g., ``Review this credit agreement'').

\keyterm{Intent}: The underlying purpose, constraints, and success criteria behind the instruction (e.g., ``identify material risks to the lender by tomorrow'').

\keyterm{Goal}: The desired end state the intent points to (e.g., ``produce a lender-risk memo that meets policy''), as defined in the GPA framework.

\keyterm{Task}: The concrete unit of work the agent will execute to advance the goal (e.g., ``extract covenants and compare to template'').

\textbf{Intent bridges instruction to goal and shapes the tasks the agent will plan.}
\end{definitionbox}

% ----------------------------------------------------------------------------
% From Instruction to Intent
% ----------------------------------------------------------------------------

\subsection{From Instruction to Intent}
\label{sec:agents2-from-instruction-to-intent}

Consider three real instructions a legal or financial professional might give:

\begin{itemize}[nosep]
\item ``Review this credit agreement for risks''
\item ``Rebalance to reduce tech exposure''
\item ``Look into the Johnson matter''
\end{itemize}

Each is ambiguous. The word ``risks'' raises immediate questions about perspective (lender or borrower), scope (material risks or all risks), and domain (legal, financial, or both). ``Reduce tech exposure'' leaves open the target level, the mechanism (sales, hedges, or both), and the tax and timing constraints. ``Look into'' specifies neither depth, urgency, nor deliverable format. The instruction is clear enough to start; the intent is not.

Pre-LLM systems handled intent through rigid parsing: keyword matching, slot filling, decision trees. These systems worked for narrow domains with controlled vocabularies but broke on natural language variation. ``Find cases on personal jurisdiction'' and ``What's the law on where you can sue someone?'' express similar intents but look nothing alike to a keyword matcher. The gap between instruction and intent has always existed; what has changed is our ability to bridge it.

Large language models dramatically improved the ability to infer intent from natural language. Where rule-based systems required exact matches, LLMs handle variation, implicit context, and domain-specific jargon. Modern LLMs excel at handling noisy input (misspellings, shorthand, tangential information), resolving references using conversational context, inferring domain-specific intent, and detecting implicit constraints that professionals take for granted.

Despite these capabilities, intent understanding remains imperfect. As conversations extend, LLMs may lose track of earlier context or constraints. When clarification is needed, LLMs sometimes proceed with a default interpretation rather than asking, resulting in a ``helpful but wrong'' failure: the agent does \textit{something} reasonable rather than confirming it understood correctly. Professionals communicate through implication---``This needs to be right'' signals high stakes; ``When you get a chance'' signals low urgency---and these signals may not be explicitly parsed. Research has shown that LLMs can ``exploit loopholes'' by selectively misunderstanding ambiguous requests in ways that appear helpful but avoid difficult work. Governance must monitor for this failure mode.

\begin{keybox}[title={Intent Inference Is Not Mind Reading}]
LLMs infer \textit{probable} intent from language patterns; they do not read minds. Inference fails when:

\begin{itemize}[nosep]
\item The instruction is genuinely ambiguous (multiple reasonable interpretations)
\item The user's intent differs from typical patterns for similar language
\item Critical context exists outside the conversation (prior meetings, firm norms)
\item The user themselves is unclear about what they want
\end{itemize}

\textbf{Design for clarification, not guessing.} The aim is an agent that surfaces uncertainty and asks, not one that pretends certainty.
\end{keybox}

% ----------------------------------------------------------------------------
% Goal Extraction from Natural Language
% ----------------------------------------------------------------------------

\subsection{Goal Extraction from Natural Language}
\label{sec:agents2-goal-extraction}

Once the agent receives an instruction, it must extract structured goals that can guide execution. This extraction transforms natural language into actionable specifications.

\textbf{Intent classification}: The first step classifies the instruction into task types that determine workflow. This translates raw words into candidate \textbf{tasks} that can advance the underlying \textbf{goal}.

\begin{itemize}[nosep]
\item \textbf{Information retrieval}: ``What's the current NAV?'' ``Find the latest 10-K''
\item \textbf{Research and analysis}: ``Research whether we can pierce the corporate veil''
\item \textbf{Document review}: ``Review the acquisition agreement for change-of-control provisions''
\item \textbf{Document generation}: ``Draft an engagement letter for the Smith matter''
\item \textbf{Calculation}: ``Calculate the IRR assuming a 5-year hold''
\item \textbf{Monitoring}: ``Alert me if tech exposure exceeds 30\%''
\end{itemize}

Different task types invoke different tools, planning patterns, and success criteria. A research task requires search and synthesis; a calculation task requires structured computation; a monitoring task requires continuous observation.

\textbf{Entity and constraint recognition}: Beyond classification, the agent must extract entities (what the task concerns) and constraints (what bounds apply):

\textbf{Entities}: Matters, clients, securities, parties, documents, jurisdictions. ``Review the Smith acquisition agreement'' references a specific document; ``Research Delaware fiduciary duties'' references a jurisdiction.

\textbf{Temporal constraints}: Deadlines, as-of dates, time windows. ``By Friday'' sets a deadline; ``as of year-end 2024'' sets a reference date; ``over the past quarter'' defines a window.

\textbf{Resource constraints}: Budget limits, scope bounds. ``Spend no more than 2 hours'' limits effort; ``focus on Articles 3 and 4'' limits scope.

\textbf{Format constraints}: Deliverable specifications. ``Summarize in one page'' constrains length; ``prepare a memo for the file'' specifies format; ``I need something to show the client'' signals external audience.

\textbf{Audience and privilege constraints}: Who will see the output (internal team, client, regulator) and what privilege or confidentiality must be preserved.

\textbf{Risk and compliance constraints}: Limits that may not be stated but must be inferred. A compliance review implicitly requires flagging violations; a client communication implicitly requires privilege protection.

\textbf{Structured goal representation}: Extracted components can be organized into structured representations that guide execution. This structured representation resembles a short assignment memo that the planning system can act on (\Cref{sec:agents2-planning}) and the termination system can measure against (\Cref{sec:agents2-termination}):

\begin{verbatim}
task_type: document_review
document: Smith Acquisition Agreement
objective: identify change-of-control provisions
constraints:
  deadline: 2025-01-15
  scope: sections 5-8
  deliverable: summary memo
success_criteria:
  - all CoC provisions identified
  - triggering events listed
  - consent requirements noted
\end{verbatim}

% ----------------------------------------------------------------------------
% Ambiguity Detection and Clarification
% ----------------------------------------------------------------------------

\subsection{Ambiguity Detection and Clarification}
\label{sec:agents2-clarification}

Not all instructions can be unambiguously interpreted. The agent must detect ambiguity and decide whether to clarify or proceed.

\textbf{When to clarify}: The decision to clarify depends on ambiguity severity and action stakes:

\textbf{Low stakes, low ambiguity}: Proceed with best interpretation. If the user asks ``What's Apple's market cap?'' and you're unsure whether they mean Apple Inc. or Apple Hospitality REIT, the dominant interpretation is obvious and the cost of being wrong is low (easy to correct).

\textbf{Low stakes, high ambiguity}: Clarify briefly. If the user asks ``Research the statute of limitations'' without specifying the claim type, a quick clarification prevents wasted effort.

\textbf{High stakes, low ambiguity}: Confirm before acting. If the instruction is clear but consequential (``File this motion''), confirmation prevents irreversible errors.

\textbf{High stakes, high ambiguity}: Clarify thoroughly. If the user says ``Handle the regulatory response'' for a complex matter, extended clarification is appropriate before taking any action.

\input{figures/fig-when-to-clarify}

\textbf{How to clarify}: Effective clarification is specific, contextual, and actionable:

\textbf{Specific}: ``Which jurisdiction's statute of limitations---Delaware or New York?'' not ``Can you clarify?''

\textbf{Contextual}: Reference what the agent already understands. ``I understand you want me to review the credit agreement. Should I focus on lender protections, borrower obligations, or both?''

\textbf{Actionable}: Offer options rather than open-ended questions. ``Should I (a) provide a comprehensive review of all provisions, (b) focus on the financial covenants, or (c) flag only provisions that differ from our standard template?''

\textbf{Bounded}: Limit clarification rounds. If the agent needs extensive clarification, it may be the wrong tool for the task, or the user may need to think through requirements before delegating.

\begin{quote}
\textit{Poor clarification}: ``Can you clarify?'' \\
\textit{Better}: ``Should I assess lender risks, borrower risks, or both?'' \\
\textit{Best}: ``You asked to reduce tech exposure. Should I (a) sell tech to 25\% target, (b) hedge with options, or (c) add non-tech positions? Which deadline mattersâ€”this week or month-end reporting?''
\end{quote}

\begin{highlightbox}[title={The Default Interpretation Risk}]
Research has documented that LLMs sometimes select a default interpretation rather than asking for clarification, even when ambiguity is significant. This ``proceed without asking'' behavior can be problematic:

\textbf{The risk}: The agent interprets ``review the contract'' as a surface-level summary when the user expected deep issue-spotting. Work product is delivered, but it is wrong.

\textbf{Mitigation strategies}:
\begin{itemize}[nosep]
\item Prompt engineering that emphasizes clarification for ambiguous requests
\item Confidence thresholds that trigger clarification below a certainty level
\item User training to provide detailed initial instructions
\item Checkpoint reviews before significant work begins
\end{itemize}

\textbf{Governance implication}: Monitor for cases where the agent proceeded confidently but delivered unexpected results. These may indicate calibration problems in ambiguity detection.
\end{highlightbox}

% ----------------------------------------------------------------------------
% Constraint Identification
% ----------------------------------------------------------------------------

\subsection{Constraint Identification}
\label{sec:agents2-constraints}

Beyond explicit instructions, agents must identify constraints that bound acceptable execution. \textbf{Temporal constraints}: Deadlines and time windows. Some are explicit (``by Friday''); others are implicit (court filing deadlines calculated from rules); still others are contextual (``before the board meeting'' requires knowing when the meeting is).

\textbf{Resource constraints}: Budget and effort limits. Token budgets limit API costs; time budgets limit calendar impact; scope constraints focus effort on high-value areas.

\textbf{Scope constraints}: What is in and out of bounds. ``Focus on Articles 3 and 4'' excludes other articles; ``just the Delaware analysis'' excludes other jurisdictions.

\textbf{Format and style constraints}: How deliverables should appear. Memo versus email versus presentation; formal versus casual tone; internal versus client-facing.

\textbf{Risk and compliance constraints}: What must be avoided. Privilege protection, conflicts of interest, regulatory restrictions, confidentiality obligations. These constraints often apply implicitly based on context.

\textbf{Inferring implicit constraints}: Professionals operate under constraints they rarely state explicitly. When a partner says ``research Section 10(b) liability,'' implicit constraints include:

\begin{itemize}[nosep]
\item Use authoritative sources (binding precedent, not blog posts)
\item Focus on the relevant jurisdiction (probably the circuit where the case is filed)
\item Assume current law (not historical analysis unless specified)
\item Protect privilege (don't disclose strategy in external searches)
\item Operate within budget norms (don't spend 40 hours on a 2-hour task)
\end{itemize}

Agents must infer these constraints from context, domain knowledge, and organizational norms. Memory systems (\Cref{sec:agents2-memory}) help by preserving firm-specific expectations; user profiles track individual preferences; matter context provides case-specific constraints.

% ----------------------------------------------------------------------------
% Validation and Domain Examples
% ----------------------------------------------------------------------------

\subsection{Validation and Domain Examples}
\label{sec:agents2-validation-examples}

Before executing, agents should validate their understanding of intent. Several patterns support validation:

\textbf{Reflection and summarization}: The agent restates its understanding before proceeding, giving the user an opportunity to correct misunderstandings before work begins.

\textbf{Chunked validation}: For complex tasks, validate in phases rather than all at once. After completing research, summarize findings and confirm direction before drafting. After drafting, confirm the approach before finalizing. Each checkpoint prevents error propagation.

\textbf{Confidence signaling}: The agent should indicate how confident it is in its own understanding. When confidence is high, the agent can proceed with light oversight; when confidence is low, the right move is to pause and ask for clarification rather than press ahead. Clear confidence signaling helps users decide how much review is needed and whether to treat the output as a draft, a starting point, or a near-final product.

\textbf{Legal example---credit agreement review}: Consider how intent extraction and validation work together for a legal task. Given the instruction ``Review this credit agreement for risks,'' the agent classifies this as a document review task and detects that ``risks'' is ambiguous (risks to whom? what types?). Context gathering reveals this is a lender-side engagement for a senior secured facility. The agent infers implicit constraints (focus on lender risks, prioritize material issues, assume current market terms as baseline) and clarifies: ``I'll review from the lender's perspective, focusing on credit risk, collateral coverage, and covenant adequacy. Should I also flag documentation risks (drafting issues, missing provisions) or focus only on substantive credit terms?'' The extracted goal:

\begin{verbatim}
{
  "task_type": "document_review",
  "perspective": "lender",
  "document": "[attached credit agreement]",
  "focus_areas": ["credit_risk", "collateral", "covenants"],
  "deliverable": "risk_summary_memo",
  "success_criteria": [
    "material_risks_identified",
    "comparison_to_market_terms",
    "recommendations_provided"
  ]
}
\end{verbatim}

The agent validates: ``I'll prepare a memo identifying material risks to the lender, comparing key terms to market standards, and recommending negotiation points. I'll have a draft for your review by tomorrow afternoon.''

\textbf{Financial example---portfolio rebalancing}: The same pattern applies to financial tasks. Given the instruction ``Rebalance to reduce tech exposure,'' the agent classifies this as a portfolio action task and immediately detects multiple ambiguities: how much reduction? through what mechanism? with what constraints? Context gathering reveals current tech exposure at 35\% against a 25\% target. Through clarification dialogue, the agent confirms the user wants to reach target through sales while minimizing tax impact (preferring loss harvesting and long-term gains over short-term). The extracted goal:

\begin{verbatim}
{
  "task_type": "portfolio_rebalancing",
  "objective": "reduce_tech_exposure",
  "target": "25%",
  "mechanism": "sales",
  "constraints": {
    "tax_optimization": true,
    "prefer_loss_harvesting": true
  },
  "approval_required": true
}
\end{verbatim}

The agent validates: ``I'll prepare a rebalancing plan to reduce tech from 35\% to 25\% through sales, optimized for tax efficiency. I'll present the plan for your approval before executing any trades.''

\begin{keybox}[title={Intent Understanding Is Continuous}]
Intent extraction is not a one-time step at task initiation. As the agent works, it may discover:

\begin{itemize}[nosep]
\item The original understanding was incomplete (new constraints emerge)
\item The user's intent has evolved (priorities shift mid-task)
\item Implicit constraints conflict (cannot optimize for both)
\item The task is impossible as specified (constraints are mutually exclusive)
\end{itemize}

Effective agents surface these discoveries through clarification rather than proceeding with outdated or impossible goals. Intent understanding is iterative, not instantaneous.
\end{keybox}

% ----------------------------------------------------------------------------
% Transition to Q3
% ----------------------------------------------------------------------------

Intent understanding connects to other framework questions. Memory (\Cref{sec:agents2-memory}) improves intent extraction over time by preserving user preferences, matter history, and firm norms. Planning (\Cref{sec:agents2-planning}) depends on clear intent; extracted goals feed the planning system, while ambiguous intent propagates through the plan as uncertainty. Governance (\Cref{sec:agents2-governance}) must address intent misalignment as a core risk, verifying goal alignment before deployment and monitoring for drift during operation.

Understanding intent bridges the gap between what users say (instruction) and what they mean (intent), shaping the goals and tasks the agent will plan. Clarification beats guessing when ambiguity is significant and stakes are high. Constraints---time, scope, audience, compliance, and budget---matter as much as goals. Validation prevents wasted effort by confirming understanding before significant work begins.

With triggers delivering work (Q1) and intent extraction revealing what's being asked (Q2), the agent needs capabilities to gather information and effect change. \Cref{sec:agents2-perception} examines the next question: how does an agent find things out?
