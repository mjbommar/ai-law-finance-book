% ============================================================================
% 08-conclusion.tex
% Conclusion
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Conclusion}
\label{sec:agents2-conclusion}

We began with a simple claim: AI agents are organized like professional teams. The associate reviewing a credit agreement needs tools (Westlaw, the precedent database), memory (prior deals, client context), planning (decompose the review into sections), protocols (how to communicate findings), and evaluation (partner review of work product). The portfolio manager monitoring client mandates needs the same components in a different domain: tools (Bloomberg, the risk engine), memory (investment research, client history), planning (coordinate monitoring, rebalancing, compliance, and risk agents), protocols (how agents share tasks and artifacts), and evaluation (continuous performance monitoring).

The reference architectures in Section~\ref{sec:agents2-synthesis} made this concrete. The credit facility review agent used ReAct planning to work through document sections, MCP tools to access precedent databases and legal research, episodic memory to track findings within the transaction, and three-layer evaluation to measure retrieval accuracy, analysis quality, and workflow completion. The portfolio management system used hierarchical planning to coordinate specialist agents, A2A protocol for task delegation and artifact exchange, and continuous evaluation to ensure mandate compliance.

But we were also honest about limitations. Current agents achieve under 10\% success on tasks exceeding four hours. Compounding errors, hallucinations in agentic loops, and brittleness at integration boundaries mean these reference architectures represent target states, not current reality. Production deployment requires human oversight, decomposition into shorter tasks, and acceptance that agents accelerate work rather than replace professional judgment.

\paragraph{What You Now Understand}

You understand that \textbf{tools} give agents the ability to interact with systems---accessing databases, running calculations, generating documents. Without tools, an agent is just a chatbot. With tools, it becomes capable of real work.

You understand that \textbf{memory} enables agents to maintain context and learn from experience---case files, precedent databases, client histories, investment research. Without memory, every interaction starts from scratch.

You understand that \textbf{planning} allows agents to decompose complex goals into manageable steps---ReAct for exploration, Plan-Execute for systematic coverage, hierarchical coordination for multi-agent workflows.

You understand that \textbf{protocols} govern how agents access tools and collaborate---MCP for standardized tool integration, A2A for agent-to-agent coordination. Protocol choice determines interoperability and audit capability.

You understand that \textbf{evaluation} measures whether agents perform at professional standards---retrieval accuracy, reasoning quality, workflow completion, security compliance. Generic benchmarks are insufficient; you need domain-specific metrics and expert review.

\paragraph{What This Lets You Do}

You can evaluate vendor claims critically. Is their ``agentic AI'' really autonomous, or just prompt engineering? Does the architecture support your workflows? What tools, memory, and planning capabilities does it actually have?

You can participate in procurement by asking the right questions. You can design governance that maps controls to architectural components. You can communicate with technical teams because you understand the system architecture. You can design deployment strategies that match your organization's risk tolerance.

\begin{keybox}[title={Architecture Enables Governance}]
You cannot govern what you do not understand. Now that you understand how agents work---their tools, memory, planning, protocols, and evaluation---you are ready to establish controls, set policies, assign accountability, and ensure compliance.
\end{keybox}

% ----------------------------------------------------------------------------
% Governance Bridge
% ----------------------------------------------------------------------------

\subsection{From Architecture to Governance}
\label{sec:agents2-governance-bridge}

Production systems in legal and financial contexts operate under governance obligations that transform architectural components into compliance artifacts. Every tool invocation becomes an audit log entry. Every memory write becomes a data retention decision subject to the tensions we examined---auditability without over-collection, structured logging without indefinite raw trace retention. Every planning step becomes a decision record subject to regulatory review.

When an SEC examiner reviews a compliance monitoring agent that detected a mandate breach, they will ask: When did the system detect the problem? What data supported the conclusion? Who approved the corrective action? What controls prevented the agent from exceeding its authority? These questions map directly to architectural components: event timestamps, tool invocation logs, memory retrieval records, and authorization checks.

Similarly, when a legal research agent accesses privileged materials, every access must respect privilege boundaries. An agent that crosses matter boundaries creates waiver risk comparable to an associate who emails the wrong client's confidential materials. The memory system's isolation controls and access logs become evidence in privilege disputes.

The architectural components you now understand become governance objects. Tools become objects of authorization policies---and the security mitigations we examined (prompt injection defense, tool result validation, privilege escalation prevention) become compliance requirements. Memory becomes subject to retention policies, legal hold obligations, and the tiered retention architecture that balances auditability with data minimization. Planning becomes subject to oversight mechanisms specifying which decisions require human review. Protocols become the infrastructure for authentication, logging, and audit trails. Evaluation becomes assurance processes that verify controls operate as intended.

\begin{keybox}[title={The Governance Imperative}]
Part III addresses a fundamental principle: \textbf{professional duties are non-delegable}. When an attorney files a brief, they are responsible for its accuracy---regardless of whether AI assisted in drafting it. When an investment adviser recommends a strategy, they bear fiduciary duty---regardless of whether an agent generated the analysis. When an auditor signs an opinion, they vouch for its conclusions---regardless of the tools used to reach them.

``The AI did it'' is not a defense to malpractice, breach of fiduciary duty, or professional sanctions.

The architecture in this chapter enables professionals to meet those non-delegable duties. The audit trails support accountability. The human-in-the-loop gates preserve professional judgment. The termination conditions prevent unbounded autonomous action. The security controls protect confidentiality. But architecture alone is insufficient---you must also establish policies, assign responsibilities, and ensure ongoing compliance.

Part III provides that governance framework. It maps the GPA+IAT properties to specific controls, establishes dimensional calibration for risk-based oversight, and translates principles into operational practice. The architecture you now understand makes governance possible. Part III makes it real.
\end{keybox}

