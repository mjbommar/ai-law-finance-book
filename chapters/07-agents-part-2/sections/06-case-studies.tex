% ============================================================================
% 06-case-studies.tex
% Case Studies: Agents in Practice
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Reference Architectures: Agents in Practice}
\label{sec:agents2-synthesis}

The preceding sections presented agent architecture as components: triggers and channels for how work enters the system (Section~\ref{sec:agents2-triggers}), surfaces for how users interact with agents (Section~\ref{sec:agents2-surfaces}), tools for perception and action, memory for context and learning, planning for strategy and termination, protocols for integration and coordination, and evaluation for quality assurance. This section synthesizes those components through two \textbf{reference architectures}---one legal, one financial---that demonstrate how the pieces fit together.

\begin{highlightbox}[title={Reference Architectures, Not Production Claims}]
The case studies below are \textit{reference architectures}: idealized designs showing how architectural components interconnect. They illustrate target states---what well-designed systems aim to achieve---not claims about what current technology reliably delivers.

As discussed in Section~\ref{sec:agents2-limitations}, agents today achieve under 10\% success on tasks exceeding four hours. Both workflows below describe multi-hour processes. \textbf{Current systems will require substantial human oversight, intervention at failure points, and acceptance of partial automation} rather than the end-to-end execution these architectures describe.

Read these case studies as blueprints for how to structure agent systems, not as descriptions of turnkey solutions available today.
\end{highlightbox}

Each reference architecture walks through a complete agent deployment: the trigger that initiates work, the surface through which users interact, the architecture that processes the task, and the evaluation that validates quality. The goal is not to provide implementation blueprints but to show how architectural choices from Sections~\ref{sec:agents2-triggers}--\ref{sec:agents2-evaluation} manifest in practice.

% ============================================================================
% CASE STUDY 1: CREDIT FACILITY DOCUMENTATION REVIEW
% ============================================================================

\subsection{Case Study: Credit Facility Documentation Review}
\label{sec:agents2-case-legal}

\paragraph{The Scenario}

A mid-market company needs to borrow \$50 million to fund expansion. The lender proposes a floating-rate credit facility---think of it like a variable-rate mortgage or credit card, but for a business. The interest rate adjusts periodically based on SOFR (the Secured Overnight Financing Rate) plus a spread. If SOFR rises, the company pays more; if it falls, they pay less.

The borrower's counsel---whether at a law firm or in the company's legal department---must review the 200-page credit agreement and related documents before closing. The stakes are significant: unfavorable terms could cost the company millions over the loan's life, and missed issues could expose counsel to malpractice claims.

\paragraph{Trigger and Surface}

The \textbf{trigger} is a document event: the lender's counsel uploads the draft credit agreement to the deal room. This external feed (Section~\ref{sec:agents2-triggers}) initiates the review workflow automatically---the agent doesn't wait for someone to remember to start the review.

The \textbf{surface} is document-first (Section~\ref{sec:agents2-surfaces}): the agent produces a structured issues list and summary memo as work products. The associate reviews and edits the document; the partner approves before delivery. Chat interaction is available for follow-up questions, but the primary output is the memo.

\subsubsection{The Task}

The partner assigns the review: ``Go through the credit agreement and flag anything that deviates from market terms or creates unusual risk for the borrower. Pay particular attention to the interest rate mechanics, financial covenants, default triggers, and prepayment provisions. I need a summary memo by Thursday.''

This is a classic legal task: document review requiring both comprehensive coverage (don't miss anything important) and professional judgment (distinguish routine terms from problematic ones). A junior associate might spend 15--20 hours on this review. An agent can accelerate the work while maintaining quality.

\subsubsection{Architecture: GPA+IAT in Practice}

The agent architecture maps directly to the GPA+IAT framework from Part I:

\textbf{Goal} becomes the planning system. The agent receives the partner's instruction and decomposes it into reviewable components: interest rate provisions (SOFR mechanics, spread adjustments, fallback rates), financial covenants (leverage ratio, interest coverage, minimum liquidity), default provisions (events of default, cross-default, cure periods), prepayment terms (voluntary prepayment, mandatory prepayment triggers, prepayment penalties), and representations and warranties. Each component becomes a subtask with defined completion criteria.

\textbf{Perception} becomes read-only tools via MCP. The agent connects to the firm's document management system to access the draft credit agreement, the precedent database to retrieve similar deals the firm has handled, and legal research platforms to check current market terms and regulatory requirements. These MCP connections (Section~\ref{sec:agents2-mcp}) provide standardized access---the agent queries Westlaw, the firm's iManage system, and the precedent database through the same protocol.

\textbf{Action} becomes write tools with appropriate controls. The agent can generate analysis memos, create comparison charts, and flag provisions for review. But it cannot modify the credit agreement itself or communicate with opposing counsel---those actions require human authorization.

\textbf{Iteration} becomes the agent loop. For each document section, the agent perceives (reads the provision), reasons (compares to precedent and market terms), acts (generates analysis), and updates (records findings in its working memory). The loop repeats until all sections are reviewed.

\textbf{Adaptation} becomes memory. The agent maintains episodic memory of what it has reviewed in this transaction, RAG access to the firm's precedent database of prior credit facilities, and semantic memory of credit agreement concepts from its training. When the agent encounters an unusual provision, it retrieves similar provisions from prior deals to assess whether this deviation is common or concerning.

\textbf{Termination} becomes guardrails and success criteria. The agent knows it's done when all identified sections have been reviewed and documented. It escalates to the associate when confidence drops below threshold---for example, when a provision uses non-standard language that doesn't match any precedent.

\subsubsection{Workflow: The Agent Loop in Action}

The review proceeds through the ReAct pattern (Section~\ref{sec:agents2-planning}):

\paragraph{Interest Rate Review} The agent reads the interest rate section: ``Interest accrues at SOFR plus 275 basis points, adjusted quarterly.'' It reasons: this is a standard floating rate structure. It retrieves precedent deals from RAG and finds comparable spreads range from 200--350 basis points for similar credit profiles. It acts: documents that the spread is within market range. It observes an unusual provision: if SOFR becomes unavailable, the lender can select a replacement rate ``in its sole discretion.'' The agent flags this---market-standard fallback provisions typically reference ARRC-recommended replacements, not lender discretion. This is a negotiation point.

\paragraph{Financial Covenant Review} The agent reads the leverage covenant: ``Borrower shall maintain a Total Debt to EBITDA ratio not exceeding 4.0:1.0.'' It retrieves comparable deals and finds this is market for the borrower's credit profile. But it notices the EBITDA definition excludes stock-based compensation and one-time restructuring charges---both borrower-favorable adjustments. It documents these as positive terms. It then reviews the cure provisions and finds the borrower has 30 days to cure covenant violations---shorter than the 45-day standard in the firm's precedent database. It flags this for negotiation.

\paragraph{Default Provisions Review} The agent identifies a cross-default provision: default under any debt instrument exceeding \$1 million triggers default under this facility. It retrieves the borrower's other debt instruments from episodic memory (loaded earlier in the session) and identifies three facilities that could trigger cross-default. It documents the interconnection risk and suggests the threshold should be raised to \$5 million to match market terms.

Throughout, the agent maintains a structured issues list with severity ratings (critical, significant, minor) and recommended responses (negotiate, accept, clarify). When it encounters provisions outside its training distribution---say, an unusual environmental compliance representation---it flags the provision for associate review rather than guessing at analysis.

\subsubsection{Where This Architecture Fails}

Reference architectures should be honest about failure modes. Here are realistic scenarios where the credit review agent falls short:

\textbf{Nuanced definitions escape statistical matching.} The agent flags a ``Change of Control'' provision as standard because it statistically matches the precedent database's patterns. But this deal's definition of ``Control'' excludes the founder's estate and family trusts---a nuance with significant implications for transaction planning that statistical similarity doesn't capture. The provision matches the pattern but misses the point.

\textbf{Cross-document dependencies break retrieval boundaries.} The credit agreement's EBITDA definition references ``Adjusted EBITDA as defined in the Intercreditor Agreement.'' The agent analyzes the credit agreement's language but doesn't retrieve and parse the separate intercreditor agreement to trace the definition chain. It reports the covenant as ``standard'' when the actual calculation methodology buried in the cross-reference is borrower-unfavorable.

\textbf{Market context requires judgment the agent lacks.} The agent retrieves precedents showing 250--350 basis point spreads for comparable credits. But those precedents are from six months ago; the current credit market has tightened significantly. The 275 basis point spread in this deal is actually aggressive in today's market---a point the agent cannot assess because market conditions aren't in the precedent database.

\textbf{Omissions are harder than inclusions.} The agent reviews provisions that exist. But experienced counsel also notice what's \textit{missing}: Where is the equity cure provision that market-standard credit facilities include? The agent finds no precedent for comparison because there's nothing in this document to match against precedents. Missing provisions require a different analytical mode than reviewing existing ones.

These failures illustrate why human review remains essential. The agent accelerates the review but cannot replace professional judgment on nuanced, contextual, or novel issues. The associate validates the agent's work product, catches these limitations, and adds the judgment that turns mechanical analysis into legal advice.

\subsubsection{Protocols: MCP and Human Coordination}

The agent uses MCP (Section~\ref{sec:agents2-mcp}) for all tool access:

\texttt{search\_precedents(deal\_type="credit facility", size\_range="25M-100M")} queries the firm's precedent database and returns relevant prior transactions.

\texttt{retrieve\_document(doc\_id="12345")} fetches the draft credit agreement from iManage.

\texttt{compare\_provision(text, provision\_type="leverage covenant")} matches the provision against market-standard language and returns deviation analysis.

\texttt{check\_current\_law(topic="SOFR transition", jurisdiction="NY")} searches Westlaw for current regulatory guidance on benchmark rate transitions.

For this single-agent deployment, A2A (Section~\ref{sec:agents2-a2a}) is not required. But for a more complex transaction---say, a leveraged buyout requiring simultaneous review of credit documents, acquisition agreement, and regulatory filings---the orchestrating agent could delegate via A2A to specialist agents: a Credit Agent for financing documents, an M\&A Agent for the acquisition agreement, and a Regulatory Agent for HSR filings. Each specialist uses MCP for tool access while A2A coordinates the overall workflow.

Human-in-the-loop integration follows the approval gate pattern (Section~\ref{sec:agents2-hitl}). The agent produces analysis autonomously but does not deliver work product to the partner without associate review. The associate reviews the issues list, validates the analysis, adds judgment where the agent flagged uncertainty, and presents the refined memo to the partner. High-stakes items---like recommending that the client reject the deal---require explicit partner approval.

\subsubsection{Evaluation: Three Layers Applied}

The agent's output is evaluated using the three-layer framework (Section~\ref{sec:agents2-eval-framework}):

\textbf{Layer 1 (Retrieval):} Did the agent find the right precedents? Metrics include retrieval accuracy (percentage of retrieved precedents that are actually comparable), coverage (did it find the firm's most relevant prior deals?), and authority appropriateness (did it prioritize recent deals over outdated ones?). For this review, target: 85\% retrieval accuracy, 90\% coverage of key precedents.

\textbf{Layer 2 (Reasoning):} Did the agent analyze provisions correctly? Metrics include issue identification accuracy (did it flag actual problems?), false positive rate (did it flag routine terms as problematic?), and severity calibration (did ``critical'' issues deserve that rating?). The associate validates by reviewing a sample of flagged and unflagged provisions. Target: 90\% issue identification accuracy, under 20\% false positive rate.

\textbf{Layer 3 (Workflow):} Did the agent complete the review appropriately? Metrics include section coverage (did it review all assigned sections?), deadline compliance (did it finish by Thursday?), escalation appropriateness (did it flag uncertain items rather than guessing?), and output quality (is the memo client-ready after associate review?). Target: 100\% section coverage, all escalations appropriate.

Security evaluation (Section~\ref{sec:agents2-eval-security}) verifies matter isolation (the agent accessed only this client's documents, not other matters), audit trail completeness (all tool calls logged for malpractice defense), and privilege protection (no privileged analysis leaked to unauthorized systems).

\begin{keybox}[title={Credit Deal Review: Architecture Summary}]
\textbf{Task:} Review 200-page credit agreement for borrower-unfavorable terms

\textbf{Tools (MCP):} Document management, precedent database, legal research

\textbf{Memory:} Episodic (this transaction), RAG (prior deals), semantic (credit concepts)

\textbf{Planning:} ReAct for section-by-section review, Plan-Execute for systematic coverage

\textbf{Human-in-the-Loop:} Associate review before partner delivery

\textbf{Evaluation:} L1 (precedent retrieval), L2 (provision analysis), L3 (workflow completion)

\textbf{Target Outcome:} 15-hour task reduced to 3 hours of associate time (agent draft + validation), issues list ready for partner review

\textbf{Current Reality:} Expect 6--8 hours with current technology; agent handles routine provisions while associate focuses on nuanced issues and failure mode catch
\end{keybox}

% ============================================================================
% CASE STUDY 2: EQUITY PORTFOLIO MANAGEMENT
% ============================================================================

\subsection{Case Study: Equity Portfolio Management}
\label{sec:agents2-case-financial}

\paragraph{The Scenario}

A pension fund has entrusted \$500 million in U.S. equities to an asset management firm. Think of it like a 401(k) but at institutional scale---the fund has specific investment objectives, risk constraints, and regulatory requirements that the portfolio manager must honor while seeking returns.

The client's investment policy statement specifies constraints: no single position exceeding 5\% of the portfolio, technology sector limited to 30\%, ESG exclusions (no tobacco, weapons manufacturers, or thermal coal), and tracking error against the S\&P 500 must stay below 3\%. The portfolio manager must continuously monitor compliance, respond to market changes, and rebalance when positions drift outside mandates.

\paragraph{Trigger and Surface}

Unlike the credit review---triggered by a discrete document event---portfolio management involves \textbf{multiple trigger types}. Market data feeds provide continuous external triggers (Section~\ref{sec:agents2-external-feeds}): price changes, corporate actions, and news events flow into the system throughout trading hours. Scheduled triggers (Section~\ref{sec:agents2-scheduled}) handle end-of-day reconciliation, weekly drift analysis, and quarterly client reporting. Human prompts arrive when the PM asks ad hoc questions: ``What's our current tech exposure?'' or ``Model the impact of selling half our NVDA position.''

The \textbf{surface} is primarily automation (Section~\ref{sec:agents2-surfaces}): the system monitors continuously and surfaces information only when action is needed. Dashboards show real-time status; alerts appear when positions approach limits; recommendation packages arrive when rebalancing is triggered. Chat interaction is available for PM queries, and quarterly reports use document surfaces. The multi-surface approach matches how the PM actually works: continuous background monitoring with periodic human engagement.

\subsubsection{The Task}

The portfolio manager needs ongoing support: ``Monitor the portfolio for mandate compliance and drift. When positions approach limits or market conditions suggest rebalancing, generate recommendations with supporting analysis. Flag any compliance issues immediately. Prepare quarterly client reports showing performance attribution and risk metrics.''

This is a continuous management task requiring real-time monitoring, periodic rebalancing decisions, and structured reporting---exactly the kind of work where agents can multiply human capacity while humans retain investment judgment.

\subsubsection{Architecture: GPA+IAT in Practice}

\textbf{Goal} becomes the planning system. The agent pursues multiple concurrent objectives: compliance monitoring (continuous), drift detection (daily), rebalancing analysis (triggered), and reporting (quarterly). Each objective has its own success criteria and termination conditions. Hierarchical planning (Section~\ref{sec:agents2-planning}) coordinates these workstreams---the monitoring agent escalates to the rebalancing agent when drift exceeds thresholds.

\textbf{Perception} becomes market data and portfolio system access via MCP. The agent connects to Bloomberg for real-time prices and market data, the firm's portfolio management system for current holdings and transaction history, compliance databases for restricted lists and regulatory limits, and risk systems for VaR calculations and factor exposures. Each connection uses MCP's standardized interface.

\textbf{Action} becomes recommendation generation with approval gates. The agent can generate rebalancing recommendations, calculate proposed trades, and draft client reports. But it cannot execute trades directly---all transactions require portfolio manager approval, with large trades requiring additional compliance sign-off.

\textbf{Iteration} becomes the monitoring loop. The agent continuously perceives (retrieves prices and positions), reasons (calculates exposures and compares to mandates), and acts (updates dashboards, generates alerts, or triggers rebalancing analysis). The loop runs continuously during market hours.

\textbf{Adaptation} becomes memory across market cycles. The agent maintains episodic memory of this client's portfolio history and past rebalancing decisions, RAG access to the firm's investment research on covered securities, and learned patterns about how this client responds to various recommendations. When the PM accepted a rebalancing recommendation six months ago, the agent remembers the reasoning and applies similar logic to current situations.

\textbf{Termination} varies by workstream. Compliance monitoring never terminates during market hours---it runs continuously. Drift detection terminates daily with end-of-day position reconciliation. Rebalancing analysis terminates when recommendations are generated and approved or rejected by the PM. Reporting terminates when quarterly reports are delivered to the client.

\subsubsection{Workflow: Multi-Agent Coordination}

Portfolio management involves multiple specialized functions. This deployment uses multi-agent orchestration (Section~\ref{sec:agents2-multi-agent}) with A2A coordination:

\paragraph{Monitoring Agent} Runs continuously during market hours. Tracks position sizes against the 5\% single-name limit, calculates sector exposures against the 30\% technology cap, screens holdings against the ESG exclusion list, and monitors tracking error against the benchmark. When any metric approaches its limit (say, a position reaches 4.5\%), the Monitoring Agent creates an A2A Task for the Rebalancing Agent.

\paragraph{Rebalancing Agent} Receives drift alerts from the Monitoring Agent and generates rebalancing recommendations. It retrieves current market conditions via MCP (liquidity, volatility, recent price movements), calculates proposed trades to bring the portfolio within mandates, estimates transaction costs and market impact, and generates a recommendation memo for PM review. The memo includes: current exposure, target exposure, proposed trades, estimated costs, and risk impact.

\paragraph{Compliance Agent} Validates all proposed trades before PM review. It checks the restricted list (no trading in securities where the firm has MNPI), verifies position limits, confirms the trades don't violate client mandates, and ensures regulatory reporting thresholds aren't triggered. If any check fails, the Compliance Agent rejects the recommendation with explanation.

\paragraph{Risk Agent} Calculates portfolio-level risk metrics. Before and after each proposed rebalancing, it computes VaR at 95\% and 99\% confidence, tracking error against benchmark, factor exposures (market, size, value, momentum), and stress test results under various scenarios. The PM uses these metrics to assess whether the rebalancing improves the portfolio's risk profile.

The agents communicate via A2A protocol (Section~\ref{sec:agents2-a2a}). The Monitoring Agent creates a Task describing the drift condition. The Rebalancing Agent returns an Artifact containing the recommendation. The Compliance Agent validates and returns approval or rejection. The Risk Agent provides metrics as supporting Artifacts. The PM reviews the complete package---recommendation, compliance approval, and risk analysis---before authorizing execution.

\subsubsection{Protocols: MCP for Data, A2A for Coordination}

Each agent uses MCP for tool access:

\texttt{get\_positions(portfolio\_id, as\_of\_date)} retrieves current holdings from the portfolio management system.

\texttt{get\_market\_data(tickers, fields=["price", "volume", "volatility"])} fetches real-time data from Bloomberg.

\texttt{check\_restricted\_list(tickers)} validates securities against the compliance restricted list.

\texttt{calculate\_var(portfolio, confidence, horizon)} computes Value-at-Risk using the firm's risk engine.

\texttt{get\_esg\_ratings(tickers)} retrieves ESG scores to verify exclusion compliance.

A2A coordinates the multi-agent workflow:

The Monitoring Agent publishes its Agent Card: ``I monitor portfolios for mandate compliance and drift. I produce drift alerts as Tasks for rebalancing analysis.''

When technology sector exposure reaches 29\% (approaching the 30\% limit), the Monitoring Agent creates a Task: ``Technology exposure approaching limit. Current: 29\%. Limit: 30\%. Largest tech holdings: AAPL (4.2\%), MSFT (3.8\%), NVDA (2.5\%). Request rebalancing analysis.''

The Rebalancing Agent accepts the Task, conducts analysis, and returns an Artifact: the recommendation memo proposing to trim AAPL by 50 basis points and reallocate to healthcare.

The Compliance Agent receives the recommendation, validates it, and returns an approval Artifact.

The Risk Agent calculates before/after metrics and returns analysis Artifacts.

The orchestrating system assembles all Artifacts into a decision package for PM review.

\subsubsection{Multi-Agent Failure Modes}

Multi-agent architectures introduce coordination failures beyond single-agent limitations:

\textbf{Cascading errors across agent boundaries.} The Monitoring Agent incorrectly calculates sector exposure due to a stale price feed---it shows technology at 28\% when actual exposure is 31\%, already over the mandate limit. The Rebalancing Agent receives this incorrect signal and generates recommendations to \textit{increase} technology exposure. The Compliance Agent validates against the same stale data. By the time a human notices, the portfolio has drifted further from mandate compliance. Bad data poisoned the entire chain.

\textbf{Coordination overhead exceeds single-agent simplicity.} The A2A handoffs between four agents---Monitoring, Rebalancing, Compliance, Risk---introduce latency. Each handoff requires task creation, artifact packaging, and response parsing. For simple rebalancing decisions, a single well-designed agent might outperform the orchestrated specialists because coordination overhead dominates.

\textbf{Debugging complexity when failures span agents.} The PM rejects a recommendation as economically unreasonable. Which agent failed? Was it bad market data (Monitoring's retrieval)? Flawed optimization logic (Rebalancing's reasoning)? Overly conservative risk estimates (Risk's calculations)? Tracing causation across agent boundaries requires sophisticated logging and often manual forensic analysis.

\textbf{Agent disagreement without resolution.} The Rebalancing Agent recommends selling NVDA. The Risk Agent's stress test shows the sale increases portfolio volatility. Neither agent has authority to override the other. The orchestrator presents conflicting recommendations to the PM without synthesis. Multi-agent architectures distribute expertise but may not aggregate it.

\textbf{When to prefer single-agent simplicity:} Multi-agent orchestration suits genuinely parallel, specialized workstreams (M\&A due diligence with distinct legal, financial, and regulatory tracks). For sequential workflows where one agent's output feeds the next, the coordination overhead and failure propagation risks often favor simpler single-agent designs with explicit human checkpoints.

\subsubsection{Evaluation: Continuous Monitoring}

Portfolio management requires continuous evaluation (Section~\ref{sec:agents2-eval-continuous}), not just deployment-time validation:

\textbf{Layer 1 (Data Quality):} Is market data accurate and timely? Metrics include data freshness (latency from exchange to agent), identifier accuracy (correct ticker/CUSIP mapping), and completeness (no missing prices for portfolio securities). Automated monitoring compares agent data against independent feeds. Target: 99.9\% accuracy, sub-second latency during market hours.

\textbf{Layer 2 (Analysis Quality):} Are rebalancing recommendations sound? Metrics include recommendation acceptance rate (what percentage does the PM approve?), post-trade performance (did recommended trades improve the portfolio?), and risk calculation accuracy (do realized volatilities match predictions?). Weekly sampling compares agent analysis to analyst review. Target: 85\% acceptance rate, risk predictions within 10\% of realized.

\textbf{Layer 3 (Mandate Compliance):} Does the portfolio stay within client constraints? Metrics include breach frequency (how often do positions exceed limits?), alert timeliness (how far in advance are approaching limits flagged?), and false alert rate (how many alerts don't require action?). Target: zero mandate breaches, 24-hour advance warning on approaching limits, under 10\% false alerts.

Security evaluation verifies client isolation (this client's portfolio data is not accessible to other client agents), MNPI protection (restricted list checking prevents trading on inside information), and audit completeness (all recommendations and approvals logged for regulatory examination).

The evaluation flywheel (Section~\ref{sec:agents2-eval-flywheel}) operates continuously: recommendations the PM rejects become training cases for improving future recommendations, mandate breaches (if any) trigger root cause analysis and system updates, and quarterly performance reviews compare agent-assisted portfolios against benchmarks.

\begin{keybox}[title={Portfolio Management: Architecture Summary}]
\textbf{Task:} Continuously monitor \$500M equity portfolio for mandate compliance and rebalancing opportunities

\textbf{Tools (MCP):} Market data (Bloomberg), portfolio system, compliance database, risk engine

\textbf{Memory:} Episodic (client history), RAG (investment research), learned (PM preferences)

\textbf{Planning:} Hierarchical coordination of Monitoring, Rebalancing, Compliance, and Risk agents

\textbf{Protocols:} MCP for data access, A2A for multi-agent coordination

\textbf{Human-in-the-Loop:} PM approval for all trades, compliance sign-off for large transactions

\textbf{Evaluation:} Continuous L1 (data), L2 (analysis), L3 (compliance) monitoring with weekly sampling

\textbf{Target Outcome:} Real-time mandate monitoring, proactive rebalancing recommendations, zero compliance breaches

\textbf{Current Reality:} Position monitoring works well; rebalancing recommendations require significant PM judgment; multi-agent coordination remains fragile and requires human oversight at handoff points
\end{keybox}

% ============================================================================
% SYNTHESIS: KEY PRINCIPLES
% ============================================================================

\subsection{Synthesis: Principles Across Domains}
\label{sec:agents2-synthesis-principles}

The two case studies---credit documentation review and portfolio management---differ in domain, time horizon, and complexity. But they share architectural principles that generalize across legal and financial applications:

\textbf{GPA+IAT maps to concrete components.} In both cases, Goals became planning systems, Perception became MCP-connected tools, Action became controlled write operations, Iteration became the agent loop, Adaptation became memory systems, and Termination became explicit stopping criteria. The framework from Part I isn't abstract theory---it's a design checklist.

\textbf{Tools require appropriate controls.} The credit review agent couldn't modify documents or contact opposing counsel. The portfolio agent couldn't execute trades without PM approval. Tool permissions matched task requirements and risk profiles---read access was permissive, write access was gated.

\textbf{Memory enables context and learning.} Both agents used episodic memory (this transaction, this portfolio), RAG (precedent deals, investment research), and semantic knowledge (legal concepts, financial principles). Memory transformed generic reasoning into domain-competent analysis.

\textbf{Protocols enable integration.} MCP provided standardized tool access in both cases. A2A enabled multi-agent coordination for portfolio management. The protocols from Section~\ref{sec:agents2-protocols} aren't optional infrastructure---they're how agents connect to the systems where work actually happens.

\textbf{Humans remain in the loop.} The credit review agent produced recommendations for associate validation. The portfolio agent generated trade proposals for PM approval. Neither agent took consequential action autonomously. Human judgment remained essential for high-stakes decisions.

\textbf{Evaluation is continuous.} Both deployments used three-layer evaluation---retrieval, reasoning, workflow---with metrics appropriate to their domains. Portfolio management added continuous monitoring because the task never ends. Evaluation isn't a deployment gate; it's an ongoing quality system.

\begin{highlightbox}[title={From Architecture to Deployment}]
The components from Sections~\ref{sec:agents2-architecture}--\ref{sec:agents2-evaluation} become a deployment checklist:

\textbf{1. Define the work.} What tasks will the agent handle? Credit review? Portfolio monitoring? Research? Drafting? The task determines the architecture.

\textbf{2. Equip with tools.} What systems does the agent need? Legal research, document management, market data, compliance databases? Connect via MCP with appropriate permissions.

\textbf{3. Provide context.} What memory does the agent need? Prior deals, investment research, client history? Build RAG and episodic memory for the domain.

\textbf{4. Design workflows.} How should the agent approach tasks? ReAct for exploration, Plan-Execute for systematic coverage, hierarchical for complex coordination?

\textbf{5. Integrate humans.} Where do humans review and approve? Associate review of legal analysis, PM approval of trades, partner sign-off on client deliverables?

\textbf{6. Measure quality.} How will you know the agent works? Layer 1 retrieval metrics, Layer 2 analysis quality, Layer 3 workflow completion? Build evaluation into the system from day one.

Architecture is the blueprint for systems that work.
\end{highlightbox}

