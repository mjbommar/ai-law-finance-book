% ============================================================================
% 07-planning.tex
% Q6: How Does an Agent Break a Big Job into Steps?
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{How Does an Agent Break a Big Job into Steps?}
\label{sec:agents2-planning}

% ----------------------------------------------------------------------------
% Opening: Q6 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

A litigation partner approaching a new matter does not start by drafting motions. The partner develops a strategy: discovery first (what facts do we need?), then dispositive motions if the law clearly favors us, settlement discussions in parallel, trial prep as a backstop. Discovery breaks into phases: initial disclosures, document requests, interrogatories, depositions. Tasks distribute across the team: senior associate handles briefing, junior associate does document review, paralegal manages scheduling and filings. Throughout, the partner monitors progress: are we on track for deadlines? Are discovery responses revealing helpful facts or should we adjust our theory?

This is \keyterm{planning}: decomposing complex goals into action sequences, much like the litigation roadmap or deal timeline that guides execution. Without planning, agents react to immediate observations without strategy. With planning, they work systematically toward objectives, adapt when circumstances change, and know when they're done.

\begin{definitionbox}[title={Planning}]
\keyterm{Planning} decomposes complex goals into sequences of actions. It encompasses:

\begin{itemize}[nosep]
\item \textbf{Decomposition}: Breaking large tasks into manageable steps
\item \textbf{Sequencing}: Ordering steps logically (what depends on what?)
\item \textbf{Allocation}: Assigning steps to tools or agents
\item \textbf{Monitoring}: Tracking progress toward the goal
\item \textbf{Adaptation}: Adjusting the plan when circumstances change
\end{itemize}

Without planning, an agent is like an associate who keeps running searches without a research strategy, busy but not progressing toward a deliverable.
\end{definitionbox}

% ----------------------------------------------------------------------------
% Planning Patterns
% ----------------------------------------------------------------------------

\subsection{Planning Patterns}
\label{sec:agents2-planning-patterns}

Three patterns dominate agent planning, each suited to different task types:

\textbf{ReAct: Reasoning + Acting.} The most fundamental pattern interleaves reasoning with action \parencite{yao2022react}. The partner asks for authority that a forum selection clause is unenforceable. The associate reasons: ``Key grounds are unconscionability and public policy. Start with \textit{Atlantic Marine}.'' They search, observe results, reason again: ``The unconscionability cases involve consumer adhesion contracts---not our commercial situation. The public policy line is closer.'' They search again, refine based on results.

Each cycle has three components:

\begin{itemize}[nosep]
\item \textbf{Thought}: Explicit reasoning about what to do next
\item \textbf{Action}: Tool call to gather information or effect change
\item \textbf{Observation}: Tool output that informs the next thought
\end{itemize}

Reasoning traces make decisions transparent and auditable. ReAct works well for exploratory tasks where you learn as you go---research questions, fact investigation, market analysis.

\textbf{Plan-Execute.} This pattern separates planning from execution. For document review (``Review 50 contracts for choice-of-law, forum selection, arbitration, and liquidated damages provisions''), the associate makes a plan: checklist of provisions, open each contract, record findings. Then they execute systematically. The plan does not change because the task is well-defined.

Plan-Execute fits workflows with established procedures: due diligence checklists, compliance reviews, document assembly. You create the plan upfront and execute methodically. Research variants like ReWOO (which separates reasoning from observation to reduce token usage) and LLMCompiler (which optimizes execution graphs for parallelism) enable parallel tool calling when steps are independent, though the basic pattern remains: plan first, then execute.

\textbf{Hierarchical Planning.} Law firms decompose matters into workstreams delegated through layers. A parent agent receives a high-level goal, breaks it into sub-goals, and delegates to specialists.

``Prepare for trial'' becomes:
\begin{itemize}[nosep]
\item Finalize witness list (delegated to one agent)
\item Prepare exhibits (another agent)
\item Draft jury instructions (another agent)
\end{itemize}

Each specialist may decompose further. This enables parallelization and specialization, mirroring how litigation teams work with multiple associates and paralegals handling different workstreams simultaneously.

See \Cref{sec:agents2-delegation} for detailed treatment of multi-agent coordination patterns.

% ----------------------------------------------------------------------------
% Choosing the Right Pattern
% ----------------------------------------------------------------------------

\subsection{Choosing the Right Planning Pattern}
\label{sec:agents2-planner-selection}

Selecting the right pattern depends on task structure and required autonomy level:

\begin{table}[htbp]
\centering
\caption{Planning pattern selection guide}
\label{tab:agents2-planner-selection}
\small
\begin{tabular}{p{0.22\textwidth}p{0.14\textwidth}p{0.14\textwidth}p{0.32\textwidth}}
\toprule
\textbf{Task Type} & \textbf{Pattern} & \textbf{Autonomy} & \textbf{Example} \\
\midrule
Well-defined steps, known scope & Plan-Execute & Moderate & Credit review, compliance audit, due diligence checklist \\
\midrule
Exploratory, learns as it goes & ReAct & Higher & Legal research, fact investigation, market analysis \\
\midrule
Complex, parallel workstreams & Hierarchical & Distributed & M\&A transaction, portfolio construction, multi-jurisdiction filing \\
\bottomrule
\end{tabular}
\end{table}

The autonomy column matters for governance. Higher-autonomy patterns require more sophisticated oversight:

\textbf{Plan-Execute (Moderate autonomy)}: The agent operates within tight bounds defined by the plan. Oversight focuses on plan validation and output review.

\textbf{ReAct (Higher autonomy)}: The agent makes decisions about what to search, what to pursue, when to stop. Oversight requires explicit termination mechanisms, confidence thresholds, and reasoning trace review.

\textbf{Hierarchical (Distributed autonomy)}: Multiple agents make decisions. Oversight requires clear delegation contracts, escalation paths between agents, and coordination monitoring.

Match oversight rigor to autonomy level.

% ----------------------------------------------------------------------------
% Understanding the Task
% ----------------------------------------------------------------------------

\subsection{Understanding the Task Before Planning}
\label{sec:agents2-pre-planning}

Before planning, agents must understand what they're being asked to do. \Cref{sec:agents2-intent} covers intent extraction in detail. For planning purposes, the key outputs are:

\textbf{Task classification}: Is this exploratory (ReAct), structured (Plan-Execute), or complex (Hierarchical)?

\textbf{Constraints}: What bounds the work? Deadlines, budgets, scope limitations.

\textbf{Success criteria}: How will we know when we're done? What deliverable is expected?

Effective planning requires clear inputs. Ambiguous goals produce unfocused plans; unclear success criteria make termination difficult.

% ----------------------------------------------------------------------------
% Budget Architecture
% ----------------------------------------------------------------------------

\subsection{Budget Architecture}
\label{sec:agents2-budgets}

Without explicit resource budgets, agents can run indefinitely. This is the ``runaway associate'' problem: you asked for two cases, the associate gives you fifty because they didn't know when the answer was sufficient.

\textbf{Budget Types.} Four budget types provide control over agent execution, each addressing a different dimension of resource consumption. Token budgets limit LLM API consumption, preventing expensive runaway reasoning loops where the agent keeps elaborating without making progress. Time budgets enforce deadlines by stopping execution after a fixed duration---perhaps 10 minutes---if no meaningful progress has occurred. Tool call budgets prevent runaway tool loops by capping the number of external calls; after 20 searches without progress, the agent should escalate rather than continuing to search. Cost budgets cap total spending in dollars, particularly important when using expensive models or external APIs where unconstrained execution could generate substantial charges.

These budgets cascade through levels: session budgets constrain entire engagements, task budgets allocate resources to specific work items, and subtask budgets subdivide further. A legal research task might receive a 30-minute time budget and 50,000-token limit; if it spawns subtasks, those subtasks share the parent budget rather than each receiving unlimited resources.

\textbf{Cost at Scale.} Token costs compound across agentic workflows. Consider a credit facility review: a 200-page document requires roughly 80,000 tokens to ingest. Each section analysis might consume 10,000--20,000 tokens across reasoning and tool calls. Retrieval from precedent databases adds tokens. Multi-iteration refinement multiplies costs.

A comprehensive review might consume 500,000--1,000,000 tokens. At illustrative pricing (late 2025: roughly \$3--15 per million input tokens for leading models; verify current rates), that's \$2--15 per review in API costs alone---before infrastructure, storage, or human review time.

For portfolio management running continuously, costs accumulate differently: thousands of small queries per day rather than occasional large tasks. Monitor aggregate daily/weekly costs, not just per-task.

\textbf{Economic Considerations.} When does agent assistance cost less than human work? Retrieval-heavy tasks (research, document review) show the clearest ROI when agents reduce hours substantially. Judgment-intensive tasks show less clear ROI when extensive human revision is required. The critical variable is human review time: agent output requiring extensive correction may cost more than human-only work.

Billing norms are evolving: some firms pass efficiency gains through as reduced hours, others add technology fees, others use fixed-fee arrangements. ABA Formal Opinion 512 requires competence regardless of tools and reasonable billing \parencite{aba-formal-opinion-512}. Transparency about AI assistance enables clients to evaluate the value proposition.

\textbf{Graceful Degradation.} When budgets tighten, agents should degrade gracefully rather than failing completely. Tiered outputs provide value at every budget level: minimal budget delivers the controlling statute with citation; moderate budget adds key holdings; full budget delivers comprehensive analysis. The user receives something useful regardless of where termination occurs.

Soft limits at 75--80\% of budget warn the agent that resources are running low, prompting it to prioritize completion over exploration. If the agent has found adequate authority, it should synthesize rather than searching for more. Hard limits at 100\% terminate execution and return whatever partial results exist. A budget-aware agent that delivers partial results is more useful than one that fails completely, and partial results often suffice for the user's immediate needs.

% ----------------------------------------------------------------------------
% Knowing When to Stop
% ----------------------------------------------------------------------------

\subsection{Knowing When to Stop}
\label{sec:agents2-termination-preview}

Perhaps the most critical planning capability is knowing when to stop. \Cref{sec:agents2-termination} provides comprehensive treatment; for planning purposes, four categories of stopping conditions guide agent behavior.

Success conditions terminate execution when the goal is achieved: the research question is answered, the document is reviewed, the analysis is complete. The agent returns its result and stops. Resource exhaustion terminates execution when budget limits are reached, returning partial results or escalating for additional allocation. Confidence thresholds terminate execution when uncertainty is too high for autonomous action, escalating for human review rather than proceeding with unreliable conclusions. Error conditions terminate execution when repeated failures indicate a problem that retrying will not solve.

Define explicit stopping rules, just as you would instruct an associate: ``If you find three on-point circuit opinions that all agree, you're done. If you've searched for two hours and found nothing, come talk to me.'' Agents need the same clarity about when their work is complete.

% ----------------------------------------------------------------------------
% Guardrails and Loop Detection
% ----------------------------------------------------------------------------

\subsection{Guardrails and Loop Detection}
\label{sec:agents2-guardrails}

Even with budgets and termination conditions, agents can get stuck in unproductive loops. Multiple mechanisms detect and prevent these patterns: step limits, reflection checkpoints, external watchdogs, and meta-policies. \Cref{sec:agents2-loop-detection} provides comprehensive treatment of loop detection and guardrail mechanisms in the context of termination.

% ----------------------------------------------------------------------------
% Connection to Other Questions
% ----------------------------------------------------------------------------

\subsection{From Planning to Termination}
\label{sec:agents2-planning-termination}

Planning answers how agents decompose work, but every plan must end. The next two questions address the boundaries that contain autonomous execution.

Termination (Q7, \Cref{sec:agents2-termination}) answers: how does an agent know when it is done? This involves defining success criteria so the agent can recognize completion, budget limits so the agent cannot run indefinitely, and completion recognition so the agent delivers results rather than continuing to refine. Escalation (Q8, \Cref{sec:agents2-escalation}) answers: how does an agent know when to ask for help? This involves confidence thresholds that trigger human review when uncertainty is high, authority boundaries that prevent agents from exceeding their mandate, and human-in-the-loop integration that makes escalation smooth rather than disruptive.

Without clear termination, agents run forever. Without escalation, agents exceed authority. These boundaries define the safe operating envelope for autonomous execution, ensuring that agents remain useful tools rather than becoming uncontrolled processes.
