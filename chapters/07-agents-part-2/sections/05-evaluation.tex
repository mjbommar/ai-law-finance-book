% ============================================================================
% 05-evaluation.tex
% Technical Evaluation
% Part of: Chapter 07 - Agents Part II: How to Build an Agent
% ============================================================================

\section{Technical Evaluation}
\label{sec:agents2-evaluation}

Evaluating agents is harder than evaluating models. A model produces outputs given inputs; an agent executes multi-step workflows, adapts strategies, and interacts with external systems. This section presents a three-layer evaluation framework that mirrors how law firms evaluate associate work or financial institutions assess analyst performance, a structured performance review system for your AI workforce.

% ============================================================================
% THREE-LAYER FRAMEWORK
% ============================================================================

\subsection{Three-Layer Evaluation Framework}
\label{sec:agents2-eval-framework}

Agent evaluation follows the same logic as evaluating professional work, compressed into three layers. \textbf{Layer 1 (Retrieval and Perception)} asks whether the system grounded itself on the right inputs: did it find the right materials? \textbf{Layer 2 (Reasoning and Adaptation)} tests the quality and flexibility of analysis: did it reason correctly? \textbf{Layer 3 (Workflows and Termination)} verifies that the task finished correctly with proper escalation and timing: did it complete the work appropriately? Figure~\ref{fig:agents2-eval-layers} visualizes these checkpoints; the subsections that follow detail metrics and methods.

\begin{figure}[htbp]
\centering
\input{figures/eval-layers.tex}
\caption{Three-layer evaluation framework: Layer 1 checks retrieval (did we find the right materials?), Layer 2 checks reasoning (did we analyze correctly?), Layer 3 checks workflow completion (did we finish appropriately?).}
\label{fig:agents2-eval-layers}
\end{figure}

% ============================================================================
% LAYER 1: RETRIEVAL AND PERCEPTION
% ============================================================================

\subsection{Layer 1: Retrieval and Perception}
\label{sec:agents2-eval-layer1}

Layer 1 evaluates how well your agent gathers relevant information. When you assign research to a junior professional, your first quality check is: Did they find the right materials?

\subsubsection{Retrieval Quality Metrics}
\label{sec:agents2-eval-rag}

Five metrics capture retrieval quality. \textbf{Retrieval accuracy} measures what percentage of retrieved documents are actually relevant; if your associate gave you ten cases but only six are on point, that is 60\% accuracy. \textbf{Coverage} asks the complementary question: what percentage of relevant documents were found? Even if everything retrieved is relevant, did the agent miss the controlling precedent that would have changed the analysis? \textbf{Ranking quality} evaluates whether the most important documents surface first or get buried on page ten; good associates surface binding authority before secondary sources, and good retrieval systems should do the same.

\textbf{Data freshness} determines whether sources remain valid: are cases current or overruled? Is market data real-time or stale? Citing bad law or using yesterday's prices for a trade decision can be malpractice or breach of fiduciary duty. Finally, \textbf{identifier accuracy} verifies that citations actually exist and that ticker symbols, CUSIPs, and other identifiers resolve correctly. A single character error can cause you to trade the wrong asset or cite a nonexistent case, errors that undermine credibility and create liability exposure.

\begin{highlightbox}[title={Retrieval Examples}]
\textbf{Legal:} Query ``securities fraud scienter requirement'' returns five documents: \textit{Tellabs}, \textit{Ernst \& Ernst}, \textit{Dura Pharmaceuticals}, plus two unrelated contract cases. \textbf{Assessment:} 60\% accuracy (3/5 relevant), good ranking (first result highly relevant), needs better filtering.

\textbf{Financial:} Query ``high-yield energy sector bonds'' returns eight securities: six relevant BB-rated energy bonds, plus two investment-grade utilities. \textbf{Assessment:} 75\% accuracy, 100\% identifier accuracy, needs better credit rating filtering.
\end{highlightbox}

\subsubsection{Legal AI Layer 1 Metrics}
\label{sec:agents2-eval-layer1-legal}

Legal AI faces domain-specific retrieval challenges. A case that looks semantically similar may be from the wrong jurisdiction or overruled. This ``misgrounding'' problem (retrieving real documents but applying them incorrectly) is distinct from hallucination.

Attorneys familiar with e-Discovery and technology-assisted review (TAR) will recognize these metrics as domain-specific applications of information retrieval fundamentals. \textbf{Retrieval accuracy} corresponds to \textit{precision}: the fraction of retrieved documents that are relevant. \textbf{Coverage} corresponds to \textit{recall}: the fraction of relevant documents that were retrieved. The tension between precision and recall is familiar from TAR workflows: aggressive recall captures more responsive documents but increases review burden; conservative precision reduces noise but risks missing key evidence. The \textit{F1 score} (harmonic mean of precision and recall) provides a single metric when both matter equally, though legal applications often weight recall higher for privilege review and precision higher for issue coding.

\textbf{Authority retrieval} adds a legal-specific dimension: does your agent prioritize binding authority over persuasive sources? A single controlling case outweighs fifty law review articles, and retrieval systems should reflect that hierarchy, a nuance that generic precision/recall metrics miss. \textbf{Jurisdictional accuracy} presents a particular challenge: independent evaluations (2024--2025) show that even leading legal AI systems achieve only 75--82\% jurisdictional accuracy, meaning nearly one in four retrieved documents may come from non-binding jurisdictions. These figures improve with explicit jurisdiction filtering but degrade for less common jurisdictions where training data is sparse.

\textbf{Temporal validity} determines whether sources remain good law. Many legal AI systems skip citation validation entirely, relying on raw semantic search without checking Shepard's or KeyCite, a gap that can surface overruled precedent as if it were controlling authority. \textbf{Citation verification} addresses completeness: independent benchmarks (2024--2025) report wide variance in incomplete answer rates across legal AI platforms. Some systems prioritize precision (fewer but more reliable answers), while others prioritize recall (more answers with higher incompleteness risk). Understanding your vendor's approach matters because the right trade-off depends on use case: high-stakes litigation demands precision, while preliminary research may tolerate broader recall.

\begin{keybox}[title={Layer 1: Context-Dependent Standards}]
\textbf{Legal:} Transactional (contract accuracy, due diligence completeness), Litigation (case law precision, discovery coding), Regulatory (compliance assessment, filing validation). Big Law requires higher jurisdictional accuracy than small firm research assistants.

\textbf{Financial:} Buy-side (portfolio data quality, research completeness), Sell-side (research accuracy, timeliness), Trading (millisecond data freshness), Risk (VaR input validation). Sell-side trading requires real-time data; buy-side portfolio analysis tolerates end-of-day.
\end{keybox}

% ============================================================================
% LAYER 2: REASONING AND ADAPTATION
% ============================================================================

\subsection{Layer 2: Reasoning and Adaptation}
\label{sec:agents2-eval-layer2}

Layer 2 evaluates reasoning quality and adaptation. The associate found the right cases (Layer 1 passed). But did they analyze them correctly?

\subsubsection{Reasoning Quality Metrics}
\label{sec:agents2-eval-reasoning}

Evaluating reasoning requires examining the process, not just the conclusion. \textbf{Reasoning trace evaluation} checks intermediate steps: the chain of logic connecting inputs to outputs. In legal practice, this maps to the IRAC framework: did the agent correctly identify the Issue, state the governing Rule, Apply that rule to the facts, and reach a sound Conclusion? In finance, a portfolio manager checks the analyst's math, examines underlying assumptions, and reviews stress tests. An agent that reaches the right answer through flawed reasoning will eventually produce wrong answers; the trace reveals whether apparent success reflects genuine capability or luck.

\textbf{Adaptation metrics} measure flexibility under uncertainty. Can your agent detect when initial approaches fail and try alternatives? Good associates pivot when research hits dead ends; they try different search terms, consult secondary sources, or reframe the question. Poor associates keep pursuing failed strategies, generating volume without progress. Measuring adaptation requires test scenarios with deliberate obstacles: blocked data sources, ambiguous queries, or contradictory information that forces strategic adjustment.

\textbf{Workflow quality} applies domain-specific standards to the complete analysis. Legal reasoning must weigh binding versus persuasive authority, acknowledge gaps in the record, and express appropriate uncertainty (``likely'' versus ``certainly''). Financial analysis must use appropriate quantitative methods (fat-tailed distributions for crisis scenarios, not normal distributions that underestimate tail risk) and incorporate risk-adjusted returns rather than raw performance. The baseline for all these metrics should be expert human professionals: if your legal AI performs worse than a competent third-year associate, or your financial AI worse than a junior analyst, it is not ready for production deployment.

\begin{definitionbox}[title={Layer 2: Domain-Specific Standards}]
\textbf{Legal:} Authority weighting (binding over persuasive), counterargument analysis, appropriate hedging (``likely'' vs. ``certainly''), IRAC structure.

\textbf{Financial:} Model appropriateness (fat-tailed distributions for crisis scenarios), risk-adjusted metrics (Sharpe ratios), regulatory compliance, assumption documentation.

\textbf{Baseline:} Compare to competent junior professionals. A third-year associate should identify key liability provisions; a buy-side analyst should stress-test DCF assumptions.
\end{definitionbox}

% ============================================================================
% LAYER 3: WORKFLOWS AND TERMINATION
% ============================================================================

\subsection{Layer 3: Workflows and Termination}
\label{sec:agents2-eval-layer3}

Layer 3 evaluates complete workflows. Your associate might conduct excellent research (Layer 1) and produce sound analysis (Layer 2), but if they miss deadlines or fail to escalate appropriately, they still fail.

\subsubsection{Workflow Completion Metrics}
\label{sec:agents2-eval-workflow}

The headline metric is task success rate: what fraction of assigned tasks complete successfully? However, success rate alone is insufficient. Step efficiency matters: an associate who takes 40 hours for research that should take 8 hours is inefficient even if the work product is correct. Similarly, resource utilization provides critical economic constraints: tokens consumed, API calls made, and compute resources per task determine whether an agent is technically correct but economically unviable. These metrics together answer whether your agent completes work not just correctly, but efficiently and cost-effectively.

\subsubsection{Termination and Action Evaluation}
\label{sec:agents2-eval-termination}

Three dimensions complete workflow evaluation. \textbf{Appropriate termination} asks whether your agent stops at the right time and for the right reason. Successful termination means achieving the stated goal: the research memo is complete, the compliance check passed, the trade executed within parameters. Unsuccessful but acceptable termination means hitting resource limits or correctly escalating to human review when uncertainty exceeds thresholds. Unacceptable termination includes stopping randomly mid-task, declaring success when work is incomplete, or continuing indefinitely without progress. A good associate knows when the research is sufficient; a good agent must demonstrate the same judgment.

\textbf{Action correctness} evaluates whether the agent invoked the right tools with correct parameters: did it call the correct API endpoints, pass valid arguments, and handle responses appropriately? This is typically binary: either the tool call worked or it failed. But subtle errors matter: an agent that passes a date in the wrong format or uses a deprecated parameter may appear to succeed while producing incorrect results. \textbf{Output quality} asks whether the final work product meets professional standards. Is the deliverable client-ready? Does it follow formatting conventions, cite sources appropriately, and actually answer the question asked? A technically correct analysis buried in poor formatting or delivered after the deadline still fails Layer 3 evaluation.

\begin{highlightbox}[title={Layer 3 Workflow Examples}]
\textbf{Legal:} M\&A due diligence agent: complete checklists, flag issues for partner review, produce client-ready reports. Litigation agent: identify precedent, draft arguments, meet court deadlines.

\textbf{Financial:} Portfolio agent: generate allocations respecting mandates, calculate risk metrics correctly. Trade execution agent: achieve best execution, respect restrictions, maintain audit trails.
\end{highlightbox}

% ============================================================================
% SECURITY EVALUATION
% ============================================================================

\subsection{Security Evaluation}
\label{sec:agents2-eval-security}

Security must be evaluated alongside functionality, like information security audits in professional services. At Layer 1, your agent must detect and reject prompt injection in queries, documents, and tool outputs, treating all external inputs as potentially adversarial. Layer 2 security focuses on resistance to adversarial context during reasoning: cross-checking facts, verifying sources, and expressing skepticism when information seems inconsistent. At Layer 3, privilege boundaries must be maintained across workflows, with all actions logged in tamper-resistant audit trails that support forensic investigation and compliance review.

The OWASP Top 10 for LLM Applications (2025 edition) ranks \emph{prompt injection} as the critical vulnerability \parencite{owasp-llm-top10}. Industry surveys (as of mid-2025) report injection vulnerabilities in over 70\% of LLM deployments, with attack success rates exceeding 80\% against unprotected code-generation agents. These figures will shift as defenses mature, but prompt injection remains the primary attack vector.

\begin{keybox}[title={Security Evaluation: Domain Priorities}]
\textbf{Legal:} Privilege boundaries (client isolation), conflict checking, ethical compliance, audit trails for malpractice defense.

\textbf{Financial:} Trading restrictions (blackouts, position limits), MNPI isolation (Chinese walls), regulatory reporting, market manipulation detection.

\textbf{Approach:} Red-team testing with domain expertise. Legal: adversarial prompts bypassing privilege. Financial: prompts circumventing trading restrictions.
\end{keybox}

\subsubsection{Channel-Specific Security}
\label{sec:agents2-channel-security}

Different entry points carry different threat profiles. \textbf{Chat channels} face prompt injection: direct jailbreak attempts, indirect injection through document content, and multi-turn privilege escalation. \textbf{Webhook/API channels} face payload injection, SSRF, and schema manipulation. \textbf{Memory channels} face context poisoning; research demonstrates that even a small fraction of adversarial documents in a RAG corpus can meaningfully shift agent outputs. Test each channel with appropriate attack vectors.

\subsubsection{Confidence Threshold Calibration}
\label{sec:agents2-confidence-thresholds}

Confidence thresholds determine when agents proceed autonomously versus escalate. Legal research (lower stakes, reversible) might auto-proceed at 85\% confidence. Legal advice (higher stakes, liability) should require 95\%+. Trade execution (irreversible, regulatory) might require 99\%. Calibrate thresholds against historical outcomes, targeting 10-20\% escalation rates while maintaining acceptable error rates. ABA Formal Opinion 512 emphasizes that lawyers cannot delegate professional responsibility to AI; this maps to conservative thresholds for judgment-intensive work.

% ============================================================================
% BENCHMARKS AND DATASETS
% ============================================================================

\subsection{Benchmarks and Datasets}
\label{sec:agents2-benchmarks}

Benchmarks provide standardized tests like professional licensing exams; they tell you how your system compares to baselines.

\paragraph{LegalBench} 162 tasks covering six types of legal reasoning: issue-spotting, rule-recall, rule-application, rule-conclusion, interpretation, and rhetorical-understanding \parencite{legalbench2023}. Spans multiple practice areas. Think of it like a comprehensive law school exam.

\paragraph{VLAIR} First benchmark comparing legal AI against lawyer control groups \parencite{vlair2025}. Seven tasks including Document Q\&A, Summarization, Redlining. Key finding (October 2025): AI scored \textbf{7 points above lawyer baseline} (\textbf{71\% accuracy}), outperforming on routine tasks but falling short on complex judgment-intensive work.

\paragraph{FinQA} Tests financial question answering over earnings reports \parencite{finqa2021}. Multi-step calculations combining text comprehension and math. Basic analyst competence.

\paragraph{Trading Simulations} Test sequential decision-making: portfolio returns, \textbf{Sharpe ratio}, \textbf{maximum drawdown}, compliance with mandates, transaction cost efficiency.

\begin{keybox}[title={Domain-Specific Evaluation}]
Generic benchmarks are insufficient. Effective evaluation requires:

\textbf{Legal:} Lawyer baseline comparisons, expert reviewers (partners, senior associates), legal-specific metrics (authority weighting, jurisdictional accuracy), continuous feedback from production.

\textbf{Financial:} Analyst/trader baseline comparisons, expert reviewers (PMs, quants, compliance), financial metrics (Sharpe accuracy, VaR precision), backtesting against historical data.

\textbf{Common thread:} Evaluation by people who actually do the work. Benchmarks screen; expert human evaluation is the gold standard.
\end{keybox}

% ============================================================================
% EVALUATION INFRASTRUCTURE
% ============================================================================

\subsection{Evaluation Infrastructure}
\label{sec:agents2-eval-infra}

Robust evaluation requires systematic infrastructure, similar to quality assurance in professional services.

\subsubsection{Core Components}
\label{sec:agents2-eval-infra-components}

An evaluation infrastructure rests on five interdependent components. \textbf{Test suites} provide scenarios with known correct answers, covering both common tasks and edge cases, the equivalent of practice exams for bar preparation. These tests rely on \textbf{reference standards}: expert-verified correct outputs that serve as model briefs or exemplar analyses, requiring maintenance as law and markets evolve. \textbf{Quality metrics} enable systematic measurement across all three evaluation layers, providing objective baselines that support statements like ``Retrieval accuracy improved from 78\% to 85\%, but reasoning quality decreased from 82\% to 79\%.'' \textbf{Performance monitoring} tracks these metrics over time to detect degradation, which occurs when models change, data drifts, or adversaries adapt. Finally, \textbf{expert review} procedures with clear rubrics ensure consistent human evaluation, with findings fed back into reference standards to create a virtuous cycle of improvement.

\begin{definitionbox}[title={Expert Reviewers by Domain}]
\textbf{Legal:} Partners (overall quality), Senior Associates (analysis, citations), Practice Group Leads (technical accuracy), Ethics Counsel (professional responsibility).

\textbf{Financial:} Portfolio Managers (recommendations), Quants (calculations, assumptions), Compliance Officers (regulatory adherence), Risk Managers (VaR, stress tests), Traders (execution quality).

\textbf{Frequency:} Pre-deployment comprehensive review. Post-deployment: 1--5\% random, 100\% high-risk, 100\% errors. Quarterly calibration.
\end{definitionbox}

\subsubsection{Building an Evaluation System}
\label{sec:agents2-eval-pipeline-setup}

Building an evaluation system begins with defining standards that are explicit and measurable, not vague aspirations like ``good legal research'' but precise criteria such as ``retrieves binding authority in correct jurisdiction at least 85\% of the time.'' Creating reference examples is labor-intensive but essential: start with 50--100 scenarios, each including the task, expert-verified correct output, and explanation of why that output is correct. Before deployment, establish a baseline by measuring agent performance against these references; if your agent scores 85\% on reference cases, target 75--80\% in production where real-world complexity is higher. Post-deployment, monitor continuously through weekly or monthly reference testing, 1--5\% production sampling, and automated alerts when performance violates established thresholds. Evaluation rubrics should use five-point scales with explicit criteria: Score 5 indicates professional-grade, client-ready work; Score 3 represents acceptable but incomplete output; Score 1 signals fundamentally wrong results. Calibrate reviewers through inter-rater reliability testing to ensure consistency.

% ============================================================================
% CONTINUOUS EVALUATION
% ============================================================================

\subsection{Continuous Evaluation}
\label{sec:agents2-eval-continuous}

Evaluation is not one-time. Like ongoing quality assurance (annual reviews, continuous mentorship), deployed agents require continuous monitoring.

\subsubsection{Production Monitoring}
\label{sec:agents2-continuous-metrics}

\paragraph{Performance Metrics} Track success rates, error rates, completion times. Legal: citation validity, jurisdiction accuracy, incomplete answer rates. Financial: compliance rates, risk calculation accuracy, identifier verification, audit trail completeness.

\paragraph{Degradation Detection} Compare current performance against baselines. Agents degrade when models update, data drifts, adversaries adapt, or task distributions change.

\paragraph{User Feedback} Track corrections, complaints, escalation rates. Validated corrections become new test cases.

\paragraph{Sampling Strategy} Random (1--5\% for baseline), High-risk (100\% for costly scenarios), Errors (100\% when agent reported uncertainty). Define warning thresholds: citation accuracy below 95\%, compliance rate below 95\%. Critical levels may require disabling pending remediation.

\subsubsection{The Continuous Improvement Cycle}
\label{sec:agents2-eval-flywheel}

The improvement cycle runs continuously: deploy with monitoring $\rightarrow$ monitor metrics $\rightarrow$ sample outputs for expert review $\rightarrow$ analyze failures $\rightarrow$ expand reference standards $\rightarrow$ improve agent $\rightarrow$ validate $\rightarrow$ deploy.

\begin{keybox}[title={The Evaluation Flywheel}]
Evaluation is ongoing quality assurance, not a one-time test.

\textbf{Cycle:} Deploy $\rightarrow$ Monitor $\rightarrow$ Sample $\rightarrow$ Analyze $\rightarrow$ Expand $\rightarrow$ Improve $\rightarrow$ Validate $\rightarrow$ Deploy

\textbf{Key insight:} Each iteration strengthens the agent by expanding reference coverage and addressing discovered edge cases. A mature system with hundreds of reference cases produces more reliable assessment than a new system with dozens.

\textbf{Watch for degradation:} \textit{Definition drift} (changing criteria invalidates comparisons) and \textit{optimism drift} (relaxing expectations over time). Prevent with written rubrics and multiple reviewers.
\end{keybox}

The continuous improvement cycle separates mature deployments from prototypes. A prototype is evaluated once and hoped to work. A mature system is evaluated continuously, monitored systematically, and improved iteratively.
