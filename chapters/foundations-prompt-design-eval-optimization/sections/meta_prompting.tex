% =============================================================================
% Meta-Prompting — Prompt Design, Evaluation, Optimization
% Purpose: Rubrics, self-critique, declarative/programmatic assembly
% Label: sec:llmD-meta
% =============================================================================

\section{Meta-Prompting and Self-Critique}
\label{sec:llmD-meta}

% Outline (comments): rubric prompts; checklists; critique-improve loops; declarative prompt programming
\begin{highlightbox}[title={Rubric Before Solve}]
Define the grading rubric first (criteria and weights), then ask the model to produce and self-evaluate against the rubric before finalizing output.
\end{highlightbox}

\subsection{Declarative/Programmatic Prompting (Preview)}
Represent prompts as data: specifications, schemas, and policies that can be composed, versioned, and tested. Automate assembly and optimization where feasible.

\subsection{Self-Reflection and Error-Driven Repair}
Implement critique → fix loops with explicit error messages (schema violations, missing citations). Cap iterations; log deltas between versions.

\subsection{Bootstrapping Examples and Synthetic Data}
Generate candidate exemplars or labeled samples with strict guardrails; filter by rubrics and human review. Track provenance, licenses, and urldates to avoid leakage and bias.
