% Practical Guide - How to Recognize an Agent
% This section provides actionable tools for evaluation

\section{How to Recognize an Agent}
\label{sec:practical}

With the three-level hierarchy and complete definition established in Section~\ref{sec:intro}, you now have the conceptual foundation. But recognizing agents in practice requires operational tools. This section provides practical approaches: a detailed evaluation rubric, concrete examples comparing agents to non-agents, and guidance for navigating common misconceptions.

\subsection{The 6-Question Evaluation Rubric: Operational Properties}
\label{sec:rubric-6q}

These six questions assess whether an entity qualifies as an \emph{agentic system}. They operationalize the six-property definition established in Section~\ref{sec:intro}. The first three (Goal, Perception, Action) establish \emph{Level 1 agency}; Questions 4--6 add the operational properties required for deployment.

\begin{questionbox}[title={\textbf{Q1. Does it have goals (G)?}}]
\textbf{Test:} Look for clear objectives, task specifications, or performance criteria that direct behavior.

\textit{Domain examples:} Legal research goal (find relevant precedents), portfolio goal (maximize risk-adjusted returns), contract analysis goal (identify liability clauses).

\textit{Falsification:} If it responds identically regardless of desired outcomes, it lacks goals. Examples: compilers, lookup tables.
\end{questionbox}

\begin{questionbox}[title={\textbf{Q2. Does it perceive (P)?}}]
\textbf{Test:} Check for environmental awareness through sensors, APIs, databases, or document access. Can it observe the results of its own actions?

\textit{Domain examples:} Read case law databases, monitor market data feeds, parse contract text, access regulatory filings.

\textit{Falsification:} If it operates identically regardless of environmental state, or cannot observe action consequences, it lacks perception. Examples: write-only systems, open-loop controllers.
\end{questionbox}

\begin{questionbox}[title={\textbf{Q3. Does it act (A)?}}]
\textbf{Test:} Verify capability to affect environment with at least minimal discretion (choosing among two+ actions contingent on perceptions).

\textit{Domain examples:} Generate legal briefs, execute trades, flag contract risks, file regulatory reports, invoke research tools.

\textit{Falsification:} If it cannot modify its environment, or executes exactly one fixed sequence, it lacks action. Examples: pure sensors, zero-conditional scripts.
\end{questionbox}

\begin{questionbox}[title={\textbf{Q4. Does it iterate (I)?}}]
\textbf{Test:} Confirm multiple perceive-act cycles with state preservation across rounds, not single-shot processing. Does it loop: act → observe → act again?

\textit{Domain examples:} Iteratively refine legal queries based on results, adjust portfolio based on market moves, review contract sections sequentially.

\textit{Falsification:} If it processes input once without maintaining state across cycles, it lacks iteration. Examples: single ChatGPT response, batch processors without inter-item updates.
\end{questionbox}

\begin{questionbox}[title={\textbf{Q5. Does it adapt (A)?}}]
\textbf{Test:} Check whether strategy modifies based on accumulated observations and feedback. Does it adjust approach when initial attempts fail?

\textit{Domain examples:} Adjust search terms when queries return no results, modify risk thresholds based on false positive rates, change trade strategies when volatility spikes.

\textit{Falsification:} If it applies identical logic regardless of outcomes, or cannot modify approach when strategies fail, it lacks adaptation. Examples: basic thermostats (fixed rules), static pattern matchers, rigid workflows.
\end{questionbox}

\begin{questionbox}[title={\textbf{Q6. Does it stop/terminate (T)?}}]
\textbf{Test:} Verify clear stopping conditions---implicit (goal satisfaction, exhausted search) or explicit (time limits, resource budgets, escalation triggers, confidence thresholds, maximum iterations).

\textit{Domain examples:} Stop when finding relevant precedent, escalate when confidence is low, halt after maximum API calls, terminate when portfolio balanced.

\textit{Falsification:} If it has no mechanism for recognizing when to stop, or could cycle indefinitely, it lacks termination. Examples: infinite loops, systems requiring manual kill.
\end{questionbox}

\textit{If Q1--Q3 are yes, the entity qualifies as an agent. If all six answers are yes, it qualifies as an agentic system. Otherwise, it may be a useful tool but lacks full agentic character.}

\textbf{Note on Professional Governance:} The six questions above establish whether something is an agent. Professional deployment in high-stakes domains (legal research, medical diagnosis, financial advising) requires additional governance safeguards---attribution, explanation, escalation, and confidentiality. These professional governance requirements are covered in Part III (\textit{How to Govern an Agent}).

\subsection{Common Misconceptions}

Understanding what agents are requires equally understanding what they are \textit{not}. Five common misconceptions lead to over-attribution of agency.

\textbf{Single-Shot Responses Are Not Agentic Systems.} A single response from a language model is not an agentic system, even if sophisticated. Without iteration and adaptation, it's a one-time transformation. Sending one query to ChatGPT and receiving one response demonstrates this limitation—no iteration, no adaptation to results, no perception-action loop. The same research system could be agentic if it iteratively refined queries based on initial results, but a single exchange lacks the fundamental properties required.

\textbf{Automation Alone Is Not an Agentic System.} Automation doesn't imply agency. A data extraction script that automatically runs nightly has goals (extract data) and acts (writes to database), but lacks perception, adaptation, and iteration. It executes a fixed sequence on a schedule. It's a scheduled task, not an agentic system. The automation trigger doesn't create the perception-action loop or adaptive behavior that define agentic systems.

\textbf{Tool Use Alone Is Not Agency.} Calling external tools doesn't automatically make an entity agentic. The critical question is whether the entity iterates on tool results, adapting its strategy based on what it observes. A script that queries an API once is not an agent—it lacks iteration and adaptation. A research system that queries the API, evaluates relevance, and decides what to search next based on results may qualify, depending on whether it completes the perception-action-adaptation cycle.

\textbf{Complexity Is Not Agency.} Complex entities aren't necessarily agents. A document processing pipeline performs sophisticated transformations—optical character recognition, entity extraction, format conversion—but follows a fixed sequence without goals, perception of results, or adaptation. Complexity measures sophistication, not agency. Conversely, simple entities can be agents. A basic thermostat has goals (maintain temperature), perceives (reads sensor), acts (turns heat on/off), iterates (continuous monitoring), and terminates (implicit when target reached), though it lacks adaptation through its fixed on/off rules. Simplicity and agency are orthogonal dimensions.

\textbf{AI-Powered Is Not Agency.} Using AI or machine learning doesn't make something an agent. A neural network that classifies documents in a single forward pass—input to output—is not an agent, regardless of model sophistication. It lacks iteration, adaptation, and the perception-action loop. An AI entity that iteratively reviews content, flags issues, refines assessments based on observed patterns, and adapts its classification strategy across multiple cycles can be an agentic system. The AI component enables flexible reasoning, but agency requires the architectural properties, not merely the presence of neural networks.

\subsection{Examples: Agents vs Non-Agents}

Table~\ref{tab:examples} orders entities along the spectrum of agency, from non-agents through AI-powered agents. Each tier illustrates increasing qualification based on the six core properties. Detailed explanations follow.

\begin{table}[!htb]
\centering
\small
\begin{tabular}{@{}llp{8.2cm}@{}}
\toprule
\textbf{System} & \textbf{Status} & \textbf{Key Properties} \\
\midrule
\rowcolor{bg-note}
\multicolumn{3}{@{}l}{\textbf{Not Agents (Missing Critical Properties)}} \\
Form validation script & No & One-pass validation; lacks goals, perception, and adaptation \\
\midrule
\rowcolor{bg-key}
\multicolumn{3}{@{}l}{\textbf{Agents with Incomplete Agentic System Properties (Missing 1--2)}} \\
Single query to research system & Partial & Has goal, perception, and action; lacks iteration and adaptation \\
Rule-based pattern detection & Partial & Iterates continuously but lacks adaptive strategy \\
Content suggestion system & Partial & Updates per interaction but uses fixed learning approach \\
\midrule
\rowcolor{bg-definition}
\multicolumn{3}{@{}l}{\textbf{Full Agents: Traditional (Rule-Based)}} \\
Smart thermostat & Yes & All 6 properties; learns patterns and adapts behavior \\
Portfolio rebalancing system & Yes & Monitors, adapts to volatility, stops when balanced \\
\midrule
\rowcolor{bg-example}
\multicolumn{3}{@{}l}{\textbf{Full Agents: AI-Powered (Flexible Reasoning)}} \\
Document review system & Yes & Iterative classification with adaptive learning \\
AI research assistant & Yes & Tool loop with query refinement and escalation \\
\bottomrule
\end{tabular}
\caption{Spectrum of agency ordered by increasing qualification, from non-agents through AI-powered systems. \textit{Accessibility:} Classification table with 3 columns (System, Status, Key Properties) and 9 rows in 4 groups. Group 1 (gray background): non-agents lacking critical properties. Group 2 (amber background): agents with incomplete agentic system properties (missing 1--2 of the six). Group 3 (blue background): full traditional agents with all 6 properties via rules/algorithms. Group 4 (green background): full AI-powered agents with all 6 properties via LLMs/neural networks.}
\label{tab:examples}
\end{table}

\paragraph{Not Agents}
These entities lack the minimal properties (goals, perception, action). The \textbf{form validation script} checks inputs against rules in a single pass—it has no independent goals beyond validation, no perception of results, and no adaptation.

\paragraph{Agents with Incomplete Agentic System Properties}
These entities qualify as agents (they exceed the 3-property minimum for Level 1 agency) but lack one or two of the six operational properties required for full agentic systems. \textbf{Rule-based pattern detection} has a goal (flag patterns), perceives incoming data, acts (blocks or alerts), and iterates continuously with each input. However, it typically doesn't adapt its detection strategy within a session—it applies fixed rules. \textbf{Content suggestion systems} similarly have a goal (suggest relevant content), perceive context, act (display suggestions), and iterate with each interaction. Yet they use a fixed learning strategy rather than adapting their suggestions based on user acceptance patterns within a session.

\paragraph{Full Agents: Traditional}
\textbf{Thermostats} illustrate the boundary case. Basic mechanical thermostats have five clear properties: goal (maintain temperature), perception (sensor readings), action (heat on/off), iteration (continuous monitoring), and termination through implicit goal satisfaction (stops heating when target reached). However, they lack true adaptation---they apply fixed rules without adjusting strategy based on results. \textbf{Smart thermostats} qualify as full agentic systems: they learn occupancy patterns, adjust heating schedules based on observed behavior, and implement explicit termination logic like time windows, energy budgets, or away modes. The contrast illustrates how reactive control (basic thermostats) differs from adaptive learning (smart thermostats). \textbf{Portfolio rebalancing systems} demonstrate explicit termination through multiple stopping conditions: goal achievement (balanced portfolio), temporal constraints (market close), or resource limits (maximum trades per session). They monitor market data and portfolio drift, generate trade orders, adapt to volatility within predefined parameters, and stop when any termination condition is met.

\paragraph{Full Agents: AI-Powered}
\textbf{Document review systems} use machine learning to iteratively classify documents based on relevance criteria. Unlike rule-based entities, they improve their categorization as they process more examples, adapting their classification strategy based on observed patterns. \textbf{AI research assistants} demonstrate the most sophisticated agency: they maintain goals (answer questions), observe search results from multiple sources, query iteratively, refine search strategies when initial approaches fail, and escalate to human oversight when encountering contradictory information or reaching confidence limits. This represents the convergence of all six properties enhanced by AI's flexible reasoning capabilities.

\subsection{When to Call Something an Agent}

Our taxonomy (Section~\ref{sec:intro}) distinguishes ``agent'' (3 minimal properties) from ``agentic system'' (6 operational properties). Table~\ref{tab:terminology-usage} provides practical guidance for applying these terms.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{@{}clll@{}}
\toprule
\textbf{Properties} & \textbf{Term} & \textbf{When to Use} & \textbf{Notes} \\
\midrule
3 (G+P+A) & \keyterm{agent} & Meets baseline & Includes thermostats, simple systems \\
6 (all) & \keyterm{agentic system} & Production-ready & Required for professional deployment \\
6 via AI/ML & \keyterm{agentic AI} & AI-powered & AI implements the six properties \\
\cmidrule(lr){1-4}
4-5 & agent with... & Partial implementation & Specify which properties present \\
1-2 & (avoid ``agent'') & Insufficient & Use ``tool,'' ``function,'' ``classifier'' \\
\bottomrule
\end{tabular}
\caption{Terminology usage based on property count. G=goals, P=perception, A=action. \textit{Accessibility:} Decision table with 4 columns (Properties, Term, When to Use, Notes) and 6 rows. Row 1: 3 properties (G+P+A) = agent (baseline). Row 2: 6 properties = agentic system (production-ready). Row 3: 6 via AI/ML = agentic AI. Row 4: 4--5 properties = agent with specific properties noted. Row 5--6: 1--2 properties insufficient for ``agent'' label, use ``tool/function/classifier'' instead.}
\label{tab:terminology-usage}
\end{table}

Precision requirements vary by context. Regulatory filings and academic papers demand explicit enumeration of properties and clear distinction between agent and agentic system. Informal discussions permit looser usage—``agent'' for entities meeting the 3-property baseline, with specific properties noted when relevant: ``has goals and perception but limited adaptation.'' Marketing claims should specify which properties are present rather than using ``agentic AI'' without substantiation.

When in doubt, use the 6-question rubric (\autoref{sec:rubric-6q}). It provides clear, answerable criteria that cut through ambiguity.

\begin{keybox}[colback=bg-key, colframe=key-base, boxrule=3pt, title={\textbf{Key Takeaways}}]
\textbf{At this point you can:}
\begin{itemize}
\item Define agents and agentic systems using the 6-property framework
\item Recognize agents using the 6-question rubric
\item Distinguish agents from sophisticated non-agents
\item Evaluate marketing claims about ``agentic AI''
\item Use terminology correctly in professional contexts
\end{itemize}

\textbf{Remaining sections provide:} Historical evolution (Section~\ref{sec:history}), disciplinary perspectives (Section~\ref{sec:disciplines}), analytical dimensions (Section~\ref{sec:dimensions}), and synthesis (Section~\ref{sec:furtherlearning}).
\end{keybox}

\bigskip
\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}
