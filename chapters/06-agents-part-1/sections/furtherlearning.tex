\section{Conclusion}
\label{sec:furtherlearning}

This chapter synthesized seven decades of scholarship from eight disciplines—philosophy, psychology, law, economics, cognitive science, complex systems, computer science, and AI—into practical frameworks for understanding and evaluating agency. You now have conceptual tools for cutting through vendor hype, evaluating AI products, and understanding what distinguishes genuinely agentic systems from sophisticated chatbots.

\subsection{The Framework You Now Have}

The three-level hierarchy provides your foundational mental model. An \keyterm{agent} is an entity requiring three minimal properties: goals, perception, and action. This baseline is deliberately broad—thermostats, organizations, and humans all qualify. An \keyterm{agentic system} is an agent with three more properties essential for reliable professional deployment: iteration across multiple steps, adaptation through learning or feedback, and termination via explicit or implicit stopping conditions. \keyterm{Agentic AI} implements all six properties using artificial intelligence rather than traditional programming. This hierarchy clarifies what entities actually do, regardless of marketing labels.

The six-question evaluation rubric makes the framework actionable: Does it have goals? Does it perceive? Does it act? Does it iterate? Does it adapt? Does it stop? Apply these questions to vendor claims. Entities failing the iteration test—no matter how sophisticated their single-shot responses—aren't agentic systems. Entities lacking clear termination mechanisms raise operational risks. The rubric transforms theoretical distinctions into practical evaluation.

Four analytical dimensions organize how disciplines define agency differently. The \keyterm{autonomy spectrum} ranges from delegated authority to self-directed initiative. \keyterm{Entity frames} span humans to institutions to machines. \keyterm{Goal dynamics} progress from accepting given objectives to negotiating them. \keyterm{Persistence requirements} distinguish one-shot interactions from ongoing processes. These dimensions explain why philosophy emphasizes intentionality while AI emphasizes tool orchestration—different disciplines inhabit different regions of this conceptual space, but all contribute legitimate insights.

\subsection{Seven Decades of Convergence}

This framework synthesizes scholarship from \textcite{anscombe1957intention}'s analysis of practical knowledge (1957) through \textcite{russellnorvig2020aima}'s perception-action loops to \textcite{willison-2025-agents-definition}'s tools-in-a-loop consensus (2025). Early definitions centered on humans or human-AI relationships, rooted in philosophy, psychology, law, and economics. The 1990s computational revolution expanded entity frames to pure machines. The LLM era sharpened around ``tools-in-a-loop'' architectures while introducing hybrid framings where humans provide goals and oversight while AI executes multi-step tasks.

Willison's practitioner synthesis—``LLM agent runs tools in a loop to achieve a goal''—captures emerging consensus across research \parencite{yao2022react}, commercial frameworks (LangChain, LlamaIndex), and vendor implementations. This convergence grounds our framework in both theoretical foundations and operational reality. The concepts are old; the technologies are new; the challenge is synthesis.

\subsection{Putting the Framework to Work}

For legal and financial applications, general agency definitions require additional safeguards: attribution to authoritative sources, auditable provenance, escalation protocols recognizing human judgment needs, and confidentiality mechanisms protecting privilege. These augment rather than replace the six-property framework—they define what makes entities suitable for high-stakes professional work.

\begin{highlightbox}[title={\textbf{For Practitioners: Apply the Framework Monday Morning}}]
\begin{itemize}[nosep,leftmargin=1.5em]
\item Evaluate vendor claims using the six-question rubric—demand evidence for each property
\item Recognize that higher autonomy brings both capability and risk—align deployment with organizational readiness
\item Insist on attribution and provenance mechanisms—absent these, verification becomes prohibitive
\item Treat AI agents as you would junior associates: supervise, verify, correct
\end{itemize}
\end{highlightbox}

\begin{highlightbox}[title={\textbf{For Researchers: Build on These Foundations}}]
\begin{itemize}[nosep,leftmargin=1.5em]
\item Develop compre\-hensive benchmarks assessing agentic properties beyond accuracy—iteration quality, adaptation effectiveness, escalation appropriateness
\item Study real-world deployments to understand adoption patterns and failure modes
\item Address responsibility attribution when high-autonomy agents err—traditional agency law provides foundations but novel questions demand new frameworks
\item Extend cross-disciplinary synthesis to medical AI, educational AI, and other professional domains
\end{itemize}
\end{highlightbox}

\begin{highlightbox}[title={\textbf{For Regulators: Regulate Architecture, Not Marketing}}]
\begin{itemize}[nosep,leftmargin=1.5em]
\item Base regulations on architectural properties (autonomy level, capability level) rather than vendor labels
\item Require transparency mechanisms enabling verification of attribution and provenance claims
\item Mandate human oversight aligned with autonomy levels—higher autonomy demands stronger oversight
\item Coordinate with professional ethics bodies to align AI governance with fiduciary and professional responsibilities
\end{itemize}
\end{highlightbox}

\paragraph{Connections within this textbook} Part II (\textit{How to Build an Agent}) explores architectures like ReAct and Reflexion in depth. Part III (\textit{How to Govern an Agent}) addresses professional safeguards, liability frameworks, and regulatory approaches. Additional chapters cover evaluation methods, deployment patterns, and domain-specific considerations for legal practice and financial services.

\paragraph{Recommended reading} For deeper engagement: \textcite{bratman1987intention} (philosophical BDI foundation), \textcite{bandura1989human} (psychological agency), \textcite{restatement2006agency} (legal foundations), \textcite{jensen1976theory} (principal-agent theory), \textcite{russell2010artificial} (comprehensive AI), \textcite{wooldridge2009introduction} (multi-agent systems), and \textcite{xi2023rise} (LLM-based agents).

\subsection{Why This Matters}

Definitional clarity carries high stakes. When a legal AI makes consequential errors, who bears responsibility? When an agent escalates inappropriately, what architectural flaw enabled it? When capabilities outpace oversight capacity, how do we ensure accountability? These questions require shared vocabulary for discussing autonomy, entity frames, goal dynamics, and termination guarantees. Without clear definitions, we cannot write enforceable regulations, establish professional standards, or hold entities accountable.

The journey from ``driving cattle'' to ``pleading a case''—both senses of Latin \textit{agō}—to ``running tools in a loop'' spans millennia of human intellectual development and decades of computational innovation. The conceptual foundations traced here continue through the remaining chapters of this textbook and into your professional practice. Apply them rigorously. The technology evolves rapidly; the principles endure.
