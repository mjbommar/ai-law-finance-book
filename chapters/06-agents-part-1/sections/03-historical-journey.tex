\section{A Historical Journey: Defining Agents Across Seven Decades}
\label{sec:history}

The concept of \keyterm{agency} has evolved dramatically since the mid-20th century, shaped by philosophical inquiry, economic theory, legal doctrine, and technological capability. This section traces agent definitions chronologically, revealing how each level of the hierarchy from Section~\ref{sec:intro} emerged historically: philosophical and legal foundations addressed general agency (Level 1), the computer science revolution instantiated these concepts in software (Level 2), and the LLM era added flexible reasoning powered by AI (Level 3). Figure~\ref{fig:timeline} provides a visual overview of key milestones.

\begin{highlightbox}
\textbf{On Etymology.} The semantic complexity surrounding ``agent'' traces back to the word's origins. The Latin root \textit{agō}, from which \textit{agent} derives, had multiple meanings in classical Latin. With numerous distinct senses in Lewis \& Short's classical dictionary, ranging from ``driving cattle'' to ``pleading a case,'' contemporary definitional debates echo longstanding ambiguity \parencite{LewisShort1879}.
\end{highlightbox}

\input{figures/timeline}

\subsection{The Origin and Evolution of Agency Law}

Agency law, which governs the relationships between principals and agents where one party acts on behalf of another, has roots tracing back to ancient civilizations. In Roman law, the concept emerged through institutions like the "mandatum," a gratuitous contract allowing one person to manage affairs for another without compensation, and the "procurator," who handled legal matters on behalf of a principal. This framework influenced early European legal systems, particularly during the Middle Ages, when mercantile practices in Italy and England began formalizing agency in commercial transactions. Further evolution of doctrines in agency law took place during the Industrial Revolution in the 18th and 19th centuries, as expanding commerce necessitated clearer rules for delegation and liability. More recently, various editions of the Restatement of Agency have been published by the American Law Institute. These foundational treatises have codified and clarified the common law principles governing the creation, scope, and termination of agency relationships between principals and agents.

\subsection{Modern Philosophical and Legal Foundations (1957--1969)}

The postwar period witnessed foundational philosophical work on action theory alongside the maturation of legal doctrine concerning agency relationships. These developments occurred independently yet addressed complementary questions: philosophy examined how intention relates to action and how we attribute actions to agents, while law formalized the conditions under which one party acts on behalf of another.

\begin{definitionbox}[importance=low,title={\textbf{1957: Intention}}]
G.E.M. Anscombe's \textit{Intention} established that actions are \keyterm{intentional} ``under a description'' and known through practical knowledge \parencite{anscombe1957intention}. This introduced the idea that intention plays a basic explanatory role: we understand actions by understanding the reasons under which agents perform them.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1958: Legal Agency}}]
The \textit{Restatement (Second) of Agency} defined \keyterm{agency relationship}: a principal manifests assent that an agent shall act on the principal's behalf and subject to control \parencite{restatement1958agency}. This emphasizes delegation, fiduciary relation, and control—concepts relevant when discussing AI systems acting on behalf of humans.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1963: Causal Theory of Action}}]
Donald Davidson's ``Actions, Reasons, and Causes'' defended the \keyterm{causal theory of action}, arguing that intentional actions are explained by an agent's primary reason (belief–desire pair) that causally produces the action \parencite{davidson1963actions}. This framework connected mental states to observable behavior.
\end{definitionbox}

These three works established complementary foundations for thinking about agency. Philosophy offered frameworks for intention and the causal structure of action, while law formalized delegation and control in principal–agent relationships. Together, they set up core tensions that would animate later debates: the relationship between mental states and observable behavior, the balance between autonomy and control in delegated relationships, and the explanatory role of reasons versus causes. When researchers began implementing computational agents in subsequent decades, they inherited both the conceptual resources and these unresolved tensions—questions that remain live today when we ask whether AI systems truly have intentions or merely simulate goal-directed behavior.

\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}

\subsection{Economic and Social Theory (1970--1989)}

The 1970s and 1980s witnessed agency concepts migrating from philosophy and law into the social sciences. Economics, sociology, and psychology each adapted the core ideas to their disciplinary concerns, generating new frameworks for understanding human behavior in organizations, social structures, and economic relationships. This period produced a remarkably diverse set of perspectives on agency, from Milgram's minimalist ``agentic state'' where individuals function as mere instruments, to Bandura's rich conception of human agency with intentionality, forethought, and self-regulation.

\begin{definitionbox}[importance=low,title={\textbf{1974: The Agentic State}}]
Stanley Milgram's \textit{Obedience to Authority} introduced the ``\keyterm{agentic state}''---individuals seeing themselves as instruments carrying out another's wishes \parencite{milgram1974obedience}. This anchors one end of the autonomy spectrum: agents as pure delegates without independent judgment.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1976: Principal-Agent Economics}}]
Building upon earlier work by Ronald Coase \parencite{coase1937nature}, Jensen and Meckling's ``Theory of the Firm'' formalized the \keyterm{principal-agent relationship}, defining it as a contract where principals engage agents with delegated decision-making authority \parencite{jensen1976theory}. This established vocabulary of agency costs, information asymmetry, and incentive alignment.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1984: Structuration Theory}}]
Anthony Giddens framed agency as the capacity to make a difference and intervene in the world \parencite{giddens1984constitution}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1986: Society of Mind}}]
Marvin Minsky's \textit{Society of Mind} popularized the idea of mind as a society of simple ``agents'' whose interactions generate intelligence, introducing the \keyterm{multi-agent system} metaphor \parencite{minsky1986society}.
\end{definitionbox}

\begin{definitionbox}[unbreakable,importance=low,title={\textbf{1987: Planning Theory}}]
Michael Bratman's \textit{Intention, Plans, and Practical Reason} developed a planning theory emphasizing partial plans structuring practical reasoning \parencite{bratman1987intention}. This introduced the Belief-Desire-Intention (BDI) framework that became foundational for intelligent agent architectures.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1987: The Intentional Stance}}]
Daniel Dennett's \textit{The Intentional Stance} argued we can treat entities as agents when attributing beliefs and desires yields reliable predictions---providing a pragmatic criterion for agency \parencite{dennett1987intentional}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1989: Human Agency}}]
Albert Bandura synthesized psychological research, defining human agency through intentionality, forethought, self-regulation, and self-reflectiveness---properties increasingly sought in artificial systems \parencite{bandura1989human}.
\end{definitionbox}

These seven works collectively established a spectrum of autonomy that would prove essential for understanding computational agents. At one extreme, Milgram's agentic state described humans functioning as mere instruments executing another's directives—agents as pure delegates without independent judgment. At the other extreme, Bandura enumerated the rich capacities of human agency: forming intentions, planning ahead, monitoring and regulating behavior, and reflecting on one's own efficacy. Between these poles, each discipline tackled different facets of the agency problem. Economics formalized principal-agent relationships as contracts with delegated decision-making but misaligned incentives, focusing on control mechanisms and information asymmetries. Sociology examined how agents act independently within structural constraints. Cognitive science explored how minds might emerge from societies of simpler agents. Philosophy developed practical frameworks for planning, prediction, and attributing mental states.

This diversity reflected genuine differences in disciplinary focus, but the frameworks proved complementary rather than contradictory. When researchers began implementing computational agents in the 1990s, they inherited this rich conceptual toolkit and translated it into software architectures. Bratman's BDI framework became the dominant architecture for intelligent agents. Dennett's intentional stance provided a pragmatic criterion for when to treat systems as agents—when doing so yields reliable predictions. Bandura's enumeration of human agency properties became a checklist for evaluating artificial systems. The spectrum from minimal delegation to full autonomy helped designers understand where their systems fell and what capabilities they still needed to develop.

\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}

\subsection{The Computer Science Revolution (1990--1999)}

The 1990s transformed agency from philosophical concept to computational reality. Previous decades had provided rich conceptual frameworks—intention, planning, autonomy, perception-action cycles—but these primarily described human or abstract theoretical entities. The convergence of distributed systems research, AI advances, and the explosive growth of the internet created conditions for a decisive shift: researchers began implementing agents directly in software, creating systems designed from the ground up as autonomous entities rather than traditional programs.

This transition posed fundamental translation challenges. How should beliefs and intentions be represented computationally? What distinguishes an autonomous agent from a reactive program? When do interactions among simple agents produce emergent intelligence? The decade's answers emerged through diverse implementations, each formalizing different aspects of the prior decades' conceptual toolkit. By decade's end, agent-oriented computing had established itself as a distinct paradigm with its own methods, architectures, and standards.

\begin{definitionbox}[importance=low,title={\textbf{1991--1993: Agent-Oriented Programming}}]
Yoav Shoham's AGENT0 and ``Agent-Oriented Programming'' implemented agents as computational entities characterized in mentalistic terms (beliefs, commitments, obligations) \parencite{shoham1993aop}. This established \keyterm{agent-oriented programming} as distinct from object-oriented programming and was closely related to the BDI (Belief-Desire-Intention) architecture.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1994: Learning Interface Agents}}]
Pattie Maes pioneered \keyterm{learning interface agents}—adaptive software assistants that reduce work and information overload by learning user preferences through observation, feedback, and collaborative filtering \parencite{maes1994agents}. Her work on systems like Ringo (music recommendations) demonstrated personalization through learning, introducing adaptation as a core agentic capability \parencite{shardanand1995sif}.
\end{definitionbox}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.38\textwidth]{figures/maes-1994-clipping.png}
  \caption{\textbf{Maes (1994): A prescient vision.} The opening of Maes's seminal CACM article outlined agents that learn user preferences, provide personalized assistance, and shift interaction from ``direct manipulation'' to collaborative delegation—patterns that resurface three decades later in modern LLM-based agentic systems.}
  \label{fig:maes-1994-clipping}
\end{figure}

Three major 1995 publications crystallized distinct approaches to defining computational agents, each addressing the translation challenge from a different angle:

\begin{definitionbox}[importance=low,title={\textbf{1995: Intelligent Agents}}]
Wooldridge \& Jennings: \textbf{Intelligent agents} exhibit autonomy, reactivity, proactivity, and social ability \parencite{wooldridge1995intelligent}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1995: Perception–Action Agents}}]
Russell \& Norvig: An \textbf{agent} perceives its environment through sensors and acts through actuators \parencite{russellnorvig1995ai}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1995: Complex Adaptive Systems}}]
Holland: \textbf{Complex adaptive system agents} exhibit local interactions under simple rules that produce emergent behavior \parencite{holland1995cas}.
\end{definitionbox}

The remaining years extended these frameworks into specialized domains and established enduring computational paradigms:

\begin{definitionbox}[importance=low,title={\textbf{1996: Autonomous Agents}}]
Franklin and Graesser distinguished \keyterm{autonomous agents} as entities situated within an environment that sense and act over time in pursuit of their own agendas \parencite{franklin-graesser-1997} (ATAL'96; published 1997 in LNCS).
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1996: Agent-Based Modeling}}]
Epstein and Axtell's \textit{Growing Artificial Societies} established agent-based modeling (ABM), defining agents as autonomous heterogeneous individuals whose local interactions produce emergent macro patterns \parencite{epstein1996growing}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1998: Reinforcement Learning}}]
Sutton and Barto's \textit{Reinforcement Learning} defined agents maximizing cumulative reward through environmental interaction, establishing the \keyterm{RL framework} \parencite{suttonbarto1998rl}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1999: Multi-Agent Systems}}]
A textbook by Jacques Ferber and an edited volume by Gerhard Weiss consolidated \keyterm{multi-agent systems (MAS)}, defining agents as autonomous problem-solving entities that cooperate, compete, and negotiate to achieve individual and collective goals \parencite{weiss1999mas,ferber1999mas}.
\end{definitionbox}

The decade's diverse frameworks addressed the translation challenges from complementary angles. To the question of representing beliefs computationally, Shoham's agent-oriented programming offered mentalistic implementations (beliefs, commitments, obligations). To the question of observable properties distinguishing agents from programs, Wooldridge and Jennings enumerated autonomy, reactivity, proactivity, and social ability. To functional architecture, Russell and Norvig established the perception-action cycle. To learning and adaptation, Maes demonstrated how agents could improve through observation and feedback. And to emergence, Holland and the ABM community showed how simple local rules generate collective intelligence. These were not competing theories but different levels of description—internal architectures, observable properties, functional patterns, adaptive mechanisms, and emergent phenomena.

Two computational paradigms proved particularly durable. Agent-based modeling demonstrated how to generate social phenomena from individual interactions, bridging agent theory and social science applications. Reinforcement learning formalized agents as reward maximizers learning through environmental feedback, a framework that would scale far beyond the decade's initial demonstrations. By 1999, multi-agent systems had matured into a distinct field with established conferences, textbooks, and architectural patterns. These 1990s frameworks—mental states, observable properties, perception-action cycles, learning mechanisms, and emergence—would shape agent definitions for the next two decades, until large language models introduced qualitatively new capabilities.

\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}

\subsection{Consolidation and Formalization (Late 1990s--2019)}

The early 21st century marked a period of consolidation rather than conceptual revolution. Following the 1990s explosion of agent definitions and implementations, the field matured through educational resources that trained new generations of researchers, reference works that synthesized accumulated knowledge, and successful demonstrations that validated the frameworks at scale. The dot-com boom and bust, the rise of Web 2.0, mobile computing, and eventually the deep learning revolution all occurred during this period, yet they produced remarkably few new definitional frameworks for agency itself.

Two early definitional contributions framed the era's concerns. Kauffman raised fundamental questions about the relationship between physical and virtual agency, proposing thermodynamic criteria that most software agents clearly failed to meet. Jennings, Sycara, and Wooldridge formalized agent-oriented software engineering, translating the 1990s research insights into practical development methodologies. Beyond these, the period's major developments were consolidative: Wooldridge's \textit{Introduction to MultiAgent Systems} and Epstein's \textit{Generative Social Science} codified best practices, legal updates like the Restatement (Third) of Agency maintained doctrinal continuity, and reference works like the Stanford Encyclopedia entries provided authoritative syntheses.

The period's most dramatic developments came from demonstrations of existing frameworks at unprecedented scale. Deep reinforcement learning combined neural networks with the RL framework established by Sutton and Barto in 1998, achieving superhuman performance first in Atari games and then notably with AlphaGo's victories over world champions. These successes validated that the 1990s conceptual frameworks could scale to complex domains without requiring fundamental redefinition. By 2019, the field had mature definitions, established pedagogical resources, and powerful demonstrations—but also growing awareness that large language models might catalyze the next definitional shift.

\begin{definitionbox}[importance=low,title={\textbf{2000: Physical Agency}}]
Stuart Kauffman's \textit{Investigations} proposed thermodynamic criteria: natural autonomous agents must reproduce and perform thermodynamic work cycles, highlighting tensions between embodied and virtual agency \parencite{kauffman2000investigations}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{1998 Roadmap: Agent Definition Consensus}}]
Jennings, Sycara, and Wooldridge's 1998 research roadmap synthesized the agent research community's consensus: an agent is ``an encapsulated computer system that is situated in some environment, and that is capable of flexible, autonomous action in that environment in order to meet its design objectives'' \parencite{jennings1998roadmap,wooldridge1995intelligent}. As a historical definition from computer science, they used ``system'' to mean a computational entity. This definition grounded agent-oriented software engineering (AOSE) work throughout the 2000s.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{2006: Restatement (Third) of Agency}}]
The \textit{Restatement (Third) of Agency} updated legal doctrine while retaining core concepts of fiduciary duty and delegated authority \parencite{restatement2006agency}.
\end{definitionbox}

\begin{definitionbox}[unbreakable,importance=low,title={\textbf{2013--2015: Deep Reinforcement Learning}}]
Deep RL combined neural networks with established RL frameworks, achieving superhuman performance in complex domains. DeepMind's DQN mastered Atari games from raw pixels \parencite{mnih2015humanlevel}, and AlphaGo defeated world champions at Go \parencite{silver2016alphago}, demonstrating that learned agents could exceed human capabilities in domains previously considered intractable.
\end{definitionbox}

The period's consolidation reflected the 1990s frameworks' success—they fit the technological capabilities available. Wooldridge's \textit{Introduction to Multi\-Agent Systems} and Bonabeau's agent-based modeling syntheses trained new generations of practitioners \parencite{wooldridge2009introduction,bonabeau2002abm}. Epstein's \textit{Generative Social Science} articulated ABM's explanatory program: explaining phenomena through simple rule-based actors whose local interactions generate macro patterns \parencite{epstein2006generative}. Stanford Encyclopedia entries on ``Agency'' and ``Action'' provided philosophical syntheses \parencite{sep-agency,sep-action}. By 2020, Russell and Norvig's 4th edition of AIMA had refined agent definitions incorporating ML progress \parencite{russellnorvig2020aima}.

Kauffman's thermodynamic criteria highlighted the still-unresolved tension between physical and virtual agency—a question that remains contentious as purely software-based LLM agents proliferate. Jennings and colleagues' software engineering formalization helped practitioners but did not redefine agency itself. Deep RL validated existing frameworks at unprecedented scale: agents learning through environmental interaction could achieve superhuman performance, confirming Sutton and Barto's 1998 framework rather than replacing it. The stage was set, however, for disruption. Large language models' capacity for flexible reasoning, few-shot learning, and natural language understanding would soon enable agent architectures qualitatively different from the hand-coded systems of previous decades.

\noindent\textcolor{border-neutral}{\rule{\textwidth}{1.5pt}}

\subsection{The LLM Era (2020--2025)}

Large language models brought capabilities that catalyzed a new wave of agent definitions. GPT-3's release in 2020 demonstrated few-shot learning and flexible reasoning at scale. ChatGPT's public launch in 2022 made conversational AI mainstream. But the definitional shift came not from language modeling itself, but from discovering that LLMs could orchestrate tool use, maintain working memory, and iteratively refine approaches to achieve complex goals. This combination—flexible reasoning plus tool use—enabled agent architectures qualitatively different from previous decades' hand-coded systems.

The key insight emerged in 2022: LLMs could implement the classical perception-action cycle not through programmer-specified rules but through learned parameters. The ReAct pattern (Reasoning and Acting) demonstrated LLMs interleaving chain-of-thought reasoning with tool use, iteratively planning, acting, and observing until goals were achieved. This simple pattern proved general, spurring implementations across research labs and commercial frameworks. Unlike previous decades' diversity of agent architectures, the LLM era quickly converged on a dominant pattern: an LLM iteratively calling tools, observing results, updating state, and choosing next actions.

This architectural convergence reflects several factors: the flexibility of LLMs as general-purpose reasoners, the natural fit between language models and tool APIs, and the economic incentives driving rapid commercialization. Where 1990s agent definitions emerged from academic research programs over a decade, LLM agent definitions coalesced in months through practitioner experimentation and commercial deployment. The speed of convergence is unprecedented, though whether this pattern will prove as durable as the 1990s frameworks remains to be seen.

\begin{definitionbox}[importance=low,title={\textbf{2022: The LLM-as-Agent Pattern}}]
Yao et al.'s ``ReAct: Synergizing Reasoning and Acting in Language Models'' showed that LLMs can interleave chain-of-thought reasoning with tool use, iteratively planning, acting, and observing to achieve goals \parencite{yao2022react}. This catalyzed the ``\keyterm{LLM-as-agent}'' pattern in contemporary practice. As Simon Willison concisely puts it: ``an LLM agent runs tools in a loop to achieve a goal'' \parencite{willison-2025-agents-definition}.
\end{definitionbox}

Framework documentation adopted this pattern explicitly. LangChain defines agents as entities that use LLMs to iteratively call tools until a stop condition is met \parencite{langchain-agents-docs}. OpenAI's Agents SDK similarly describes agents as LLMs configured with instructions and tools, operating in loops where the LLM decides which tools to call based on iterative feedback \parencite{openai-agents-sdk}. This convergence across academic research and commercial frameworks reflects rapid standardization around a single architectural pattern.

\begin{definitionbox}[importance=low,title={\textbf{2023: Generative Agents}}]
Park et al.'s ``Generative Agents'' showcased LLM-powered agents simulating believable human behaviors over long horizons via memory, planning, and reflection, popularizing sandboxed LLM agents in society simulations \parencite{park2023generative}.
\end{definitionbox}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.85\textwidth]{figures/park-2023-generative-agents.png}
  \caption{\textbf{Park et al.~(2023): Generative agents in Smallville.} Twenty-five LLM-powered agents inhabit a sandbox environment inspired by \textit{The Sims}, each with memories, daily routines, and social relationships. The architecture extends an LLM with three core components: a memory stream storing experiences in natural language, a reflection mechanism synthesizing memories into higher-level insights, and a planning module generating behavior dynamically. In evaluation, agents exhibited emergent social behaviors---autonomously spreading party invitations, forming new acquaintances, coordinating attendance, and asking each other on dates---all from a single seed notion that one agent wanted to throw a Valentine's Day party. This demonstrated that LLM-based agents could sustain coherent, believable behavior over extended time horizons. Source: \textcite{park2023generative}, Figure 1.}
  \label{fig:park-2023-generative-agents}
\end{figure}

\begin{definitionbox}[importance=low,title={\textbf{2023: The Rise of LLM Agents}}]
Xi et al.'s survey ``The Rise and Potential of Large Language Model Based Agents'' documented the emerging architectural consensus: entities using LLMs to iteratively call tools, observe results, and adapt until goals are achieved \parencite{xi2023rise}.
\end{definitionbox}

\begin{definitionbox}[importance=low,title={\textbf{2024--2025: Commercial Frameworks}},breakable=false]
LangChain defines agents as entities using LLMs to iteratively call tools until a stop condition is met \parencite{langchain-agents-docs}. OpenAI's Agents SDK describes agents as LLMs configured with instructions and tools, operating in loops where the LLM decides which tools to call based on iterative feedback \parencite{openai-agents-sdk}. This convergence represents an emerging architectural consensus across research and commercial frameworks.
\end{definitionbox}

The LLM era's definitional landscape differs from previous periods. Rather than multiple competing frameworks addressing different facets of agency, the field quickly converged on architectural patterns exploiting LLMs' flexible reasoning. ReAct established the tool-orchestration template in 2022, and within months implementations proliferated across academia and industry. Park's generative agents demonstrated a complementary pattern—memory, reflection, and planning enabling sustained believable behavior in social simulations—while commercial frameworks embedded tool orchestration as default infrastructure.

What is genuinely new remains debated. Skeptics note that the perception-action cycle, goal-directed behavior, and tool use all appeared in 1990s definitions—the LLM era implements these through learned parameters rather than programmer-specified logic. Proponents argue that flexible reasoning over natural language representations enables qualitatively different agentic capabilities. A plausible resolution is that LLMs shifted where decision-making knowledge resides (from code to weights) without fundamentally changing the functional architecture of agency. Regardless, the speed of convergence—from research demonstration to industry standard in under three years—marks the LLM era as distinctive in the seven-decade history of agent definitions.

\subsection{Historical Patterns}

Five patterns emerge across seven decades:

\paragraph{Broadening entity frames}
Early definitions focused on humans or human–AI relationships. By the 1990s, purely computational agents became common. The LLM era introduced hybrid framings: humans provided goals, AI systems executed multi-step tasks.

\paragraph{Increasing autonomy}
Definitions migrated from delegated proxies (1958) through perception-action systems (1995) to self-directed tool orchestrators (2025). The locus of decision-making shifted progressively toward the agent, reflecting evolving expectations of autonomy.

\paragraph{From mental states to observable behavior}
Philosophical definitions emphasized intention and reasoning. Computer science implemented these as mentalistic constructs. Contemporary definitions focus on observable capabilities—tool use, iterative problem solving, task completion.

\paragraph{``Tools-in-a-loop'' convergence}
The AI engineering community has converged toward iterative tool orchestration patterns, documented in practitioner accounts \parencite{willison-2025-agents-definition} and academic surveys \parencite{xi2023rise}. This reflects technological maturity (LLMs capable of reliable tool use), research foundations (ReAct and related frameworks), and market pressures (customers evaluating empirical capabilities). While not yet a formal standard, this architectural pattern dominates contemporary implementations.

\paragraph{From hand-coded to learned behavior}
Early agents used explicit rules and plans specified by programmers. Deep RL demonstrated agents learning behaviors through environmental interaction. LLMs shifted the architectural locus of planning and orchestration from programmer-specified logic to learned parameters, changing where decision-making knowledge resides rather than the fundamental agentic capabilities.

\medskip
These patterns help explain why contemporary definitions vary: they reflect different points in this evolutionary trajectory and different disciplinary emphases.

The next section examines how different disciplines approached agency, revealing persistent tensions and complementary insights.
