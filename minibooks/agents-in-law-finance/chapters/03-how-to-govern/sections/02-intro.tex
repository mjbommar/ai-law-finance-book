% ============================================================================
% Introduction --- Chapter 3 - How to Govern an Agent (Govern)
% Purpose: Establish governance imperative and map properties to requirements
% Label: sec:agents3-intro
% ============================================================================

\section{Introduction}
\label{sec:agents3-intro}

The governance imperative is clear: software has always required governance. We audit code, review changes, test deployments, and maintain access controls. Yet the governance challenges posed by agentic systems differ in \emph{kind}, not merely degree. Understanding this shift begins with recognizing what makes agents fundamentally different from the passive tools that dominate enterprise software today, and why those differences create accountability obligations that traditional governance structures were not designed to address.

\subsection{From Tools to Agents}
\label{sec:agents3-tools-to-agents}

Most enterprise software operates as a passive tool: you invoke it, it executes a predetermined sequence, and it stops. Spreadsheets recalculate when you enter data, databases return results when you query them, and compilers translate source code when you run them. These tools are \keyterm{reactive}: waiting for explicit human commands, executing well-defined operations, and producing outputs that can be traced directly to inputs and logic paths.

Governance for passive tools focuses on \emph{authorization} (who can invoke the tool), \emph{configuration} (what parameters are allowed), and \emph{validation} (does the output match expectations). When a spreadsheet miscalculates, we examine the formulas. When a database returns incorrect results, we inspect the query and schema. The causal chain from invocation to outcome is short, deterministic, and observable.

Agents introduce \keyterm{Goal}, \keyterm{Perception}, and \keyterm{Action}: the GPA properties from Chapter~1. As Chapter~1 established, \emph{agents} (Level 1) possess these three minimal properties, while \emph{agentic systems} (Levels 2/3) add Iteration, Adaptation, and Termination (required for production deployment). An agent is not merely invoked; it is assigned an objective. It does not passively wait for instructions; it perceives its environment, evaluates possible actions, and selects behaviors designed to advance its goal.

This autonomy creates three immediate accountability challenges:

\begin{enumerate}
\item \textbf{Purpose Drift}: A tool does what you tell it to do. An agent interprets what you \emph{want} it to achieve. If the goal specification is ambiguous, incomplete, or misaligned with actual intent, the agent may pursue objectives you did not intend. Effective oversight requires verifying goal alignment before deployment and monitoring for drift during operation.

\item \textbf{Perceptual Opacity}: Agents make decisions based on what they perceive. If perception is incomplete, biased, or adversarially manipulated, actions may be inappropriate even if the goal is well-specified. Unlike a passive tool whose inputs are explicit function parameters, an agent's perceptual inputs may include external data sources, sensor readings, or inferred environmental state. Organizations must establish \emph{input validation}, \emph{data provenance}, and \emph{bias detection} mechanisms.

\item \textbf{Actuation Risk}: Agents take actions that affect their environment, such as filing documents, executing trades, or sending communications. Unlike passive tools that produce outputs for human review, agents \emph{do things}. If an agent's action set includes high-consequence operations (e.g., signing contracts, disbursing funds, disclosing confidential information), governance must enforce \emph{approval gates}, \emph{actuation constraints}, and \emph{rollback capabilities}.
\end{enumerate}

\textit{Scope note:} This chapter addresses governance challenges specific to \textbf{agentic systems}: AI systems exhibiting all six GPA+IAT properties. Broader AI governance questions (foundation model evaluation, training data provenance, algorithmic fairness in non-agentic classifiers) will be addressed in a forthcoming companion volume.

\begin{keybox}[title={The GPA Governance Gap}]
Traditional software governance assumes human-in-the-loop execution: humans decide when to invoke tools, interpret outputs, and take consequential actions. GPA properties move decision-making \emph{inside} the system boundary. This demands a shift from \emph{access control} to \emph{behavioral oversight}.
\end{keybox}

Many of these governance controls (goal authorization, perceptual validation, actuation constraints) mirror requirements we impose on human agents. When we hire a paralegal or junior analyst, we specify their objectives, verify the quality of their information sources, and limit their authority to take binding actions. The GPA framework makes explicit what has long been implicit in human delegation: \emph{agency requires accountability structures}. What differs for AI agents is the need to encode these controls in technical systems rather than organizational policies alone.

If GPA creates accountability challenges for basic agents, the IAT properties that define full \keyterm{agentic systems} (see Chapter~1) amplify them. Iteration means the system operates across multiple perceive-act cycles, each depending on prior state and environmental feedback. Governance must maintain \emph{audit trails} that reconstruct decision sequences and enable reproducibility. Adaptation means the system changes its strategy based on experience; governance must implement \emph{change control} and continuous revalidation. Termination means the system must know when to stop, hand off to a human, or escalate; governance must define \emph{exit protocols} and \emph{emergency stop mechanisms}.

\glsadd{governance-surface}
These six properties combine multiplicatively. An agentic system that adapts its perception across iterated interactions while pursuing evolving goals creates a governance surface far larger than a deterministic, single-invocation tool.

\subsection{Professional Stakes}
\label{sec:agents3-stakes}

The governance imperative becomes urgent when we recognize a foundational legal and professional principle: \textbf{professional duties cannot be delegated to AI}. Attorneys, investment advisers, auditors, and other licensed professionals remain fully liable for the quality, accuracy, and ethical propriety of their work product, regardless of whether they used AI assistance.
\glsadd{delegation}

\paragraph{Legal Practice}
The American Bar Association's Model Rules of Professional Conduct impose duties of \emph{competence} (Rule 1.1), \emph{confidentiality} (Rule 1.6), and \emph{candor to the tribunal} (Rule 3.3) on attorneys personally. ABA Formal Opinion 512 (July 2024) represents the ABA's first comprehensive ethics guidance on generative AI. The opinion makes clear that attorneys must independently verify AI-generated content and cannot delegate professional judgment to AI tools \parencite{aba-formal-opinion-512}. Courts have sanctioned attorneys for AI hallucinations in cases ranging from \$1,000 to \$15,000, with some attorneys facing suspension or revocation of practice privileges. The professional duty to verify legal research is non-delegable. Courts sanction the attorney, not the AI vendor. Even non-agentic AI tools create professional responsibility obligations. Agentic systems with autonomous iteration and actuation capabilities demand even greater governance.

\paragraph{Financial Services}
Investment advisers owe fiduciary duties to clients under the Investment Advisers Act of 1940, including duties of care and loyalty. FINRA's Regulatory Notice 24-09 (June 2024) clarified that existing securities laws and FINRA rules apply fully to AI and generative AI tools. There is no ``AI exception'' to supervision, suitability, or recordkeeping requirements \parencite{finra-notice-24-09}. The SEC's 2025 examination priorities explicitly target firms' AI usage, including whether representations about AI capabilities are accurate and whether adequate supervisory policies exist. ``The AI recommended it'' is not a defense to breach of fiduciary duty. Firms remain responsible for AI-generated content just as they would be for any employee communication.

\paragraph{Audit and Accounting}
The Public Company Accounting Oversight Board (PCAOB) requires auditors to exercise \emph{professional skepticism} and maintain \emph{independence} when auditing financial statements. If an auditor uses AI to select samples for testing or analyze accounting estimates, the auditor must understand the tool's methodology, validate its outputs, and document the rationale in workpapers. The auditor cannot delegate professional judgment to the AI and remain compliant with PCAOB standards \parencite{pcaob-as1015,pcaob-as1105}.

\begin{keybox}[title={``The AI Did It'' Is Not a Defense}]
Across legal, financial, and audit domains, professional responsibility rules establish that using AI tools does not diminish the professional's accountability. Governance is not optional; it is the operational mechanism for maintaining professional competence and fulfilling non-delegable duties.
\end{keybox}

\subsection{Forces Driving Adoption}
\label{sec:agents3-forces}

Beyond professional obligations, three converging forces make governance essential for any organization deploying agentic systems:

\paragraph{Regulatory Momentum}
AI-specific regulation is no longer hypothetical. The European Union's AI Act entered into force in August 2024, establishing risk-based requirements for high-risk AI systems in credit decisioning, employment, law enforcement, and critical infrastructure \parencite{eu-ai-act-2024}. Systems classified as high-risk must undergo conformity assessments, maintain documentation, implement human oversight, and enable auditability, or face penalties up to â‚¬35 million or 7\% of global annual turnover, whichever is greater.

In the United States, sector-specific regulators are issuing guidance at an accelerating pace. The Federal Reserve's SR 11-7 guidance on model risk management applies to AI/ML systems used by banking institutions \parencite{fed-sr11-7}. The Equal Credit Opportunity Act requires lenders to provide ``principal reasons'' for adverse credit decisions, a requirement that extends to AI-driven underwriting \parencite{ecoa-reg-b}. States are enacting their own requirements. Colorado's AI Act (effective January 2026) prohibits algorithmic discrimination and requires impact assessments for high-risk systems \parencite{colorado-ai-act}.

This regulatory patchwork means organizations cannot rely on a single compliance framework. Governance must layer multiple obligations.

\paragraph{Liability Exposure}
Litigation is establishing clear precedents: governance gaps create liability, and ``the AI made the mistake'' is not a defense. Courts have sanctioned attorneys for AI-generated hallucinations across hundreds of cases, with penalties including monetary sanctions, practice suspensions, and mandatory disclosures on future filings. The SEC has pursued ``AI washing'' enforcement actions against firms that misrepresented their AI capabilities. Fair lending enforcement under the Equal Credit Opportunity Act applies disparate impact theory to AI systems. Facially neutral algorithms can create liability if they disproportionately harm protected classes without adequate business justification. If an AI credit scoring model produces discriminatory outcomes, the lender faces regulatory and litigation exposure regardless of whether the model was ``neutral'' or purchased from a reputable vendor.

Vendor contracts typically shift risk to deployers through liability caps, warranty disclaimers, and indemnification clauses. A foundation model vendor may cap damages at the subscription fee, often insufficient to cover regulatory penalties, reputational harm, or class action settlements. Governance (demonstrating reasonable care through risk assessment, validation, monitoring, and incident response) becomes the primary defense.

\paragraph{Trust and Reputation}
\glsadd{fiduciary-duty}Legal, financial, and audit services are \emph{trust-intensive} domains. Clients hire attorneys because they trust professional judgment. Investors entrust assets to advisers based on fiduciary obligations. Public companies rely on auditors to provide independent assurance. AI failures that compromise accuracy, confidentiality, or impartiality erode this trust irreparably.

A law firm that discloses client confidential information through an AI tool's training data breach faces not only regulatory sanctions but client defection. An investment adviser whose AI chatbot provides unsuitable recommendations faces not only fiduciary duty claims but loss of clients. An audit firm whose AI sampling tool produces biased or incomplete samples faces not only PCAOB sanctions but damage to its reputation for independence.

In trust-intensive domains, governance is not merely a compliance obligation; it is a competitive necessity.

\subsection{Mapping Properties to Requirements}
\label{sec:agents3-property-mapping}

Effective governance begins with a systematic mapping from the six GPA+IAT properties (Chapter~1) to the specific controls required to manage risk, ensure compliance, and maintain accountability. This section provides that mapping, organized by property.

\textbf{Note on System Architecture:} This chapter assumes familiarity with the GPA+IAT framework from Chapter~1. Organizations evaluating whether a specific system qualifies as an ``agentic system'' should apply Chapter~1's six-question rubric and falsification tests. Chapter~2 covers specific architectures (ReAct, Reflexion, tool-calling frameworks) and helps teams distinguish agentic systems from sophisticated chatbots or single-shot inference systems.

\paragraph{Goal: Purpose Limitation and Alignment}
An agent's goal determines what it optimizes for. Governance must ensure goals are \emph{authorized}, specifying who may set goals and under what authority, since regulated domains may require approval from compliance officers, general counsel, or clients. Goals must also be \emph{aligned} with actual organizational or client objectives. Misaligned goals that optimize for throughput at the expense of quality or minimize cost without considering risk create liability. Furthermore, goals must be \emph{bounded} by constraints that limit aggressive pursuit, preventing agents from ignoring side effects, ethical boundaries, or resource limits. Finally, goals must be \emph{monitorable} so that governance can detect when the agent fails to achieve its objective or when goal pursuit causes unintended harms. This requires the establishment of Key Performance Indicators (KPIs) and Service Level Agreements (SLAs) that track both goal satisfaction and side-effect metrics.

\paragraph{Perception: Data Governance and Input Validation}
An agent's perception defines what information it uses to make decisions. Organizations must address \emph{provenance}: establishing where data comes from, whether it is authoritative, current, and trustworthy, since agents that perceive stale, fabricated, or biased data will make flawed decisions. For third-party systems, establishing provenance can be exceedingly difficult, requiring vendor assessment protocols and documentation of provenance gaps as residual risk. Controls must also address \emph{bias and representation}, determining whether the agent's perceptual model reflects population diversity or encodes historical biases, and implementing bias detection and fairness audits accordingly. \emph{Input validation} is equally critical. Adversaries may manipulate what the agent perceives through prompt injection, data poisoning, or adversarial examples, necessitating input validation, sanitization, and anomaly detection. Finally, governance must address \emph{privacy and confidentiality} when perception requires access to sensitive data, ensuring data minimization, encryption, and access controls that preserve confidentiality and comply with privacy regulations.

\paragraph{Action: Actuation Controls and Approval Gates}
An agent's action set determines what it can \emph{do}. Organizations must manage actuation risk through \emph{action authorization}: defining what actions the agent is permitted to take and requiring explicit authorization for high-consequence actions such as signing contracts or disbursing funds. \emph{Pre-action approval} determines whether certain actions require human approval before execution. Human-in-the-loop oversight is appropriate for irreversible or high-stakes actions. Systems must also ensure \emph{rollback and remediation} capabilities. If an action causes harm, can it be undone? Systems must be designed with rollback capabilities and remediation protocols. Finally, \emph{rate limiting} addresses whether the agent can take actions too quickly or too frequently, requiring governance to enforce rate limits and circuit breakers that prevent runaway execution.

\paragraph{Iteration: State Management and Audit Trails}
\glsadd{iteration}Iteration means the system operates across multiple cycles, each building on prior state. Organizations must ensure \emph{reproducibility}: the ability to replay the system's decision sequence, since debugging, auditing, and compliance reviews require reconstructing what the system perceived and why it acted as it did. \emph{State integrity} requires that the system's internal state be protected from tampering or corruption through tamper-evident logging and state validation. Systems must also define \emph{termination conditions} that specify when the system should stop iterating, whether because the goal has been achieved, a resource limit has been reached, or a safety violation has been detected.

\paragraph{Adaptation: Change Control and Revalidation}
\glsadd{adaptation}Adaptation means the system's behavior changes over time. Organizations must manage behavioral drift through \emph{change detection}: tracking when the system's behavior changed and what triggered the adaptation, which requires model versioning and change logs. \emph{Revalidation triggers} determine whether adapted behavior still satisfies safety, fairness, and compliance constraints. Systems must define triggers such as performance degradation, distribution shift, or policy updates that initiate revalidation. Controls must also enable \emph{rollback to known-good states} so that if adaptation introduces failures, the system can revert to a prior validated version. Finally, \emph{human oversight of learning} addresses whether adaptation should require human approval. In high-stakes domains, unsupervised learning may be inappropriate.

\paragraph{Termination: Exit Protocols and Escalation}
\glsadd{termination}Termination governs when and how the system stops operating. Organizations must define \emph{escalation triggers}: the conditions under which the system hands off to a human, such as ambiguous inputs, conflicting objectives, safety violations, or low-confidence decisions. \emph{Graceful shutdown} procedures specify how the system cleanly exits, since abrupt termination may leave systems in inconsistent states. \emph{Handoff procedures} determine what information the system must provide when it escalates to a human, since effective handoff requires context. Finally, \emph{override and emergency stop} mechanisms must allow humans to immediately halt the system. Controls must provide emergency stop mechanisms (the ``red button'') accessible to authorized personnel.

\begin{keybox}[title={From Properties to Controls}]
The six properties from Chapter~1 are not merely a taxonomy for understanding agents; they form a \emph{requirements map} for governance. Each property creates specific risks. Each risk demands specific controls. Organizations that deploy agentic systems without systematically addressing all six properties face gaps in accountability, compliance, and safety.
\end{keybox}

The remainder of this chapter builds on this foundation. Section~\ref{sec:agents3-dimensional} shows how to calibrate control intensity based on system autonomy, entity frame, goal dynamics, and persistence, establishing the control logic that governs agentic systems. Section~\ref{sec:agents3-governance-stack} then maps regulatory obligations into a five-layer framework, demonstrating how to apply these calibrated controls across regulatory layers. Sections~\ref{sec:agents3-implementation} and \ref{sec:agents3-accountability} translate principles into operational practices and organizational structures. Section~\ref{sec:agents3-examples} demonstrates governance through worked examples in legal, financial, and audit contexts. Section~\ref{sec:agents3-conclusion} synthesizes the governance imperative and provides a maturity-based path forward.

