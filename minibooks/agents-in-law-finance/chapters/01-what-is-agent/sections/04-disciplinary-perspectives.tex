\section{Disciplinary Perspectives}
\label{sec:disciplines}

While Section~\ref{sec:history} traced chronological evolution, this section examines how different disciplines approach agency from distinct theoretical foundations. Each field emphasizes different facets of agency, revealing complementary insights and persistent tensions.

\subsection{Philosophy: Intentionality and Reasons}

Philosophy's distinctive contribution centers on \keyterm{intentional descriptions}, responsibility for actions, and predictive stances. \textcite{anscombe1957intention} established that actions are intentional ``under a description.'' The same physical movement can be intentional under one description (``signing a contract'') but not another (``smudging ink''). This insight matters for AI agents: when we attribute intentions to entities, we are choosing explanatory frameworks, not discovering intrinsic properties.

\textcite{bratman1987intention}'s planning theory and \textcite{dennett1987intentional}'s \keyterm{intentional stance}\glsadd{intentional-stance} provide complementary perspectives: agents act according to their plans and commitments (Bratman) or can be treated as rational goal-pursuers regardless of internal mechanism (Dennett). Philosophy emphasizes \textit{reasons} for action over mechanical causation, focusing on responsibility for actions and conceptual foundations rather than implementation.

\subsection{Psychology: Self-Regulation and Control}

Psychology approaches agency through \keyterm{self-regulation}, \keyterm{perceived control}, and contextual variability. Bakan's agency-communion spectrum characterizes fundamental modes of human existence, while \textcite{milgram1974obedience}'s agentic state describes conditions under which people suppress personal judgment and defer to authority. \textcite{bandura1989human}'s social cognitive theory emphasizes human agency as exercised through intentionality, forethought, self-reactiveness, and self-reflectiveness.

Psychology's distinctive insight: agency is not merely a property of isolated actions but a stable characteristic of cognitive entities capable of self-regulation. Crucially, agency exists on a spectrum and can be diminished by context \parencite{milgram1974obedience} or enhanced through \keyterm{self-efficacy} beliefs. For AI systems, this highlights how humans \textit{perceive} AI agency, affecting trust and delegation decisions.

\subsection{Law: Delegation and Fiduciary Duty}

\glsadd{agency-relationship}Legal doctrine understands agency through \keyterm{delegated authority}, \keyterm{fiduciary duties}\glsadd{fiduciary-duty}, and \keyterm{institutional contexts}. The Restatement of Agency defines the relationship as arising when a principal manifests assent that an agent shall act on the principal's behalf and subject to the principal's control. Key elements include: (1) relational structure requiring a principal-agent pair, (2) delegated authority derived from the principal, (3) fiduciary obligations of loyalty and care, and (4) liability attribution within scope of authority.

Law's distinctive approach focuses on external authority structures and legally enforceable duties rather than internal intentions or motivations. Legal frameworks embed agency in formal mechanisms such as contracts, corporate charters, and powers of attorney that create, bound, and terminate authority relationships. For AI systems, this raises immediate questions: Can an AI be an agent in the legal sense? Who serves as principal? As illustrated in Section~\ref{sec:intro}, misunderstanding these relationships can have serious consequences when entities lack mechanisms for fulfilling fiduciary duties.

\subsection{Economics: Incentives and Information Asymmetry}

\glsadd{principal-agent-relationship}Economic analysis frames agency through \keyterm{utility alignment}, \keyterm{information asymmetry}\glsadd{information-asymmetry}, and \keyterm{organizational design}. \textcite{jensen1976theory}'s principal-agent model identifies the core problem: principals and agents have divergent preferences and asymmetric information. Agents may pursue self-interest at principals' expense (\keyterm{moral hazard}\glsadd{moral-hazard}), or principals may struggle to assess agent quality ex ante (\keyterm{adverse selection}\glsadd{adverse-selection}).

Economics' distinctive approach emphasizes cost-benefit analysis and divergence of preferences rather than rights or duties. The focus is pragmatic: given that perfect alignment is impossible, how do we minimize \keyterm{agency costs}\glsadd{agency-costs} through compensation structures, performance metrics, and oversight mechanisms? For AI systems, economic frameworks highlight alignment challenges: How do we design ``reward functions'' that elicit desired behavior? What monitoring mechanisms detect when AI agents pursue proxy metrics rather than true objectives?

\subsection{Cognitive Science: Mental Architecture and Plans}

Cognitive science approaches agency through \keyterm{modular mental societies}, \keyterm{plan-based control}, and \keyterm{representational structures}. \textcite{minsky1986society}'s Society of Mind framed intelligence as emerging from interactions among simple processes without central control. \textcite{bratman1987intention}'s planning theory emphasized that intentions are elements of partial plans structuring practical reasoning over time. The \keyterm{BDI (Belief-Desire-Intention)} architecture implements this: agents maintain beliefs about the world, desires representing goals, and intentions as committed plans.

Cognitive science's distinctive insight treats agency as a lens on \textit{mental architecture}, not just overt action, focusing on information processing, symbolic manipulation, and hierarchical plan structures. The framework readily generalizes from human to machine minds via abstraction. For AI systems, cognitive science provides architectural blueprints. The BDI framework has directly influenced agent programming languages, and concepts like partial plans and means-end reasoning transfer naturally to computational implementations.

\subsection{Complex Systems: Emergence and Local Rules}

Complex systems science characterizes agency through \keyterm{local rules generating global effects}, \keyterm{adaptation}, and \keyterm{environmental coupling}. \textcite{holland1995cas}'s complex adaptive systems, \textcite{epstein1996growing}'s agent-based models, and \textcite{kauffman2000investigations}'s autonomous agents emphasize that interesting behavior emerges from interactions among rule-following entities. Individual agents need not be sophisticated; simple local rules, diversity, and environmental feedback suffice to produce \keyterm{emergent macro patterns}\glsadd{emergent-behavior}.

Complex systems' distinctive insight treats agents as embedded in adaptive networks rather than as isolated actors. Behaviors co-evolve through selection pressures, learning, and resource flows. For AI systems, this perspective highlights how collections of simple agents exhibit sophisticated collective behavior (multi-agent systems, swarm robotics), but also warns against over-attributing agency to individual components when behavior primarily reflects system-level dynamics.

\subsection{Computer Science: Protocols and Mentalistic Abstractions}

Computer science, particularly agent-oriented software engineering, frames agency through \keyterm{mentalistic programming abstractions}, \keyterm{interaction protocols}, and \keyterm{organizational modeling}. \textcite{shoham1993aop}'s agent-oriented programming introduced beliefs, commitments, and choices as first-class programming constructs. Unlike object-oriented programming (encapsulation and message-passing), agent-oriented programming treats components as having mental states and engaging in speech acts. FIPA agent communication languages formalized performatives (inform, request, propose) modeled on human \keyterm{speech acts}.

Computer science's distinctive approach uses mentalistic vocabulary as an \textit{engineering tool} rather than philosophical claim. Attributing beliefs to software entities improves code clarity and reasoning about distributed systems, whether or not agents ``really'' have beliefs. For AI systems, computer science provides the middleware such as communication standards, coordination mechanisms, and organizational structures that enable multi-agent systems to function. Agents negotiate rather than merely receiving method calls.

\subsection{Cross-Disciplinary Synthesis}
\glsadd{autonomy-spectrum}
\glsadd{entity-frame}

Table~\ref{tab:disciplines} summarizes key differences across disciplines.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabular}{@{}l ccc ccc@{}}
\toprule
& \multicolumn{3}{c}{\textbf{Autonomy}} & \multicolumn{3}{c}{\textbf{Entity Focus}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
\textbf{Discipline} & Low & Med & High & Human & Hybrid & Machine \\
\midrule
\rowcolor{bg-neutral}
\multicolumn{7}{@{}l}{\textit{Responsibility-oriented (normative)}} \\
Philosophy     & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); \\
Psychology     & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); \\
Law            & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); \\
Economics      & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); \\
\midrule
\rowcolor{bg-neutral}
\multicolumn{7}{@{}l}{\textit{Capability-oriented (descriptive)}} \\
Cognitive Science  & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); \\
Complex Systems    & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); \\
Computer Science   & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); \\
AI             & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\draw[primary] (0,0) circle (0.08cm); & \tikz\fill[primary] (0,0) circle (0.08cm); \\
\bottomrule
\end{tabular}
\caption{Cross-disciplinary agency perspectives. Filled = primary focus; empty = not emphasized.}
\label{tab:disciplines}
\end{table}

Several patterns emerge:
\begin{itemize}
\item \textbf{Autonomy gradient}: Disciplines focusing on human-AI relationships (law, economics) assume lower autonomy, while those focused on machine capabilities (AI, complex systems) assume higher autonomy
\item \textbf{Entity frames}: Philosophy, psychology, and cognitive science start with humans; law and economics examine hybrid relationships; complex systems, computer science, and AI embrace machine entities
\item \textbf{Mechanism diversity}: Each discipline emphasizes different causal mechanisms (intentions, self-regulation, incentives, plans, rules, protocols, actions)
\item \textbf{Normative vs descriptive}: Philosophy and law are more normative (concerned with responsibility and duty), while AI and complex systems are more descriptive (concerned with observable behavior)
\end{itemize}

Understanding agentic AI in professional contexts requires synthesis across these disciplines. Philosophy provides conceptual foundations for attributing agency; psychology illuminates how humans perceive and interact with AI agents. Law establishes liability frameworks and professional responsibilities; economics offers tools for analyzing alignment problems. Cognitive science suggests architectural patterns; complex systems warns about emergent behavior in multi-agent contexts. Computer science provides implementation protocols; AI defines practical capabilities and evaluation criteria.

The next section extracts analytical dimensions that cut across these disciplinary perspectives.
