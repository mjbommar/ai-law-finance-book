\section{Analytical Framework}
\label{sec:synthesis}

Having traced the historical evolution (\Cref{sec:history}), examined disciplinary perspectives (\Cref{sec:disciplines}), and identified analytical dimensions (\Cref{sec:dimensions}), we now synthesize these insights into a coherent analytical framework. This framework provides both the theoretical foundation for understanding agency and practical tools for evaluating real-world systems.



\subsection{Cross-Cutting Patterns}

Our analysis across disciplines reveals four fundamental patterns that consistently appear when scholars and practitioners identify agency. These patterns transcend individual fields, providing a shared foundation for recognizing agentic behavior regardless of whether we are examining human decision-makers, organizational structures, or computational entities.

\begin{keybox}[title={\textbf{Pattern 1: Goal-Directedness}}]
Agents pursue objectives or performance criteria. Whether we call them intentions in philosophy, rewards in reinforcement learning, mandates in law, or incentives in economics, the common thread remains: agents exhibit behavior oriented toward achieving outcomes rather than merely reacting to stimuli. This distinguishes purposeful action from reflexive response.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 2: Perception–Action Coupling}}]
Agents operate through iterative cycles of sensing and acting. This pattern appears across theoretical frameworks---from \textcite{russellnorvig2020aima}'s sensor-actuator loops to \textcite{bratman1987intention}'s practical reasoning and \textcite{bandura1989human}'s reciprocal determinism. Entities that transform inputs once without iteration, such as compilers or single-shot classifiers, lack this essential coupling that characterizes agentic systems.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 3: Selective Autonomy}}]
Agents require some degree of discretion in their choices, as we explored in \Cref{sec:dimensions}. Entities that execute fixed scripts without any decision-making capacity do not qualify as agents. The degree of autonomy varies significantly across domains---from minimal discretion in rule-based entities to substantial independence in modern AI agents---with higher levels of discretion typically raising more complex governance and accountability questions.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 4: Termination}},breakable=false]
Purposeful behavior requires boundaries. Agents must have mechanisms for recognizing when to stop, whether through explicit controls (resource budgets, time limits, escalation triggers) or implicit goal satisfaction (achieving target state, exhausting search space). Entities that run indefinitely without clear stopping conditions or decision points fall outside our definition of agentic systems, as they lack the goal-oriented closure that characterizes purposeful action.
\end{keybox}

\subsection{Formal Core}

While the patterns above provide intuitive understanding, precision demands formal specification. We now translate our insights into a mathematical framework that enables rigorous analysis and falsifiable claims about agency.

To formalize our framework, we denote the six essential properties as follows:
\begin{itemize}[leftmargin=1.5em]
\item $G$ = goal or utility function that directs behavior
\item $P$ = perception capability for sensing environment
\item $A$ = action capability for affecting environment
\item $I$ = iteration across multiple steps with state preservation
\item $A_{\text{adapt}}$ = adaptation through modifying strategy based on accumulated observations and feedback
\item $T$ = termination through explicit or implicit conditions
\end{itemize}

Consistent with the operational definition in Section~\ref{sec:intro}, the six formal properties are Goal, Perception, Action, Iteration, Adaptation, and Termination (GPA + IAT). Autonomy or discretion remains an analytical dimension from Section~\ref{sec:dimensions} that describes \emph{how} these properties are exercised rather than a separate core property.

We use the notation $\mathit{has}(x, \cdot)$ to indicate that entity $x$ possesses a given property.

\begin{definitionbox}[title={\textbf{Formal Definitions}}]
We define three nested categories of entities based on their properties:
\begin{align*}
\mathit{Agent}(x) &\iff \mathit{has}(x,G) \land \mathit{has}(x,P) \land \mathit{has}(x,A) \\
&\quad\text{(Minimal agency: goals, perception, action)}\\[0.5em]
\mathit{AgenticSystem}(x) &\iff \mathit{Agent}(x) \land \mathit{has}(x,I) \land \mathit{has}(x,A_{\text{adapt}}) \land \mathit{has}(x,T) \\
&\quad\text{(Full operational system: all six properties)}\\[0.5em]
\mathit{AIAgent}(x) &\iff \mathit{AgenticSystem}(x) \land \mathit{implementedWithAI}(x)\\
&\quad\text{(AI-powered implementation of agentic system)}
\end{align*}
\end{definitionbox}

These formal definitions yield several important theoretical results that clarify the relationships between different types of entities:

\begin{theorembox}[unbreakable,title={\textbf{Key Theoretical Results}}]
\textbf{Hierarchical Subsumption.} The categories form a nested hierarchy:
$$\mathit{AIAgent}(x) \Rightarrow \mathit{AgenticSystem}(x) \Rightarrow \mathit{Agent}(x)$$
Every AI agent is necessarily an agentic system, and every agentic system is necessarily an agent, but the converses do not hold.

\textbf{Non-Equivalence.} The categories are strictly distinct:
$$\exists x:\ \mathit{Agent}(x) \land \neg\mathit{AgenticSystem}(x)$$
For example, a single-shot classifier might have goals, perception, and action capabilities, but lacks the iteration, discretion, and termination properties required for an agentic system.

\textbf{Property Attribution.} Individual features can be described as ``agentic'' (such as ``agentic perception'' or ``agentic planning'') without implying the system as a whole qualifies as an $\mathit{AgenticSystem}$. Full agentic system status requires all six properties.
\end{theorembox}

The termination property deserves special attention, as it can manifest in two distinct forms:

\begin{definitionbox}[unbreakable,title={\textbf{Termination Modes}}]
An entity satisfies the termination requirement through either explicit or implicit mechanisms:
$$\mathit{has}(x,T) \iff \text{ExplicitTerm}(x)\ \lor\ \text{ImplicitTerm}(x)$$

\textbf{Explicit termination} involves programmed monitoring of conditions such as:
\begin{itemize}[leftmargin=1.5em,nosep]
\item Resource budgets (computation time, API calls, token limits)
\item Timeout constraints (wall clock time, iteration counts)
\item Escalation triggers (confidence thresholds, error conditions)
\end{itemize}

\textbf{Implicit termination} occurs naturally through:
\begin{itemize}[leftmargin=1.5em,nosep]
\item Goal satisfaction (target state achieved)
\item Quiescent states (no further actions available)
\item Bounded episodes (fixed-duration tasks)
\end{itemize}

Professional deployments typically combine both modes. A legal research agent might terminate implicitly when a satisfactory precedent is found while respecting explicit resource limits and escalating to humans when confidence falls below thresholds.
\end{definitionbox}

\begin{highlightbox}[title={\textbf{Termination Audit Checklist for Deployments}}]
Before deploying an agentic system in professional contexts, verify it implements at least one explicit termination mechanism:

\begin{itemize}[leftmargin=1.5em,nosep]
\item[$\square$] \textbf{Time budget:} Maximum wall-clock duration (seconds/minutes)
\item[$\square$] \textbf{Iteration limit:} Maximum perception-action cycles
\item[$\square$] \textbf{Token/request budget:} Maximum API calls, LLM tokens, or database queries
\item[$\square$] \textbf{Cost ceiling:} Maximum monetary expenditure per task
\item[$\square$] \textbf{Escalation trigger:} Automatic handoff when confidence drops below threshold
\item[$\square$] \textbf{Error accumulation:} Stop after N consecutive failures or exceptions
\item[$\square$] \textbf{Human intervention point:} Required approval before high-stakes actions
\end{itemize}

\textit{For high-stakes deployments (legal, medical, financial), implement multiple explicit termination mechanisms to prevent runaway processes and ensure predictable resource consumption.}
\end{highlightbox}

\subsection{Foundations in Context}

Our formal framework draws from multiple theoretical traditions, each contributing essential insights about how agents operate. Understanding these foundations helps practitioners from different backgrounds connect our framework to established concepts in their fields.

\paragraph{Plan-Based Control}
The tradition of \keyterm{plan-based control} emphasizes how agents decompose high-level goals into actionable steps. This approach, central to cognitive science and AI planning systems, treats agency as means-end reasoning---connecting what we want to achieve with how to achieve it. In legal practice, we see this pattern in structured workflows like due diligence, where complex objectives decompose into systematic review tasks. Modern AI agents employ similar hierarchical planning when breaking down user requests into sequences of tool calls and sub-tasks.

\paragraph{Reactive Control}
Not all intelligent behavior requires elaborate planning. The \keyterm{reactive control} paradigm demonstrates that simple perception-action rules can produce sophisticated behavior when properly designed. This insight, pioneered in robotics by researchers like Rodney Brooks, shows that agents can be effective without maintaining complex world models. In fast-changing environments or when sensing is reliable, responsive behavior often outperforms deliberative planning. Contemporary LLM agents balance both approaches---maintaining high-level plans while reacting dynamically to unexpected outputs or errors.

\paragraph{BDI Models}
The \keyterm{Belief-Desire-Intention} (BDI) framework provides a structured account of agent mental states. Agents maintain beliefs about their environment, desires about preferred outcomes, and intentions representing committed plans. This model, influential in agent-oriented programming, explains how agents form and revise commitments as new information arrives. Legal agents exemplify this pattern: maintaining beliefs about applicable law, desires to serve client interests, and intentions manifested as filed documents or negotiation positions.

\paragraph{Reinforcement Learning}
The \keyterm{reinforcement learning} paradigm formalizes how agents learn from experience. Through cycles of action, observation, and reward, agents discover which behaviors achieve their goals. This framework clarifies why iterative interaction outperforms single-shot processing---each cycle provides information that improves future decisions. Modern AI agents increasingly incorporate reinforcement learning, whether through fine-tuning on human feedback or real-time adaptation during task execution.

\paragraph{Intentional Stance}
Following philosopher Daniel Dennett's influential work \parencite{dennett1987intentional}, the \keyterm{intentional stance} treats agency as a pragmatic attribution rather than metaphysical fact. We describe systems as having beliefs, desires, and intentions when such descriptions improve our ability to predict and control their behavior. This perspective liberates us from endless debates about whether AI systems ``really'' have goals or merely simulate goal-directed behavior. What matters is whether treating them as agents yields practical benefits.

\begin{highlightbox}[title={\textbf{Why These Foundations Matter}},breakable=false]
These foundations directly inform how we build and evaluate agentic systems:

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Planning} supports goal decomposition and task orchestration
\item \textbf{Reactive control} validates simple, robust responses to changing environments
\item \textbf{BDI models} supply architectures for managing commitments and updating beliefs
\item \textbf{Reinforcement learning} explains improvement through experience and feedback
\item \textbf{Intentional stance} guides when to treat systems as agents for prediction and control
\end{itemize}

Together, these traditions ground the six-property framework in established theory while keeping it practically usable across disciplines from law to computer science.
\end{highlightbox}

\subsection{Boundary Cases and Clarifications}

Testing our framework against boundary cases reveals both its strengths and the subtleties of applying formal definitions to real-world systems. These edge cases illuminate practical fault lines—where practitioners disagree about agency and where our framework proves most valuable.

\textbf{Thermostats and the Agency Spectrum.} A basic mechanical thermostat demonstrates the spectrum from minimal agency to full agentic systems. It has five of six properties: goal (maintain temperature), perception (sensor readings), action (heating/cooling), iteration (continuous monitoring), and termination (implicit when target reached). However, it lacks adaptation, applying fixed on/off rules without modifying strategy based on outcomes. This distinguishes reactive control from adaptive learning. Smart thermostats, by contrast, qualify as full agentic systems—they learn occupancy patterns, adjust schedules based on observed behavior, and implement explicit termination logic. This progression illustrates why minimal agency (Level 1) sets a deliberately low bar, while agentic systems (all six properties) require operational sophistication suitable for professional deployment.

\textbf{Multi-Agent Collectives and Emergent Behavior.} When multiple agents interact, the collective can exhibit properties no single agent possesses. A trading floor, legal team, or swarm of autonomous vehicles may demonstrate emergent behaviors that challenge our individual-focused definitions. Legal systems have long grappled with this through doctrines distinguishing corporate from individual liability. Our framework accommodates both perspectives: we can analyze individual agents within the system or treat the collective itself as an agent with emergent goals and capabilities. The choice depends on analytical purpose—attribution questions favor individual analysis, while coordination and emergent behavior favor collective treatment.

\textbf{Virtual versus Embodied Agents.} Software agents need not have physical bodies to qualify as agents. A contract analysis system operating entirely in cloud infrastructure satisfies all six properties without any physical embodiment. While robotics researchers sometimes insist on embodiment as essential to agency, our framework follows the broader AI tradition of recognizing virtual agents as fully legitimate instances. The perception-action loop operates equally well whether the environment is physical (sensor readings, motor commands) or virtual (API queries, database updates). Professional legal and financial applications predominantly involve virtual agents, making this inclusive stance practically necessary.

\textbf{Single-Shot LLM Interactions.} A single ChatGPT response, however sophisticated, typically lacks the iteration property required for agentic systems. It processes input once and generates output without maintaining state across multiple perception-action cycles. The system does not observe results, adjust strategy, and continue pursuing goals—it completes and stops. For the purposes of this framework, we treat such single-shot LLM calls as non-iterative, even if they internally perform multiple reasoning steps, because they do not observe and react to \emph{external} feedback within a perception-action loop.

\textbf{Tool Use versus Tool-Using Agents.} Simply using tools does not make an entity agentic. A script that calls an API once is not an agent—it lacks the iteration and adaptation that characterize agentic systems. What matters is purposeful, iterative tool use directed toward goals. \textbf{Simply calling an API once does \emph{not} make a system an agent; the difference lies in goal-directed, iterative tool use with adaptation.} An LLM that repeatedly queries databases, processes results, and decides which tool to invoke next based on accumulated information exhibits agency. The distinction lies in the goal-directed iteration, not the mere presence of tool use. Advanced agentic systems extend this further by creating new tools—writing Python scripts to expand their capabilities or composing prompts that define specialized sub-agents \parencite{claude-code-subagents}. This recursive capability, where agents manage the design and implementation of their own tools, represents a meta-level of agency beyond mere tool invocation.

\textbf{Continuous Control and Implicit Termination.} Entities engaged in continuous control—like systems maintaining server uptime or monitoring market conditions—might seem to violate our termination requirement. However, these entities typically operate in bounded episodes with implicit termination conditions: achieving steady state, exhausting available actions, or reaching time limits. Even ``always-on'' entities have mechanisms for recognizing when to stop particular behaviors, satisfying our termination criterion. A market monitoring system may run continuously, but individual monitoring tasks terminate when specific conditions trigger alerts or when trading sessions close. The distinction between the persistent entity and its bounded tasks resolves the apparent contradiction.

\subsection{Professional Deployment}

Understanding what makes an entity agentic is only the first step. In regulated domains like law and finance, deploying AI agents requires robust governance frameworks that ensure compliance, maintain professional standards, and protect sensitive information. The stakes are particularly high: errors can trigger regulatory sanctions, breach fiduciary duties, or compromise client confidentiality.

Professional deployment demands that we move beyond theoretical properties to operational safeguards. Each of the six properties we've identified creates both opportunities and risks that must be managed through careful entity design and governance controls. The following controls transform theoretical capabilities into trustworthy tools:

\begin{keybox}[title={\textbf{1. Attribution and Provenance}}]
Every factual claim must be traceable to its source. The system should record not just what it concluded but where that information originated---whether from case law, regulatory filings, market data, or expert analysis. This enables independent verification and helps practitioners assess the reliability of agent-generated work product. Modern AI agents should maintain citation chains that would satisfy the standards of legal briefs or investment memoranda.
\end{keybox}

\begin{keybox}[title={\textbf{2. Auditable Reasoning}}]
Professional decisions require transparent rationales. The system must log major reasoning steps, tool invocations, and decision points in forms that preserve privilege while enabling review. This is not about exposing every neural network activation---it is about capturing the business logic an expert would expect to see. Think litigation hold decisions, trade execution rationales, or compliance determination paths.
\end{keybox}

\begin{keybox}[title={\textbf{3. Bounded Operation}}]
Unbounded iteration violates both practical and regulatory constraints. Systems need explicit limits: computational budgets, time constraints, maximum iteration counts. These bounds prevent runaway processes while ensuring predictable resource consumption. In financial contexts, this might mean limiting the number of market data queries; in legal contexts, capping document review cycles.
\end{keybox}

\begin{keybox}[title={\textbf{4. Escalation Pathways}}]
Agents must know their limits and when to seek help. Clear escalation triggers---low confidence scores, high-stakes decisions, detected conflicts---should route matters to human review. The escalation logic should be transparent and adjustable based on risk tolerance and regulatory requirements. A contract review agent, for instance, might escalate unusual liability provisions or non-standard jurisdiction clauses.
\end{keybox}

\begin{keybox}[title={\textbf{5. Confidentiality Controls}}]
Information barriers are non-negotiable in professional services. The system must enforce ethical walls between matters, implement role-based access controls, and apply appropriate redaction rules. This goes beyond basic security to encompass the nuanced confidentiality obligations of legal privilege, insider trading restrictions, and client confidentiality duties.
\end{keybox}

\begin{keybox}[title={\textbf{6. Accountability Mapping}}]
When things go wrong---and they will---clear lines of responsibility enable rapid response and remediation. Who configured the agent? Who reviewed its outputs? Who authorized its deployment? These questions must have clear answers before the system processes its first client matter. Accountability is not about blame; it is about maintaining professional standards when humans and AI systems collaborate.
\end{keybox}

\subsection{Common Questions About Agency}

Our framework inevitably raises questions about edge cases and apparent contradictions. Here we address the most common concerns that arise when applying these definitions to real-world systems.

\textbf{Is iteration just repeated action?} Iteration means more than simply repeating the same action multiple times. True iteration involves maintaining state across perception-action cycles, enabling the entity to learn from previous attempts and adjust its approach. An entity that simply retries the same failing action indefinitely is not iterating in our sense—it is stuck in a loop. Iteration requires the ability to incorporate feedback, correct errors, and make progress toward goals through successive refinements. A system that queries the same API endpoint repeatedly with identical parameters is not iterating; one that refines its query based on previous results is. This is why single-shot entities, no matter how sophisticated, do not qualify as agentic systems under our framework.

\textbf{Do continuous tasks violate the termination requirement?} Entities performing continuous tasks like monitoring or maintenance might seem to run forever, apparently violating our termination requirement. However, these entities still satisfy the termination property through implicit or explicit mechanisms. Implicit termination occurs when the entity reaches a steady state or exhausts available actions. Explicit termination happens through resource budgets, time limits, or escalation triggers. Even an entity that monitors markets continuously operates in bounded episodes—trading sessions, reporting periods, or maintenance windows. The key is that the entity has clear conditions under which it will cease its current behavior pattern, even if it later resumes. The termination requirement ensures predictable resource consumption and prevents runaway processes, not that entities can never restart.

\textbf{If thermostats count as agents, have we trivialized the concept?} Recognizing basic thermostats as minimal agents does not trivialize agency—it establishes a meaningful floor. Many entities fail even this basic test: databases lack goals, compilers lack perception-action loops, and lookup tables lack any form of action. More importantly, while basic thermostats have five operational properties (lacking adaptation), they demonstrate precisely why we distinguish minimal agency (Level 1: three properties) from agentic systems (six properties). The distinction between a basic thermostat and an AI legal research assistant is not whether they are agents, but the degree of autonomy, adaptation capability, sophistication, and responsibility they possess. Our framework acknowledges the thermostat's basic agency while reserving "agentic system" status for entities with all six properties (like smart thermostats that learn patterns), and "AI agent" designation for those using AI/ML for implementation. The gradient from simple to sophisticated agents reflects operational reality.

\textbf{Is "AI agent" just marketing hype?} In our framework, "AI agent" has a precise, falsifiable meaning: an agentic system that uses AI or machine learning (particularly large language models) for planning and orchestration. This excludes single-shot chat completions, no matter how impressive, because they lack iteration. It excludes traditional rule-based systems, even if they meet all six properties, because they do not use AI for implementation. It also excludes simple ML classifiers that lack goal-directedness and autonomous action. When vendors claim their product is an "AI agent," we can test this claim against our six properties and the AI implementation requirement. This transforms marketing language into testable assertions about system architecture and capabilities. The framework provides accountability—vendors must demonstrate iteration, adaptation, and termination mechanisms, not just AI-powered text generation.
