\section{Analytical Framework}
\label{sec:synthesis}

Having traced the historical evolution (\Cref{sec:history}), examined disciplinary perspectives (\Cref{sec:disciplines}), and identified analytical dimensions (\Cref{sec:dimensions}), we now synthesize these insights into a coherent analytical framework. This framework provides both the theoretical foundation for understanding agency and practical tools for evaluating real-world systems.



\subsection{Cross-Cutting Patterns}

Our analysis across disciplines reveals four fundamental patterns that consistently appear when scholars and practitioners identify agency. These patterns transcend individual fields, providing a shared foundation for recognizing agentic behavior regardless of whether we are examining human decision-makers, organizational structures, or computational entities.

\begin{keybox}[title={\textbf{Pattern 1: Goal-Directedness}}]
\glsadd{goal}
Agents pursue objectives—whether called intentions, rewards, mandates, or incentives—exhibiting behavior oriented toward outcomes rather than merely reacting to stimuli.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 2: Perception–Action Coupling}}]
\glsadd{perception}
\glsadd{action}
\glsadd{iteration}
Agents operate through iterative cycles of sensing and acting. Entities that transform inputs once without iteration—compilers, single-shot classifiers—lack this essential coupling.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 3: Selective Autonomy}}]
\glsadd{autonomy-spectrum}
Agents exercise discretion in their choices. Entities executing fixed scripts without decision-making capacity do not qualify.
\end{keybox}

\begin{keybox}[title={\textbf{Pattern 4: Termination}},breakable=false]
\glsadd{termination}
\glsadd{escalation}
\glsadd{stopping-conditions}
Agents must recognize when to stop---through explicit controls or implicit goal satisfaction. Entities running indefinitely without stopping conditions lack goal-oriented closure. \Cref{sec:agents2-termination} provides detailed implementation guidance for termination conditions.
\end{keybox}

\subsection{Precise Definitions}

The four patterns above provide intuition. Now we make them precise. A rigorous framework lets us make testable claims: given any system, we can determine whether it qualifies as an agent, an agentic system, or an AI agent---and defend that determination.

\paragraph{The Six Properties}

Our framework rests on six observable properties. The first three establish minimal agency; the additional three distinguish fully operational agentic systems:

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Goal}---a clear objective or performance criterion that directs behavior
\item \textbf{Perception}---the capability to sense and gather information from the environment
\item \textbf{Action}---the capability to affect or change the environment
\item \textbf{Iteration}---operating across multiple steps while preserving state between cycles
\item \textbf{Adaptation}---modifying strategy based on accumulated observations and feedback
\item \textbf{Termination}---recognizing when to stop, through explicit controls or goal satisfaction
\end{itemize}

The mnemonic \textbf{GPA + IAT} captures this structure: Goals, Perception, and Action form the minimal foundation; Iteration, Adaptation, and Termination complete the operational requirements. Autonomy---explored in \Cref{sec:dimensions}---describes \emph{how} these properties are exercised rather than serving as a separate core property.

\begin{definitionbox}[title={\textbf{Three Categories of Entities}}]
\begin{itemize}[leftmargin=1.5em,nosep]
\item \textbf{Agent} (minimal): Has goals, perception, and action. Includes humans, organizations, animals, and computational systems.
\item \textbf{Agentic System} (operational): Has all six properties. Ready for deployment---operates through cycles, learns from feedback, knows when to stop.
\item \textbf{AI Agent}: An agentic system implemented using AI/ML technologies.
\end{itemize}
\end{definitionbox}

These categories form a strict hierarchy: every AI agent is an agentic system, and every agentic system is an agent---but the reverse does not hold. A single-shot image classifier qualifies as an agent (goals, perception, action) but lacks iteration, adaptation, and termination, so it fails as an agentic system. Individual features can be ``agentic'' without the whole system qualifying.

\paragraph{Termination in Detail}

Termination deserves special attention because runaway processes pose real risks in professional contexts. An entity can satisfy the termination requirement through either of two mechanisms---or both:

\glsadd{escalation}
\textbf{Explicit termination} involves programmed hard limits: resource budgets, timeout constraints, and escalation triggers. \textbf{Implicit termination} occurs naturally when goals are satisfied, no further actions are available, or task episodes end. Professional deployments combine both---a legal research agent terminates implicitly upon finding precedent while respecting explicit resource limits. \Cref{sec:agents2-termination} examines five categories of termination conditions (success, resource budgets, confidence thresholds, error conditions, and escalation triggers), while \Cref{sec:agents2-escalation} addresses when agents should pause and request human input rather than terminating outright.

\subsection{Foundations in Context}

Our formal framework draws from multiple theoretical traditions, each contributing essential insights about how agents operate. Understanding these foundations helps practitioners from different backgrounds connect our framework to established concepts in their fields.

\paragraph{Plan-Based Control}
The tradition of \keyterm{plan-based control} emphasizes how agents decompose high-level goals into actionable steps. This approach, central to cognitive science and AI planning systems, treats agency as means-end reasoning---connecting what we want to achieve with how to achieve it. In legal practice, we see this pattern in structured workflows like due diligence, where complex objectives decompose into systematic review tasks. Modern AI agents employ similar hierarchical planning when breaking down user requests into sequences of tool calls and sub-tasks.

\paragraph{Reactive Control}
Not all intelligent behavior requires elaborate planning. The \keyterm{reactive control} paradigm demonstrates that simple perception-action rules can produce sophisticated behavior when properly designed. This insight, pioneered in robotics by researchers like Rodney Brooks, shows that agents can be effective without maintaining complex world models. In fast-changing environments or when sensing is reliable, responsive behavior often outperforms deliberative planning. Contemporary LLM agents balance both approaches---maintaining high-level plans while reacting dynamically to unexpected outputs or errors.

\paragraph{BDI Models}
\glsadd{bdi-architecture}The \keyterm{Belief-Desire-Intention} (BDI) framework provides a structured account of agent mental states. Agents maintain beliefs about their environment, desires about preferred outcomes, and intentions representing committed plans. This model, influential in agent-oriented programming, explains how agents form and revise commitments as new information arrives. Legal agents exemplify this pattern: maintaining beliefs about applicable law, desires to serve client interests, and intentions manifested as filed documents or negotiation positions.

\paragraph{Reinforcement Learning}
\glsadd{reinforcement-learning}The \keyterm{reinforcement learning} paradigm formalizes how agents learn from experience. Through cycles of action, observation, and reward, agents discover which behaviors achieve their goals. This framework clarifies why iterative interaction outperforms single-shot processing---each cycle provides information that improves future decisions. Modern AI agents increasingly incorporate reinforcement learning, whether through fine-tuning on human feedback or real-time adaptation during task execution.

\paragraph{Intentional Stance}
\glsadd{intentional-stance}Following philosopher Daniel Dennett's influential work \parencite{dennett1987intentional}, the \keyterm{intentional stance} treats agency as a pragmatic attribution rather than metaphysical fact. We describe systems as having beliefs, desires, and intentions when such descriptions improve our ability to predict and control their behavior. This perspective liberates us from endless debates about whether AI systems ``really'' have goals or merely simulate goal-directed behavior. What matters is whether treating them as agents yields practical benefits.

\begin{keybox}[title={\textbf{Why These Foundations Matter}}]
These foundations directly inform how we build and evaluate agentic systems:

\begin{itemize}[leftmargin=1.5em]
\item \textbf{Planning} supports goal decomposition and task orchestration
\item \textbf{Reactive control} validates simple, robust responses to changing environments
\item \textbf{BDI models} supply architectures for managing commitments and updating beliefs
\item \textbf{Reinforcement learning} explains improvement through experience and feedback
\item \textbf{Intentional stance} guides when to treat systems as agents for prediction and control
\end{itemize}

Together, these traditions ground the six-property framework in established theory while keeping it practically usable across disciplines from law to computer science.
\end{keybox}

\subsection{Boundary Cases and Clarifications}

Testing our framework against boundary cases reveals both its strengths and the subtleties of applying formal definitions to real-world systems. These edge cases illuminate practical fault lines—where practitioners disagree about agency and where our framework proves most valuable.

\textbf{Thermostats and the Agency Spectrum.} \glsadd{adaptation}A basic mechanical thermostat demonstrates the spectrum from minimal agency to full agentic systems. It has five of six properties: goal (maintain temperature), perception (sensor readings), action (heating/cooling), iteration (continuous monitoring), and termination (implicit when target reached). Yet it lacks adaptation, applying fixed on/off rules without modifying strategy based on outcomes. This distinguishes reactive control from adaptive learning. Smart thermostats, by contrast, qualify as full agentic systems—they learn occupancy patterns, adjust schedules based on observed behavior, and implement explicit termination logic. This progression illustrates why minimal agency (Level 1) sets a deliberately low bar, while agentic systems (all six properties) require operational sophistication suitable for professional deployment.

\textbf{Multi-Agent Collectives and Emergent Behavior.} When multiple agents interact, the collective can exhibit properties no single agent possesses. A trading floor, legal team, or swarm of autonomous vehicles may demonstrate emergent behaviors that challenge our individual-focused definitions. Legal systems have long grappled with this through doctrines distinguishing corporate from individual liability. Our framework accommodates both perspectives: we can analyze individual agents within the system or treat the collective itself as an agent with emergent goals and capabilities. The choice depends on analytical purpose—attribution questions favor individual analysis, while coordination and emergent behavior favor collective treatment.

\textbf{Virtual versus Embodied Agents.} Software agents need not have physical bodies to qualify as agents. Consider a contract analysis system operating entirely in cloud infrastructure—it satisfies all six properties without any physical embodiment. While robotics researchers sometimes insist on embodiment as essential to agency, our framework follows the broader AI tradition of recognizing virtual agents as fully legitimate instances. The perception-action loop operates equally well whether the environment is physical (sensor readings, motor commands) or virtual (API queries, database updates). Professional legal and financial applications predominantly involve virtual agents, making this inclusive stance practically necessary.

\textbf{Single-Shot LLM Interactions.} A single ChatGPT response, however sophisticated, typically lacks the iteration property required for agentic systems. It processes input once and generates output without maintaining state across multiple perception-action cycles. The system does not observe results, adjust strategy, and continue pursuing goals—it completes and stops. For the purposes of this framework, we treat such single-shot LLM calls as non-iterative, even if they internally perform multiple reasoning steps, because they do not observe and react to \emph{external} feedback within a perception-action loop.

\textbf{Tool Use versus Tool-Using Agents.} Simply using tools does not make an entity agentic. A script that calls an API once is not an agent—it lacks the iteration and adaptation that characterize agentic systems. What matters is purposeful, iterative tool use directed toward goals. \textbf{Simply calling an API once does \emph{not} make a system an agent; the difference lies in goal-directed, iterative tool use with adaptation.} An LLM that repeatedly queries databases, processes results, and decides which tool to invoke next based on accumulated information exhibits agency. The distinction lies in the goal-directed iteration, not the mere presence of tool use. Advanced agentic systems extend this further by creating new tools—writing Python scripts to expand their capabilities or composing prompts that define specialized sub-agents \parencite{claude-code-subagents}. This recursive capability, where agents manage the design and implementation of their own tools, represents a meta-level of agency beyond mere tool invocation.

\textbf{Continuous Control and Implicit Termination.} Entities engaged in continuous control—like systems maintaining server uptime or monitoring market conditions—might seem to violate our termination requirement. In practice, these entities typically operate in bounded episodes with implicit termination conditions: achieving steady state, exhausting available actions, or reaching time limits. Even ``always-on'' entities have mechanisms for recognizing when to stop particular behaviors, satisfying our termination criterion. A market monitoring system may run continuously, but individual monitoring tasks terminate when specific conditions trigger alerts or when trading sessions close. The distinction between the persistent entity and its bounded tasks resolves the apparent contradiction.

\subsection{Professional Deployment}

Understanding what makes an entity agentic is only the first step. In regulated domains like law and finance, deploying AI agents demands robust governance frameworks that ensure compliance, maintain professional standards, and protect sensitive information. The stakes are high: errors can trigger regulatory sanctions, breach fiduciary duties, or compromise client confidentiality.

\begin{cautionbox}[title={\textbf{Governance Cannot Be Retrofitted}}]
\glsadd{governance-surface}
Systems designed without audit logging cannot produce compliance reports when regulators arrive. Those lacking approval gates cannot enforce human oversight after deployment. Build governance in from the start.
\end{cautionbox}

Professional deployment demands that we move beyond theoretical properties to operational safeguards. Each of the six properties creates both opportunities and risks that must be managed through careful design and governance controls. \Cref{ch:how-to-design} addresses the \textit{how}---designing agents with governance-aware architecture---while \Cref{ch:how-to-govern} addresses the \textit{what}---the five-layer regulatory framework and organizational accountability structures that deployed agents must satisfy.

\paragraph{Attribution and Auditability}
Every factual claim must be traceable to its source---case law, regulatory filings, market data, or expert analysis. Modern AI agents should maintain citation chains that satisfy legal brief or investment memorandum standards. Beyond citations, log major reasoning steps, tool invocations, and decision points in forms that preserve privilege while facilitating review. \Cref{sec:agents2-logging-arch} details the logging architecture that makes this possible; \Cref{sec:agents3-audit-logging} specifies what governance frameworks demand.

\paragraph{Bounded Operation}
\glsadd{termination}
Without explicit termination conditions, agentic systems lack the property that distinguishes controlled systems from runaway processes. Enforce computational budgets, time constraints, and maximum iteration counts. In financial contexts, limit market data queries; in legal contexts, cap document review cycles. \Cref{sec:agents2-termination} provides implementation guidance for five categories of termination conditions; \Cref{sec:agents2-loop-detection} addresses guardrails that detect and interrupt unproductive loops.

\paragraph{Escalation Pathways}
\glsadd{escalation}
\glsadd{human-in-the-loop}
Clear triggers---low confidence, high-stakes decisions, detected conflicts---should route matters to human review. A contract review agent might escalate unusual liability provisions; a trading agent might pause before executing orders that exceed authorization thresholds. Escalation represents professionalism, not failure: recognizing when you need help is exactly what we expect from junior professionals. \Cref{sec:agents2-escalation} examines when agents should escalate; \Cref{sec:agents3-human-oversight} addresses human-in-the-loop patterns.

\paragraph{Confidentiality Controls}
Attorney-client privilege, insider trading restrictions, and client confidentiality duties are non-delegable professional obligations. Governance systems must operationalize them through technical controls---ethical walls between matters, role-based access, and redaction rules---not rely on agent ``judgment.'' \Cref{sec:agents2-least-privilege} addresses least-privilege enforcement; \Cref{sec:agents3-professional-obligations} maps the professional responsibility rules that govern legal and financial practice.

\paragraph{Accountability Mapping}
Establish clear responsibility lines before deployment: Who configured the agent? Who reviewed its outputs? Who authorized deployment? These questions must have answers before processing the first client matter. \Cref{sec:agents3-accountability} presents organizational governance models and examines how liability flows when agents cause harm.

\begin{keybox}[title={\textbf{From Framework to Practice}}]
This chapter establishes \textit{what} agents are. The chapters that follow address \textit{how} to build them (\Cref{ch:how-to-design}) and \textit{how} to govern them (\Cref{ch:how-to-govern}). The six properties and governance controls introduced here reappear throughout---as design requirements, as audit checkpoints, and as the foundation for regulatory compliance.
\end{keybox}

\subsection{Common Questions About Agency}

Our framework inevitably raises questions about edge cases and apparent contradictions. Here we address the most common concerns that arise when applying these definitions to real-world systems.

\textbf{Is iteration just repeated action?} Iteration means more than simply repeating the same action multiple times. True iteration involves maintaining state across perception-action cycles, enabling the entity to learn from previous attempts and adjust its approach. An entity that simply retries the same failing action indefinitely is not iterating in our sense---it is stuck in a loop. Iteration requires the ability to incorporate feedback, correct errors, and make progress toward goals through successive refinements. A system that queries the same API endpoint repeatedly with identical parameters is not iterating; one that refines its query based on previous results is. This connection between iteration, memory, and adaptation is fundamental: \Cref{sec:agents2-memory} examines how agents maintain state across cycles, while \Cref{sec:agents2-adaptation} addresses how memory enables behavioral change based on experience. Single-shot entities, no matter how sophisticated, do not qualify as agentic systems under our framework because they lack this capacity to learn and adjust.

\textbf{Do continuous tasks violate the termination requirement?} Entities performing continuous tasks like monitoring or maintenance might seem to run forever, apparently violating our termination requirement. However, these entities still satisfy the termination property through implicit or explicit mechanisms. Implicit termination occurs when the entity reaches a steady state or exhausts available actions. Explicit termination happens through resource budgets, time limits, or escalation triggers. Even an entity that monitors markets continuously operates in bounded episodes—trading sessions, reporting periods, or maintenance windows. The key is that the entity has clear conditions under which it will cease its current behavior pattern, even if it later resumes. The termination requirement ensures predictable resource consumption and prevents runaway processes, not that entities can never restart.

\textbf{If thermostats count as agents, have we trivialized the concept?} Recognizing basic thermostats as minimal agents does not trivialize agency—it establishes a meaningful floor. Many entities fail even this basic test: databases lack goals, compilers lack perception-action loops, and lookup tables lack any form of action. More importantly, while basic thermostats have five operational properties (lacking adaptation), they demonstrate precisely why we distinguish minimal agency (Level 1: three properties) from agentic systems (six properties). The distinction between a basic thermostat and an AI legal research assistant is not whether they are agents, but the degree of autonomy, adaptation capability, sophistication, and responsibility they possess. Our framework acknowledges the thermostat's basic agency while reserving "agentic system" status for entities with all six properties (like smart thermostats that learn patterns), and "AI agent" designation for those using AI/ML for implementation. The gradient from simple to sophisticated agents reflects operational reality.

\textbf{Is "AI agent" just marketing hype?} In our framework, "AI agent" has a precise, falsifiable meaning: an agentic system that uses AI or machine learning (particularly large language models) for planning and orchestration. This excludes single-shot chat completions, no matter how impressive, because they lack iteration. It excludes traditional rule-based systems, even if they meet all six properties, because they do not use AI for implementation. It also excludes simple ML classifiers that lack goal-directedness and autonomous action. When vendors claim their product is an "AI agent," we can test this claim against our six properties and the AI implementation requirement. This transforms marketing language into testable assertions about system architecture and capabilities. The framework provides accountability—vendors must demonstrate iteration, adaptation, and termination mechanisms, not just AI-powered text generation.
