% ============================================================================
% 08-termination.tex
% Q7: How Does an Agent Know When It's Done?
% Part of: Chapter 07 - Agents Part II: How to Design an Agent
% ============================================================================

\section{How Does an Agent Know When It's Done?}
\label{sec:agents2-termination}

% ----------------------------------------------------------------------------
% Opening: Q7 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

Every professional learns to recognize completion. The research memo is done when you have found sufficient authority and synthesized it. The due diligence is done when you have reviewed all material documents. The trade is done when the order executes and settles. Knowing when work is complete distinguishes effective professionals from those who over-research or under-deliver.

Agents face the same challenge. Without explicit termination conditions, agents can run indefinitely: searching one more database, trying one more approach, refining one more time. We call this the ``runaway associate'' problem: you ask for two relevant cases, and the associate gives you fifty because they did not know when to stop.

\begin{definitionbox}[title={Termination}]
	\keyterm{Termination} conditions define when an agent should stop executing. Three outcomes are possible:
	\begin{itemize}[nosep]
		\item \textbf{Success:} The goal is achieved, and the agent delivers the result.
		\item \textbf{Failure:} The goal cannot be achieved; the agent reports why and stops.
		\item \textbf{Escalation:} The agent cannot determine success or failure, transferring the decision to human judgment.
	\end{itemize}

	Termination implements the ``T'' in the GPA+IAT framework introduced in \href{https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5806982}{\textit{What is an Agent?}}. Without it, agentic systems lack the property that distinguishes systems from runaway processes.
\end{definitionbox}

% ----------------------------------------------------------------------------
% Termination Condition Categories
% ----------------------------------------------------------------------------

\subsection{Termination Condition Categories}
\label{sec:agents2-termination-categories}

Five categories of termination conditions bound agent execution, each addressing a different aspect of when and why an agent should stop working autonomously.

\textbf{Success conditions} represent the ideal outcome: the agent terminates because it has achieved its goal. But recognizing success requires clear criteria. \textit{Completeness} asks whether all required items have been addressed---have all fifty contracts in the review queue been analyzed? \textit{Quality} asks whether the output meets the required standard---are conclusions supported by binding authority rather than secondary sources? \textit{Convergence} recognizes when continued effort yields diminishing returns---if three consecutive searches produce no new relevant authority, the research space is likely saturated. Quality assessment often requires human validation, but completeness and convergence can frequently be evaluated programmatically.

\textbf{Resource budgets} provide hard limits that prevent runaway execution. \Cref{sec:agents2-budgets} details budget architecture---token limits, time limits, tool call caps, and cost ceilings. The termination implication is straightforward: when any budget is exhausted, the agent must stop. Budget exhaustion is not necessarily failure; partial results assembled before the limit may be valuable and should be preserved. This principle is formalized in \Cref{sec:agents2-graceful-degradation} as tiered output design.

\textbf{Confidence thresholds} gate autonomous action on the agent's certainty about its conclusions. When confidence is high, the agent delivers its answer and terminates normally. When confidence drops below a specified threshold---perhaps eighty percent for routine matters, higher for consequential decisions---the agent should stop and escalate rather than proceeding with uncertain conclusions. This mirrors the behavior expected of a well-trained associate: ``I've found relevant authority, but I'm not confident it controls here. Let me ask the partner before we rely on this.'' Calibrating these thresholds presents a genuine challenge, as research has shown that language models can be systematically overconfident in their outputs \parencite{kadavath2022calibration}. Effective calibration requires testing against known outcomes and adjusting thresholds based on observed reliability.

\textbf{Error conditions} require agents to recognize when something has gone wrong and continued execution is unlikely to help. \textit{Repeated failures}---a database that times out on three consecutive attempts, an API that returns malformed responses---indicate problems that retrying will not solve. \textit{Inconsistent data}, such as revenue figures in a 10-K that conflict with the earnings release, suggests either an error in the source documents or a parsing problem that requires human investigation. \textit{Constraint violations} demand immediate termination: if a planned trade would exceed position limits or a proposed filing would miss a regulatory deadline, the agent must stop before taking the problematic action. Perhaps most important is recognizing \textit{impossibility}---when requirements genuinely conflict or the requested task cannot be completed as specified, the agent should report this finding rather than compromising on some requirements to satisfy others.

\textbf{Escalation triggers} require human judgment regardless of whether the agent has succeeded or failed at its immediate task. Novel situations without clear precedent, high-stakes decisions with significant consequences, and actions that would exceed the agent's authority boundaries must all trigger handoff to human oversight. Unlike termination with failure, escalation \textit{pauses} the task and requests human input before continuing---the work may resume once the human provides guidance. \Cref{sec:agents2-escalation} examines when escalation (rather than termination) is the appropriate response.

% ----------------------------------------------------------------------------
% Explicit Success Criteria
% ----------------------------------------------------------------------------

\subsection{Defining Success Criteria}
\label{sec:agents2-success-criteria}

While \Cref{sec:agents2-budgets} addressed resource limits---the \textit{hard} stopping points that prevent runaway execution---success criteria address the complementary question: how does the agent recognize that its work is \textit{complete}? Vague goals produce unclear termination. An instruction to ``research the statute of limitations'' leaves the agent uncertain about scope, depth, and format. Which statute of limitations---for what claims, in which jurisdiction? How many sources are enough? What form should the output take? Without answers to these questions, the agent cannot recognize when its work is complete.

Effective success criteria take several forms, often used in combination. \textbf{Completeness checklists} enumerate the specific deliverables required. A credit agreement review might specify that the agent must identify all financial covenants, compare each to market terms from a reference database, and summarize material risks in a structured format. The agent terminates only when every item on the checklist has been addressed, providing a clear and verifiable completion signal.

\textbf{Sufficiency thresholds} define what ``enough'' means for tasks without exhaustive requirements. Legal research rarely requires finding every case ever decided on an issue; instead, sufficiency might mean identifying three on-point opinions from the controlling circuit, or finding both the majority rule and any significant minority positions. Once the threshold is reached, the agent stops rather than continuing to search every available database. This prevents over-research while ensuring adequate coverage.

\textbf{Convergence criteria} recognize when continued effort produces diminishing returns. If three consecutive searches using varied query strategies yield no new relevant results, the research space is likely saturated. The agent can terminate with confidence that additional searching would not materially improve the analysis. This approach works particularly well for exploratory tasks where the scope cannot be fully specified in advance.

\textbf{Deliverable specifications} define the expected output format, which itself signals completion. ``Produce a two-page memorandum with an executive summary, statement of facts, analysis, and conclusion'' tells the agent exactly what success looks like. When the document matches the specification, the task is done.

The most effective approach combines these criteria in instructions that read like guidance to a junior associate:

\begin{quote}
	\textit{``Research the statute of limitations for breach of fiduciary duty claims in Delaware. If you find clear Court of Chancery authority, you are done. If the courts have split or the issue is unsettled, map the competing positions. If you find nothing on point after searching for two hours, stop and report that the issue may be novel. Deliver your findings in a one-page summary with citations.''}
\end{quote}

% ----------------------------------------------------------------------------
% Failure Recognition
% ----------------------------------------------------------------------------

\subsection{Recognizing Failure}
\label{sec:agents2-failure-recognition}

Agents must recognize and report failure honestly, resisting any tendency to mask problems or present incomplete work as complete. This requires a cultural shift in how we think about agent outputs: \textit{negative results are valuable information}, not embarrassing admissions. ``I searched all available databases using multiple query strategies and found no authority on point'' is a legitimate and useful finding---it suggests the issue may be novel, the search terms may need refinement, or the legal theory may lack support.

The difference between useful and useless failure reports lies in \textbf{diagnostic detail}. ``Task failed'' tells the human supervisor nothing actionable. A proper failure report explains what was attempted and why it did not succeed:

\begin{quote}
	\textit{``I searched Westlaw and Lexis using the following queries: [list]. Westlaw returned twelve results, none addressing the specific issue of whether the duty extends to indirect subsidiaries. Lexis returned zero results. The absence of authority suggests either that the issue is novel, that practitioners use different terminology, or that the question is typically resolved through contract rather than litigation. I recommend manual review with an expanded search strategy.''}
\end{quote}

\noindent This report enables the supervisor to decide whether to try different approaches, consult additional resources, or conclude that the absence of authority is itself the answer.

\textbf{Partial completion} must be preserved and clearly reported. If an agent was reviewing ten articles in a contract and encountered a failure after completing four, it should not discard its work. Instead, it should report: ``Analysis complete for Articles 1 through 4; results attached. Articles 5 through 10 remain unanalyzed due to [reason for failure].'' This preserves the value already created and gives the supervisor a clear picture of what remains.

\textbf{Root cause identification} aids the human response by distinguishing between \textit{transient} and \textit{structural} problems. A database timeout is likely transient---waiting and retrying may succeed. A parsing error on a malformed document may require human intervention to obtain a clean copy. Fundamentally conflicting requirements are structural---no amount of retrying will resolve them. The agent's diagnosis of the failure mode directly informs what the supervisor should do next.

% ----------------------------------------------------------------------------
% Guardrails and Loop Detection
% ----------------------------------------------------------------------------

\subsection{Guardrails and Loop Detection}
\label{sec:agents2-loop-detection}

Even well-designed termination conditions cannot prevent every failure mode. Agents can become trapped in unproductive loops---repeating the same actions, cycling through equivalent states, or making nominal progress that adds no real value \parencite{zou2024circuitbreakers,ma2024agentboard}. Production systems require explicit guardrails to detect and interrupt these patterns; \Cref{sec:agents2-override-patterns} addresses how circuit breakers implement these guardrails at the governance layer.

\textbf{Step limits} provide the simplest and most reliable backstop. Regardless of other conditions, after $N$ total steps the agent must stop and require human approval before continuing. This prevents unbounded execution even when other detection mechanisms fail. The appropriate limit depends on the task: a simple lookup might warrant only ten steps, while complex research might allow a hundred. The key is that \textit{some} limit exists and is enforced.

\textbf{Progress detection} monitors whether recent actions have produced value. If the last five tool calls returned no new information---the same documents retrieved, the same search results, the same analysis repeated---the agent is likely stuck in a loop or has exhausted productive avenues. This pattern should trigger either a reflection step (if the agent has that capability) or escalation to human oversight. Progress detection requires defining what ``new information'' means for each task type, which can be as simple as tracking whether retrieved documents have been seen before.

\textbf{Reflection steps} give agents the opportunity to assess their own behavior. Periodically, or when triggered by apparent lack of progress, the agent pauses to ask itself: ``Am I making progress toward the goal? Have my recent actions been productive? Should I try a different approach, or is it time to stop and report what I've found?'' This metacognitive capability is not yet reliable in all models, but when it works, it can catch problems that simple pattern matching would miss.

\textbf{External watchdogs} monitor agent behavior from outside the agent's own reasoning process. A watchdog might detect that the same tool has been called repeatedly with identical or near-identical parameters---a clear sign of a loop---and intervene to halt execution. Watchdogs can also enforce patterns that would be difficult to specify within the agent's instructions, such as rate limits on expensive operations or detection of oscillating behavior where the agent alternates between two states without progressing. Without some form of loop detection, agents deployed in production will eventually get stuck, potentially consuming significant resources before anyone notices.

% ----------------------------------------------------------------------------
% The Reliability Cliff
% ----------------------------------------------------------------------------

\subsection{The Reliability Cliff}
\label{sec:agents2-reliability}

Benchmarking reveals a striking pattern: agent performance does not degrade gradually as tasks become more complex, but instead \textit{falls off a cliff}. METR's 2025 research found near-perfect success on tasks completable in minutes, but under ten percent success on multi-hour tasks \parencite{metr-agent-capability-2025}. The exact boundary shifts as models improve, but the underlying design challenge persists.

\begin{keybox}[title={The Reliability Cliff}, breakable=false]
This cliff reflects \textbf{compounding errors} \parencite{press2023compositionality}: each step has some probability of failure, and these probabilities \textit{multiply}. A chain of twenty steps at 95\% per-step reliability yields only 36\% end-to-end success. Two factors steepen the cliff: \textbf{planning fragility} (early errors propagate forward and invalidate subsequent work) and \textbf{integration brittleness} (API timeouts and rate limits accumulate over time).

\vspace{0.5em}
\textbf{Design for this reality:} Decompose aggressively. Keep agent tasks short. Insert human checkpoints. Build for resumability. And test against your own workflows---benchmark results establish a baseline, but your specific tasks may differ from standardized test suites.
\end{keybox}

% ----------------------------------------------------------------------------
% Graceful Degradation
% ----------------------------------------------------------------------------

\subsection{Graceful Degradation}
\label{sec:agents2-graceful-degradation}

When termination occurs before task completion---whether due to budget exhaustion, confidence drops, or error conditions---the agent should not simply stop and report failure. Instead, it should \textit{degrade gracefully}, delivering whatever value it has accumulated and positioning the human supervisor to continue effectively.

The key to graceful degradation is \textbf{tiered output design}. Rather than treating tasks as all-or-nothing, effective agents structure their work to provide value at multiple levels of completion. Consider a legal research task: with minimal resources, the agent might deliver only the controlling statute and its citation---a modest but genuinely useful result. With moderate resources, it adds the key holdings from relevant cases, providing context for how courts have interpreted the statute. With full resources, it delivers comprehensive analysis including counterarguments, circuit splits, and practical implications. Each tier represents a complete, usable work product rather than a fragment of an unfinished whole. This structure allows the user to assess whether the partial result suffices or whether additional investment is warranted.

\textbf{Progress preservation} ensures that early termination does not waste the work already completed. If an agent stops mid-way through a contract review, it should save its state in a form that allows either itself or a human to resume without starting over. The four articles already analyzed should not require re-analysis; the search queries already executed should not need re-running. This requires deliberate architectural choices---checkpointing intermediate state, maintaining audit trails of completed steps, and structuring tasks as resumable sequences rather than monolithic operations.

\textbf{Clear status reporting} transforms an incomplete result into an actionable handoff. Rather than a bare ``Task incomplete,'' the agent should report its progress precisely: ``Completed analysis of Articles 1 through 4 (60\% of task). Remaining: Articles 5 through 8. Findings so far: two material deviations from market terms identified in covenant structure.'' Critically, the agent should also provide a \textit{recommendation} for next steps: ``Recommend allocating 30 additional minutes to complete the remaining articles, or proceed with partial findings if timeline requires.'' This enables informed human decision-making about whether to invest additional resources, proceed with partial information, or reassign the task entirely.

% ----------------------------------------------------------------------------
% Evaluating Termination
% ----------------------------------------------------------------------------

\subsection{Evaluating Termination Capabilities}
\label{sec:agents2-termination-eval}

Termination evaluation requires six assessments: \textit{success clarity} (are termination conditions explicit and predictable?), \textit{budget enforcement} (are limits actually respected, not exceeded by 20\%?), \textit{loop detection} (does it escape unproductive patterns?), \textit{failure reporting} (do error messages enable effective human follow-up?), \textit{graceful degradation} (do partial results have standalone value?), and \textit{escalation handoff} (does the human receive sufficient context to continue?).

% ----------------------------------------------------------------------------
% Connection to Other Questions
% ----------------------------------------------------------------------------

\subsection{From Termination to Escalation}
\label{sec:agents2-termination-escalation}

Termination and escalation are closely related but distinct concepts. Termination defines \textit{when} an agent stops; escalation defines \textit{what happens next} when stopping requires human involvement. An agent that terminates successfully has completed its task and can deliver results. An agent that terminates due to failure has determined that the task cannot be completed and reports why. But an agent that \textit{escalates} has reached a different conclusion: not ``I'm done'' or ``This is impossible,'' but rather ``I need help to proceed.''

This third category is critical for safe deployment in professional contexts. An agent researching a legal question might find genuinely conflicting authority that requires judgment to reconcile. An agent reviewing a contract might encounter a provision outside its training distribution that it cannot confidently interpret. An agent executing a financial transaction might face a decision that exceeds its authorization limits. In each case, the correct response is neither to forge ahead (risking error) nor to report failure (abandoning recoverable work), but to pause and request human input. This is not failure---it is \textit{safety}.

\Cref{sec:agents2-escalation} examines escalation in depth: when should an agent stop autonomous operation and ask for help? What information should it provide to the human taking over? How should escalation thresholds be calibrated for different risk levels? Together, termination and escalation define the complete boundary of autonomous execution. Without robust termination, agents run indefinitely. Without appropriate escalation, they exceed their authority or make decisions they are not qualified to make. Both capabilities are essential for trustworthy deployment. \Cref{sec:agents2-governance} examines how escalation hooks must be architecturally implemented to ensure they are reliable and cannot be bypassed.
