% ============================================================================
% 04-perception.tex
% Q3: How Does an Agent Find Things Out?
% Part of: Chapter 2 - How to Design an Agent
% ============================================================================

\section{How Does an Agent Find Things Out?}
\label{sec:agents2-perception}

% ----------------------------------------------------------------------------
% Opening: Q3 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

Understanding what someone wants is not the same as being able to deliver it. The previous section examined how agents interpret instructions, extracting goals, detecting ambiguity, and gathering the context needed to proceed. But even an agent that perfectly understands ``Research Ninth Circuit authority on personal jurisdiction for foreign corporations'' cannot help if it lacks access to case law databases. Where Chapter~1 introduced perception as the ability to observe the world, we now examine the concrete mechanisms---tools and integrations---that enable agents to gather the information they need. An agent that correctly interprets ``Analyze this credit agreement from the lender's perspective'' is useless without the credit agreement itself.

This constraint will feel familiar to anyone who has onboarded a new professional. A junior associate's effectiveness depends as much on access as on reasoning ability. Can they query Westlaw? Do they have credentials for the Bloomberg terminal? Can they search the firm's precedent database and document management system? The answers to these questions determine which problems they can solve. A brilliant analyst without access to portfolio data reasons in a vacuum; a talented associate without access to the case file works blind.

\glsadd{llm}Agentic systems face exactly the same constraint. A large language model can reason impressively about legal doctrines and financial concepts, but without integrations into external or internal sources, it cannot access current case law, live market prices, client documents, or regulatory filings. These connections are what we call \textit{perception tools}: the interfaces through which an agent observes the world beyond its training data.

\begin{definitionbox}[title={Tools and Perception}, breakable=false]
\glsadd{tools}
\glsadd{perception}
	A \keyterm{tool} is a function that allows an agent to interact with external systems. \keyterm{Perception tools} are the subset that are read-only: they let the agent observe without changing anything. When an agent queries a database, retrieves a document, or fetches market data, the external system's state remains unchanged.

	\vspace{0.5em}
	Perception defines the boundary of what information the agent can access, and that boundary shapes every downstream decision. An agent with access to public filings reasons differently than one with access to internal deal documents. An agent that can query real-time market data operates differently than one limited to end-of-day prices.
\end{definitionbox}

Here we examine perception: the read-only tools that let agents gather the information they need. The next section, on action, examines write tools that let agents change things in the world. The distinction matters for governance: reading a document and sending an email carry fundamentally different risks, and your controls should reflect that difference. \Cref{sec:agents2-action} develops these distinctions and the different oversight mechanisms each requires.

% ----------------------------------------------------------------------------
% Perception Tool Categories
% ----------------------------------------------------------------------------

\subsection{Perception Tool Categories}
\label{sec:agents2-perception-categories}

Not all perception tools work the same way, and choosing the right tool for the task matters. Perception tools generally fall into three categories: information retrieval, document processing, and computation.

\textbf{Information retrieval tools} query external platforms and databases to bring back answers. On the legal side, these tools connect to research services like Westlaw and Lexis in the United States, Beck-Online in Germany, LawNet in Singapore, and similar platforms in other jurisdictions, allowing agents to search case law, retrieve full opinions, check citing references, and download court filings. On the finance side, similar tools connect to platforms like Bloomberg, FactSet, or Refinitiv, enabling agents to pull real-time prices, retrieve company fundamentals, and access analyst research. Many organizations also have internal knowledge bases, including document management systems, deal archives, and precedent databases, that agents need to search to find prior work product relevant to a current matter.

\textbf{Document processing tools} transform raw files into data an agent can actually work with. A credit agreement arrives as a PDF; a financial statement comes as a scanned image; a data room contains thousands of files in mixed formats. Before an agent can reason about this content, it needs to extract the text (using OCR for scanned documents), identify what type of document each file is, and pull out structured information like party names, dates, and dollar amounts. During due diligence, for example, an agent reviewing a data room must distinguish contracts from correspondence, extract key terms from each agreement, and organize findings in a way that supports analysis. Without document processing tools, the agent sees only filenames and metadata.

\textbf{Computation tools} generate new information through calculation rather than lookup. Take deadline calculation: the Federal Rules of Civil Procedure require an answer within 21 days of service \parencite{frcp-rule-12}, but determining the actual due date requires accounting for weekends, court holidays, and local rules. That is a computational task, not a database query. Citation formatters perform a similar function, converting case information into proper Bluebook or other citation styles. In finance, computation tools normalize security identifiers (mapping between tickers, CUSIPs, and ISINs), calculate risk metrics like Value at Risk from position data, or derive analytics that inform investment decisions. These tools produce new information for the agent to use without changing anything in the external world.

% ----------------------------------------------------------------------------
% In-Context Learning and Retrieval Fundamentals
% ----------------------------------------------------------------------------

\subsection{In-Context Learning and Retrieval}
\label{sec:agents2-icl-retrieval}

Before examining specific retrieval mechanisms, we must understand the fundamental capability that makes retrieval useful: \keyterm{in-context learning}. This concept is central to how modern language models work and why retrieval matters.

\begin{definitionbox}[title={In-Context Learning}, breakable=false]
\glsadd{in-context-learning}
	\keyterm{In-context learning (ICL)} is the ability of large language models to learn from information provided in their input---without any update to the model's underlying parameters \parencite{brown2020gpt3}. When you provide examples, documents, or instructions in a prompt, the model adapts its responses based on that context. This is not ``learning'' in the traditional machine learning sense of updating weights; it is learning \textit{within the conversation}.

	\vspace{0.5em}
	In-context learning is what makes retrieval valuable. When an agent retrieves a relevant statute or prior memo and includes it in its prompt, the model can reason about that specific content even though it never saw it during training. The model's behavior changes based on what appears in its context window.
\end{definitionbox}

In-context learning explains why agents can work with information far newer than their training data. A model trained in 2024 can analyze a regulation enacted in 2025---provided that regulation appears in its context. The limitation is the \keyterm{context window}: the maximum amount of text the model can process at once. Current models handle roughly 200,000 to 1,000,000 tokens (where a token is roughly three-quarters of a word),\footnote{Context window sizes as of December 2025 (vendor-reported): Claude Opus 4.5 (200k tokens), Gemini 3 (up to 1M tokens), GPT-5.2 (256k tokens). These specifications change frequently; consult current model documentation.} but the cost, speed, and quality of results vary widely---and dangerously---when operating on large amounts of input. A model may technically accept a million tokens while producing degraded results on tasks requiring attention to details scattered throughout. Professional knowledge bases contain millions of documents regardless; the gap between what fits in context and what exists in the world creates the need for retrieval.

\begin{keybox}[title={Why Agentic Architecture Works}]
	The practical limits of context windows explain much of the value of agentic systems. Rather than attempting to process an entire knowledge base in a single prompt---which would exceed limits and degrade quality regardless---an agent decomposes work into focused steps, retrieves only what is relevant to each step, and reasons over smaller, manageable contexts.

	\vspace{0.5em}
	This is not merely automation; it is an architectural solution to a fundamental constraint. Planning (\Cref{sec:agents2-planning}) breaks complex tasks into subtasks, ensuring each retrieval step is targeted rather than overwhelming. Retrieval fetches specific information for each subtask. The LLM operates within its effective range on each step, even when the overall task would be impossible to handle monolithically.

	\vspace{0.5em}
	This decomposition explains both why agentic systems are powerful and why they fail in predictable ways. The reliability cliff (\Cref{sec:agents2-termination}) shows that short, focused tasks succeed while long tasks fail---precisely because decomposition keeps individual steps within the model's effective operating range, but errors compound across many steps.
\end{keybox}

\paragraph{Retrieval Methods}

To retrieve relevant information from large collections, we need a way to find what matters. Several approaches exist, each with distinct strengths:

\textbf{Keyword and boolean search} is the oldest and most familiar approach. Platforms like Westlaw, Lexis, and Bloomberg have offered sophisticated keyword search for decades. Boolean operators (AND, OR, NOT), proximity searches, and field-specific queries give users precise control. When you need documents containing exact terms---a specific case citation, a statutory section, a company name---keyword search excels. Its limitation is vocabulary mismatch: a search for ``breach of fiduciary duty'' will not find documents discussing ``violation of trust obligations'' unless both phrases happen to appear.

\textbf{BM25 and probabilistic search} extends keyword matching with statistical weighting \parencite{robertson2009bm25}. Terms that appear rarely in the corpus but frequently in a document signal relevance more strongly than common terms. BM25 powers many production search systems, including Elasticsearch and the baseline retrievers in academic benchmarks. It requires no machine learning infrastructure---just an inverted index---making it fast, interpretable, and cheap to operate.

\textbf{Embedding-based semantic search} uses machine learning to encode meaning numerically. An \keyterm{embedding} is a vector---a list of numbers---that represents semantic content. Similar concepts produce similar vectors; different concepts produce distant vectors. The phrases ``breach of fiduciary duty'' and ``violation of trust obligations'' have embeddings close together in vector space, even though they share no words. This enables \textit{meaning matching} rather than vocabulary matching. Embedding models vary in quality and domain fit; legal-specific models may outperform general-purpose ones for professional content. The infrastructure cost is higher: you need embedding models, vector storage, and similarity search capabilities.

\textbf{Structured queries} retrieve from databases and APIs. An agent checking a company's SEC filings queries EDGAR; an agent researching a case retrieves from a court's electronic filing system. These are not ``search'' in the traditional sense---they are direct data access---but they serve the same function in RAG: finding relevant information to inject into context.

\textbf{Hybrid approaches} combine methods. A common pattern uses BM25 for initial retrieval (fast, cheap, catches exact matches) followed by semantic reranking (slower, more expensive, captures meaning). Many production systems blend keyword and semantic scores, getting the best of both approaches.

\begin{keybox}[title={Choosing Retrieval Methods}, breakable=false]
	No single retrieval method dominates. The right choice depends on your content, queries, and constraints:
	\begin{itemize}[nosep]
		\item \textbf{Exact terms matter}: Keyword or BM25. Case citations, statutory references, and party names must match precisely.
		\item \textbf{Conceptual similarity matters}: Embeddings. Finding documents about ``materiality'' when the query says ``significance'' requires semantic matching.
		\item \textbf{Structured data}: Database queries or APIs. Retrieving a specific filing from EDGAR or a price from Bloomberg.
		\item \textbf{Production constraints}: BM25 is cheaper and faster; embeddings require more infrastructure but capture meaning better.
	\end{itemize}
	Most professional systems use hybrid approaches, combining methods to balance precision, recall, and cost.
\end{keybox}

\paragraph{Vector Stores for Embedding-Based Search}

When using embeddings, storing and searching millions of vectors efficiently requires specialized infrastructure.

\begin{definitionbox}[title={Vector Stores}, breakable=false]
\glsadd{vector-store}
	A \keyterm{vector store} (or vector database) is a system optimized for storing embeddings and performing similarity search. Given a query embedding, a vector store returns the most similar document embeddings from its collection. Vector stores use specialized indexing structures (like HNSW or IVF) that make approximate nearest-neighbor search practical at scale.

	\vspace{0.5em}
	Vector stores are \textit{infrastructure}---the retrieval mechanism that powers semantic search. They are not the same as the retrieval pattern itself.
\end{definitionbox}

\paragraph{Retrieval-Augmented Generation (RAG)}

With these retrieval methods in hand, we can now define RAG precisely.

\begin{definitionbox}[title={Retrieval-Augmented Generation (RAG)}, breakable=false]
\glsadd{rag}
	\keyterm{Retrieval-Augmented Generation (RAG)} is a \textit{prompt pattern}---not a software system or mathematical technique \parencite{lewis2020rag}. RAG augments a model's context with retrieved information before generation. The pattern has three steps:
	\begin{enumerate}[nosep]
		\item \textbf{Retrieve}: Given a query, find relevant content using \textit{any} retrieval method---keyword search, BM25, embeddings, database queries, API calls, or combinations thereof.
		\item \textbf{Augment}: Insert the retrieved content into the model's prompt as additional context.
		\item \textbf{Generate}: The model produces a response informed by both its training and the retrieved content.
	\end{enumerate}

	\vspace{0.5em}
	RAG works because of in-context learning. The retrieved content appears in the prompt, and the model adapts its response to incorporate that specific information. The retrieval step is completely method-agnostic: what matters is getting relevant content into context, not how you find it.
\end{definitionbox}

The distinction matters. When evaluating a system, you should ask: What retrieval infrastructure does it use? (Keyword index, vector store, database, external API, hybrid?) What RAG pattern does it implement? (Single-stage, multi-stage, with reranking?) These are separate architectural decisions with different tradeoffs.

\begin{keybox}[title={RAG Is Retrieval-Agnostic}, breakable=false]
	A common misconception equates RAG with embeddings and vector stores. In practice, RAG implementations vary widely:
	\begin{itemize}[nosep]
		\item \textbf{Keyword search}: Westlaw, Lexis, or internal full-text search---no embeddings required.
		\item \textbf{BM25}: Fast probabilistic retrieval powering many production systems.
		\item \textbf{Semantic search}: Embeddings and vector stores for meaning-based matching.
		\item \textbf{Structured queries}: SQL databases, EDGAR filings, court record APIs.
		\item \textbf{Hybrid}: Combining methods to balance precision, recall, and cost.
	\end{itemize}
	The ``embedding plus vector store'' pattern dominates tutorials because it is general-purpose and handles semantic similarity well. But many production systems---especially those leveraging existing search infrastructure---use simpler approaches that work well for their specific needs.
\end{keybox}

% ----------------------------------------------------------------------------
% Model Context Protocol (MCP) for Perception
% ----------------------------------------------------------------------------

\subsection{Model Context Protocol (MCP)}
\label{sec:agents2-mcp-perception}

\glsadd{mcp}
One of the persistent challenges in building agentic systems is integration. Every database, every document management system, every market data feed has its own way of accepting queries and returning results. Historically, connecting an agent to a new information source meant writing custom code for that specific system. Multiply that by the number of agents and the number of tools an organization uses, and the integration burden grows quickly.

The Model Context Protocol (MCP) addresses this problem by standardizing how agents access tools \parencite{anthropic-mcp,mcp-spec}. Instead of learning a different integration pattern for each system, agents learn the protocol once and can then access any compatible tool. For organizations, this means a single point of audit and control rather than dozens of bespoke connections to monitor.

The architecture has three components. The \keyterm{MCP Host} manages the agent and controls what it can access, functioning like a firm's IT department deciding which systems a new employee can use. The \keyterm{MCP Client} is the agent-side component that discovers available tools and makes requests. The \keyterm{MCP Server} is what each information source runs to expose its capabilities through the standard interface. A document management system, an internal knowledge base, or a proprietary analytics platform can each run as an MCP server, and any MCP-compatible agent can then access them without custom integration work.

For perception specifically, MCP defines \keyterm{Resources} as read-only data access endpoints. These might include document repositories, market data feeds, regulatory filing databases, or internal knowledge bases. The read-only designation matters: an agent with resource access can retrieve documents but cannot modify or file them. This separation enables fine-grained access control that mirrors how organizations already think about permissions.

\begin{keybox}[title={The Integration Landscape Is Changing}]
	Historically, many information providers in law and finance offered no programmatic access at all. Legal research platforms required human users working through web interfaces; market data terminals assumed a person at the keyboard. This created a barrier for agentic systems, which need machine-readable interfaces to function.

	\vspace{0.5em}
	That landscape is shifting. Competitive pressure and customer demand are pushing providers to open up. Document management systems, e-discovery platforms, and financial data providers increasingly offer APIs that agents can use. Some legal research providers are beginning to follow. As this trend continues, the range of information sources available to agentic systems will expand significantly.

	\vspace{0.5em}
	Standards like MCP accelerate this shift by reducing the cost of integration. Without MCP, connecting 10 agents to 10 tools requires 100 custom integrations. With MCP, the same setup requires only 20 implementations, since each agent and each tool learns the protocol once. Recent benchmarks found over ten thousand MCP servers already in the ecosystem \parencite{livemcpbench-2025}, and that number will likely grow substantially in the years ahead.
\end{keybox}

% ----------------------------------------------------------------------------
% Memory as Perception
% ----------------------------------------------------------------------------

\subsection{Memory as Perception into Institutional Knowledge}
\label{sec:agents2-memory-perception}

Memory systems (\Cref{sec:agents2-memory}) serve as perception tools for institutional knowledge. The retrieval concepts introduced above---embeddings, vector stores, and RAG---enable agents to perceive accumulated expertise that would otherwise be inaccessible.

When an agent queries a precedent database using the RAG pattern (\Cref{sec:agents2-icl-retrieval}), it perceives institutional knowledge through in-context learning. The retrieved content appears in the agent's prompt, allowing it to reason about specific precedents, prior analyses, and established approaches. A search for ``breach of fiduciary duty'' retrieves documents about ``violation of trust obligations'' because their embeddings are similar---not because they share keywords.

This mechanism enables perception into knowledge bases far too large to fit in any model's context window. A law firm's precedent database might contain decades of work product; a financial institution's research archive might span thousands of analyst reports. No agent can hold all of this in active context. RAG allows selective retrieval: the agent perceives only the most relevant fragments, guided by semantic similarity to the current task.

Institutional memory provides access to prior work product. When drafting a new registration statement, an agent can perceive prior S-1 filings (IPO registrations), SEC comment histories, and successful disclosure language. This access allows current work to build on verified precedents rather than starting from first principles.

Memory-as-perception distinguishes experienced agents from novices. A junior associate reasons from what they learned in law school; a senior associate draws on pattern recognition from hundreds of matters. Memory provides agents with this accumulated experience---but only if the retrieval infrastructure connects them to the right knowledge at the right time. \Cref{sec:agents2-memory} develops these requirements in detail, including how memory systems must enforce the authority, temporal validity, and isolation constraints introduced above.

% ----------------------------------------------------------------------------
% Domain-Specific Perception Requirements
% ----------------------------------------------------------------------------

\subsection{Domain-Specific Perception Requirements}
\label{sec:agents2-perception-domain}

Perception for regulated professional services requires specialized enhancements. General-purpose search is insufficient; professional agents require authority tracking, jurisdictional awareness, and confidentiality boundaries.

\paragraph{Authority and Verification}

Information varies in authority. Perception systems must track provenance to ensure reliability. Authority weighting ranks primary sources (statutes, binding precedent) above secondary sources (law reviews, news). When searching for ``insider trading liability,'' a Supreme Court opinion outranks a commentary article. Source verification confirms that retrieved information originates from the claimed source. Perception tools must return verifiable citations, not just text. Currency validation ensures the authority remains valid. Integrated citators (like Shepard's or KeyCite) verify that retrieved cases have not been overruled.

\paragraph{Jurisdiction and Temporal Scope}

Legal and regulatory information is bounded by jurisdiction. California precedent does not bind Texas courts; SEC rules differ from CFTC rules. Perception tools must filter results by relevant jurisdiction. Temporal validity is equally critical. Laws change, and financial data expires. Perception systems must track effective dates. In finance, validity varies by context: milliseconds for trading prices, quarters for compliance reporting. Identifier resolution manages the proliferation of formats. ``123 F.3d 456'' and ``123 F3d 456'' refer to the same case. Financial identifiers include tickers, CUSIPs, and LEIs (Legal Entity Identifiers). Perception must normalize these to ensure consistent retrieval.

\paragraph{Matter and Client Isolation}

Critically, perception must respect confidentiality boundaries. Whether a human or AI, an agent working on Matter A cannot perceive documents from adverse Matter B. This enforcement of \keyterm{ethical walls} arguably must occur at the perception layer. In financial contexts, an agent advising Client X cannot perceive material non-public information (MNPI) from Client Y's engagement. Every perception event must be logged, capturing the agent, the query, and the matter context. This audit trail enables compliance review and breach detection. See \Cref{sec:agents2-memory} for detailed treatment of isolation requirements; Chapter~3 (\textit{How to Govern an Agent}) addresses the professional responsibility obligations---including attorney confidentiality duties and fiduciary obligations---that mandate these controls.

% ----------------------------------------------------------------------------
% Tool Design Principles for Perception
% ----------------------------------------------------------------------------

\subsection{Tool Design Principles}
\label{sec:agents2-perception-design}

Robust perception tools follow design principles that enable reliable operation in professional environments.

\paragraph{Single Responsibility}

Each tool should perform one function well. Poorly designed tools bundle multiple functions---searching, formatting, and validation---into opaque interfaces. Untyped return values obscure what callers can expect.

\begin{listingbox}[title={Poor Design: Bundled Functions, Untyped Returns}, listing options={language=Python}]
def legal_research(query: str) -> dict:
  """Returns... something. Good luck."""
  ...
\end{listingbox}

When such a tool fails, diagnosing the error is difficult. A better approach separates tools by function with typed returns. This allows the agent to compose them and isolates failures.

\begin{listingbox}[title={Better Design: Single Responsibility, Typed Returns}, listing options={language=Python}, breakable=false]
def search_cases(query: str, jurisdiction: str) -> list[Citation]:
  """Returns matching citations from case law database."""

def retrieve_case(citation: Citation) -> CaseText:
  """Fetches full text for a specific citation."""

def shepardize(citation: Citation) -> CitatorResult:
  """Checks validity: good law, distinguished, overruled."""

def format_citation(case: CaseText, style: str) -> str:
  """Converts to Bluebook, ALWD, or other format."""
\end{listingbox}

\paragraph{Graceful Failure}

Production systems inevitably fail. Tools should return informative errors rather than generic exceptions. A poor approach raises exceptions that provide no context.

\begin{listingbox}[title={Poor: Opaque Exception}, listing options={language=Python}]
def retrieve_case(citation: str) -> dict:
  result = db.query(citation)
  return result["text"]  # raises KeyError if not found
\end{listingbox}

A better approach uses typed result objects that make success and failure explicit.

\begin{listingbox}[title={Better: Typed Result with Structured Errors}, listing options={language=Python}]
class CaseNotFoundError(BaseModel):
  citation: str
  reason: str
  suggestions: list[str]

def retrieve_case(citation: Citation) -> CaseText | CaseNotFoundError:
  """Returns case text or structured error with recovery options."""
  if not (result := db.query(citation)):
    return CaseNotFoundError(
      citation=str(citation),
      reason="Case may not be in database",
      suggestions=["Check citation format", "Try alternative reporter"]
    )
  return CaseText(...)
\end{listingbox}

In professional practice, graceful failure prevents malpractice. When an agent cannot find authority, it must report that explicitly rather than proceeding silently.

\paragraph{Least Privilege and Rate Limiting}

Perception tools should request minimum necessary permissions. A legal research tool requires read access to case databases, not write access to the document management system. If a compromised agent gains perception credentials, damage is limited to information disclosure rather than destruction. Rate limiting addresses a common failure mode: infinite search loops. Tools should track invocation frequency and refuse requests beyond reasonable thresholds. If an agent searches five times without results, the tool should force a stop and escalation (\Cref{sec:agents2-escalation}).

% ----------------------------------------------------------------------------
% Evaluating Perception Capabilities
% ----------------------------------------------------------------------------

\subsection{Evaluating Perception Capabilities}
\label{sec:agents2-perception-eval}

When evaluating agentic systems, you should assess perception against criteria that matter for professional practice.

\textbf{Coverage} determines which sources the agent can access. A litigation agent that queries Westlaw but not state-specific databases has incomplete coverage. You must map available perception tools against information needs to identify gaps.

\textbf{Retrieval quality} measures whether the agent finds relevant information. Test with known-good queries where the correct result is established. Measure both \keyterm{precision} (the fraction of retrieved documents that are actually relevant) and \keyterm{recall} (the fraction of all relevant documents that the system successfully retrieves). These metrics will be familiar to legal professionals from technology-assisted review (TAR) in e-discovery, where courts have recognized precision and recall as the standard measures of retrieval effectiveness \parencite{grossman2011tar}. The same framework applies to agent perception: high precision means the agent does not waste time on irrelevant results; high recall means the agent does not miss important authorities. The tradeoff between them---casting a wider net improves recall but may reduce precision---is a design decision that should be calibrated to the task's risk profile.

\textbf{Verification} confirms that the system distinguishes authoritative from secondary sources. You must ensure that retrieved information is traceable to its source and that citations are independently verifiable.

\textbf{Access controls} ensure that permissions are appropriate. The agent must access only what it should, and confidentiality boundaries must hold across matter and client lines.

\textbf{Failure handling} reveals system behavior when perception fails. Does it retry, try alternatives, or escalate? It must not crash or proceed with incomplete information.

\textbf{Audit capability} confirms that every perception event is logged. You must be able to reconstruct what information the agent accessed during a task for compliance review.

% ----------------------------------------------------------------------------
% Connection to Other Questions
% ----------------------------------------------------------------------------

\subsection{From Perception to Action}
\label{sec:agents2-perception-action}

Perception enables agents to gather information, but professional value ultimately requires effecting change: filing documents, sending communications, executing trades. This distinction between observing the world and changing it is fundamental to agent architecture, and the protocols we use reflect it. MCP, introduced earlier in this section, explicitly separates \textbf{Resources} (read-only data access) from \textbf{Tools} (operations that modify state). A single MCP server might expose both capabilities---a document management system could offer read access to files alongside the ability to create, modify, or delete them---but the protocol distinguishes these so that access control and governance can treat them differently.

The distinction matters because the consequences of failure differ fundamentally. When perception tools fail---a search returns wrong results, a document fails to load---the external world remains unchanged. The agent can retry, try alternatives, or escalate to human review without having caused any harm beyond wasted time. Action tools carry different stakes entirely. They file documents that become part of court records, send emails that reach recipient inboxes, execute trades that transfer ownership at market prices. Once executed, many actions cannot be undone, or can only be undone at significant cost. The agent's mistakes become facts in the world.

\Cref{sec:agents2-action} examines action capabilities in detail, beginning with the conceptual foundations that distinguish actions from observations and developing the governance frameworks that these differences require.
