% ============================================================================
% 04-perception.tex
% Q3: How Does an Agent Find Things Out?
% Part of: Chapter 2 - How to Design an Agent
% ============================================================================

\section{Perception}
\label{sec:agents2-perception}

% ----------------------------------------------------------------------------
% Opening: Q3 Framing and Organizational Analogy
% ----------------------------------------------------------------------------

\textit{How does an agent find things out?} Understanding what someone wants is not the same as being able to deliver it. The previous section examined how agents interpret instructions, extracting goals, detecting ambiguity, and gathering the context needed to proceed. But even an agent that perfectly understands ``Research Ninth Circuit authority on personal jurisdiction for foreign corporations'' cannot help if it lacks access to case law databases. Where Chapter~1 introduced perception as the ability to observe the world, we now examine the concrete mechanisms (tools and integrations) that enable agents to gather the information they need. An agent that correctly interprets ``Analyze this credit agreement from the lender's perspective'' is useless without the credit agreement itself.

This constraint will feel familiar to anyone who has onboarded a new professional. A junior associate's effectiveness depends as much on access as on reasoning ability. Can they query Westlaw? Do they have credentials for the Bloomberg terminal? Can they search the firm's precedent database and document management system? The answers to these questions determine which problems they can solve. A brilliant analyst without access to portfolio data reasons in a vacuum; a talented associate without access to the case file works blind.

\glsadd{llm}Agentic systems face exactly the same constraint. A large language model can reason impressively about legal doctrines and financial concepts, but without integrations into external or internal sources, it cannot access current case law, live market prices, client documents, or regulatory filings. These connections are what we call \textit{perception tools}: the interfaces through which an agent observes the world beyond its training data.

\begin{definitionbox}[title={Tools and Perception}, breakable=false]
\glsadd{tools}
\glsadd{perception}
	A \keyterm{tool} is a function that allows an agent to interact with external systems. \keyterm{Perception tools} are the subset that are read-only: they let the agent observe without changing anything. When an agent queries a database, retrieves a document, or fetches market data, the external system's state remains unchanged.

	\smallskip
	Perception defines the boundary of what information the agent can access, and that boundary shapes every downstream decision. An agent with access to public filings reasons differently than one with access to internal deal documents. An agent that can query real-time market data operates differently than one limited to end-of-day prices.
\end{definitionbox}

Here we examine perception: the read-only tools that let agents gather the information they need. The next section, on action, examines write tools that let agents change things in the world. The distinction matters for governance: reading a document and sending an email carry fundamentally different risks, and your controls should reflect that difference. \Cref{sec:agents2-action} develops these distinctions and the different oversight mechanisms each requires.

% ----------------------------------------------------------------------------
% Perception Tool Categories
% ----------------------------------------------------------------------------

\subsection{Perception Tool Categories}
\label{sec:agents2-perception-categories}

Not all perception tools work the same way, and choosing the right tool for the task matters. Perception tools generally fall into three categories: information retrieval, document processing, and computation.

\textbf{Information retrieval tools} query external platforms and databases to bring back answers. On the legal side, these tools connect to research services like Westlaw and Lexis in the United States, Beck-Online in Germany, LawNet in Singapore, and similar platforms in other jurisdictions, allowing agents to search case law, retrieve full opinions, check citing references, and download court filings. On the finance side, similar tools connect to platforms like Bloomberg, FactSet, or Refinitiv, enabling agents to pull real-time prices, retrieve company fundamentals, and access analyst research. Many organizations also have internal knowledge bases, including document management systems, deal archives, and precedent databases, that agents need to search to find prior work product relevant to a current matter.

\textbf{Document processing tools} transform raw files into data an agent can actually work with. A credit agreement arrives as a PDF; a financial statement comes as a scanned image; a data room contains thousands of files in mixed formats. Before an agent can reason about this content, it needs to extract the text (using OCR for scanned documents), identify what type of document each file is, and pull out structured information like party names, dates, and dollar amounts. During due diligence, for example, an agent reviewing a data room must distinguish contracts from correspondence, extract key terms from each agreement, and organize findings in a way that supports analysis. Without document processing tools, the agent sees only filenames and metadata.

\textbf{Computation tools} generate new information through calculation rather than lookup. Take deadline calculation: the Federal Rules of Civil Procedure require an answer within 21 days of service \parencite{frcp-rule-12}, but determining the actual due date requires accounting for weekends, court holidays, and local rules. That is a computational task, not a database query. Citation formatters perform a similar function, converting case information into proper Bluebook or other citation styles. In finance, computation tools normalize security identifiers (mapping between tickers, CUSIPs, and ISINs), calculate risk metrics like Value at Risk from position data, or derive analytics that inform investment decisions. These tools produce new information for the agent to use without changing anything in the external world.

% ----------------------------------------------------------------------------
% In-Context Learning and Retrieval Fundamentals
% ----------------------------------------------------------------------------

\subsection{In-Context Learning and Retrieval}
\label{sec:agents2-icl-retrieval}

Before examining specific retrieval mechanisms, we must understand the fundamental capability that makes retrieval useful: \keyterm{in-context learning}. This concept is central to how modern language models work and why retrieval matters.

\begin{definitionbox}[title={In-Context Learning}, breakable=false]
\glsadd{in-context-learning}
	\keyterm{In-context learning (ICL)} is the ability of large language models to learn from information provided in their input, without any update to the model's underlying parameters \parencite{brown2020gpt3}. When you provide examples, documents, or instructions in a prompt, the model adapts its responses based on that context. This is not ``learning'' in the traditional machine learning sense of updating weights; it is learning \textit{within the conversation}.

	\smallskip
	In-context learning is what makes retrieval valuable. When an agent retrieves a relevant statute or prior memo and includes it in its prompt, the model can reason about that specific content even though it never saw it during training. The model's behavior changes based on what appears in its context window.
\end{definitionbox}

In-context learning explains why agents can work with information far newer than their training data. A model trained in 2024 can analyze a regulation enacted in 2025, provided that regulation appears in its context. The limitation is the \keyterm{context window}: the maximum amount of text the model can process at once. Context windows have expanded rapidly (from thousands of tokens to hundreds of thousands, with some models now accepting a million or more), and this trend will likely continue. But raw capacity tells only part of the story. Cost, speed, and quality of results vary widely when operating on large amounts of input. A model may technically accept a million tokens while producing degraded results on tasks requiring attention to details scattered throughout. Professional knowledge bases contain millions of documents regardless; the gap between what fits in context and what exists in the world creates the need for retrieval.

The practical limits of context windows explain much of the value of agentic systems. Rather than attempting to process an entire knowledge base in a single prompt (which would exceed limits and degrade quality regardless), an agent decomposes work into focused steps, retrieves only what is relevant to each step, and reasons over smaller, manageable contexts.

This is not merely automation; it is an architectural solution to a fundamental constraint. Planning (\Cref{sec:agents2-planning}) breaks complex tasks into subtasks, ensuring each retrieval step is targeted rather than overwhelming. Retrieval fetches specific information for each subtask. The LLM operates within its effective range on each step, even when the overall task would be impossible to handle monolithically.

\begin{keybox}[title={Why Decomposition Matters}]
Decomposition explains both why agentic systems are powerful and why they fail in predictable ways. The reliability cliff (\Cref{sec:agents2-termination}) shows that short, focused tasks succeed while long tasks fail; this occurs precisely because decomposition keeps individual steps within the model's effective operating range, but errors compound across many steps.
\end{keybox}

\paragraph{Retrieval Methods}

To retrieve relevant information from large collections, we need a way to find what matters. Several approaches exist, each with distinct strengths:

\textbf{Keyword and boolean search} is the oldest and most familiar approach. Platforms like Westlaw, Lexis, and Bloomberg have offered sophisticated keyword search for decades. Boolean operators (AND, OR, NOT), proximity searches, and field-specific queries give users precise control. When you need documents containing exact terms (such as a specific case citation, a statutory section, or a company name), keyword search excels. Its limitation is vocabulary mismatch: a search for ``breach of fiduciary duty'' will not find documents discussing ``violation of trust obligations'' unless both phrases happen to appear.

\textbf{BM25 and probabilistic search} extends keyword matching with statistical weighting \parencite{robertson2009bm25}, building on \keyterm{TF-IDF} (Term Frequency-Inverse Document Frequency). TF-IDF weights terms by how often they appear in a document relative to how common they are across the corpus. A term that appears frequently in one document but rarely elsewhere signals that document's relevance more strongly than a common term appearing everywhere. BM25 refines TF-IDF with saturation functions that prevent very frequent terms from dominating scores. The method requires no machine learning infrastructure, just an inverted index, making it fast, interpretable, and cheap to operate. BM25 remains the baseline retriever in academic benchmarks and powers many production search systems.

\textbf{Embedding-based semantic search} uses machine learning to encode meaning numerically. An \keyterm{embedding} is a vector (a list of numbers) that represents semantic content. The goal is that similar concepts should produce similar vectors, while different concepts produce distant vectors. When this works well, phrases like ``breach of fiduciary duty'' and ``violation of trust obligations'' have embeddings close together in vector space, even though they share no words. This enables \textit{meaning matching} rather than vocabulary matching.

Embedding quality varies significantly across models and tasks. Different embedding models excel at different objectives: some are optimized for clustering documents by topic, others for matching semantically similar sentences, and still others for asymmetric retrieval where short queries must find longer passages. A model trained for one task may perform poorly on another. Legal-specific or financial-domain models may outperform general-purpose ones for professional content, but this depends on training data and task alignment. The infrastructure cost is also higher than traditional search: you need embedding models, vector storage, and similarity search capabilities.

\textbf{Structured queries} retrieve from databases and APIs. An agent checking a company's SEC filings queries EDGAR; an agent researching a case retrieves from a court's electronic filing system. These are not ``search'' in the traditional sense; they are direct data access. However, they serve the same function in RAG: finding relevant information to inject into context.

\textbf{Hybrid approaches} combine methods. A common pattern uses BM25 for initial retrieval (fast, cheap, catches exact matches) followed by semantic reranking (slower, more expensive, captures meaning). Many production systems blend keyword and semantic scores, getting the best of both approaches.

\begin{keybox}[title={No Single Method Dominates}, breakable=false]
The right retrieval method depends on whether you need exact matching (keywords, BM25), conceptual similarity (embeddings), or structured data access (database queries). Most professional systems use hybrid approaches, combining methods to balance precision and cost.
\end{keybox}

\paragraph{Vector Stores for Embedding-Based Search}

Most professionals have conceptual familiarity with traditional keyword search from years of using Westlaw, Lexis, Bloomberg, or even web search engines. Embedding-based search operates differently and may be less intuitive: instead of matching words, the system matches meaning by comparing numerical vectors in high-dimensional space. Understanding this infrastructure helps when evaluating agentic systems that rely on semantic retrieval. Storing and searching millions of vectors efficiently requires specialized infrastructure.

\begin{definitionbox}[title={Vector Stores}, breakable=false]
\glsadd{vector-store}
	A \keyterm{vector store} (or vector database) is a system optimized for storing embeddings and performing similarity search. Given a query embedding, a vector store returns the most similar document embeddings from its collection. Vector stores use specialized indexing structures (like HNSW or IVF) that make approximate nearest-neighbor search practical at scale.

	\smallskip
	Vector stores are \textit{infrastructure}, the retrieval mechanism that powers semantic search. They are not the same as the retrieval pattern itself.
\end{definitionbox}

Organizations need not adopt entirely new infrastructure to use vector search. Traditional relational databases are rapidly adding vector capabilities; databases such as PostgreSQL now offer vector extensions, and most major cloud providers include native vector search in their managed database offerings. For firms with established database infrastructure, this convergence means semantic search can be added alongside existing systems rather than requiring a parallel technology stack. The trade-off is that purpose-built vector databases may offer better performance at scale, while integrated solutions offer simpler operations and unified data management.

\textbf{Knowledge graph retrieval} takes a different approach. Rather than matching meaning through vector similarity, \keyterm{knowledge graphs} represent entities and their relationships explicitly: corporate structures, ownership chains, party relationships, regulatory hierarchies. An agent researching a company can traverse the graph to find subsidiaries, board members, or regulatory filings, following connections that semantic similarity alone would miss. Graph databases (such as Neo4j) power this approach. For legal and financial applications where relationships matter (who owns whom, who advised whom, which entities share directors), graph retrieval complements vector search. Some systems combine both: vector search finds semantically relevant documents, then graph traversal enriches results with related entities and context.

\paragraph{Retrieval-Augmented Generation (RAG)}

With these retrieval methods in hand, we can now define RAG precisely.

\begin{definitionbox}[title={Retrieval-Augmented Generation (RAG)}, breakable=false]
\glsadd{rag}
	\keyterm{Retrieval-Augmented Generation (RAG)} is a \textit{prompt pattern}, not a software system or mathematical technique \parencite{lewis2020rag}. RAG augments a model's context with retrieved information before generation. The pattern has three steps:
	\begin{enumerate}[nosep]
		\item \textbf{Retrieve}: Given a query, find relevant content using \textit{any} retrieval method, including keyword search, BM25, embeddings, database queries, API calls, or combinations thereof.
		\item \textbf{Augment}: Insert the retrieved content into the model's prompt as additional context.
		\item \textbf{Generate}: The model produces a response informed by both its training and the retrieved content.
	\end{enumerate}
\end{definitionbox}

RAG works because of in-context learning. The retrieved content appears in the prompt, and the model adapts its response to incorporate that specific information. The retrieval step is completely method-agnostic: what matters is getting relevant content into context, not how you find it.

The distinction matters for system evaluation. A system's retrieval infrastructure (whether keyword index, vector store, database, external API, or some hybrid) determines its search capabilities and performance characteristics. The RAG pattern it implements (single-stage, multi-stage, or reranked) shapes how effectively it surfaces relevant content. These are separate architectural decisions with different tradeoffs, and understanding both helps you assess whether a system fits your needs.

\begin{keybox}[title={RAG Is Retrieval-Agnostic}, breakable=false]
A common misconception equates RAG with embeddings and vector stores. In practice, any retrieval method works: keyword search, BM25, database queries, or API calls. The ``embedding plus vector store'' pattern dominates tutorials because it handles semantic similarity well, but many production systems use simpler approaches that leverage existing search infrastructure.
\end{keybox}

% ----------------------------------------------------------------------------
% Model Context Protocol (MCP) for Perception
% ----------------------------------------------------------------------------

\subsection{Model Context Protocol (MCP)}
\label{sec:agents2-mcp-perception}

\glsadd{mcp}
One of the persistent challenges in building agentic systems is integration. Every database, document management system, and market data feed has its own interface. Historically, connecting an agent to a new information source meant writing custom code for that specific system. The Model Context Protocol (MCP) addresses this by standardizing how agents access external capabilities \parencite{anthropic-mcp,mcp-spec}.

\Cref{fig:agents2-mcp-architecture} illustrates the three-tier architecture. The \keyterm{MCP Host} is the agent application itself; it manages access control and coordinates connections, functioning like a firm's IT department deciding which systems a new employee can use. Each \keyterm{MCP Client} maintains a connection to one \keyterm{MCP Server}. Servers expose their capabilities through standardized primitives that clients discover and invoke via JSON-RPC.

\input{chapters/02-how-to-design/figures/fig-mcp-architecture}

Servers expose three types of primitives. \keyterm{Resources} provide read-only data access, including document repositories, market data feeds, and regulatory databases. The read-only designation matters: an agent with resource access can retrieve documents but cannot modify them. \keyterm{Tools} are executable functions that can change state by filing documents, sending messages, or executing queries. \keyterm{Prompts} are reusable templates encoding standard procedures, such as checklists, SOPs, and structured workflows that ensure consistency.

For perception, Resources are the primary primitive. An agent perceives external information by querying Resources exposed by MCP Servers. Separating Resources (read-only) from Tools (read-write) enables fine-grained access control that mirrors how organizations already manage permissions.

The integration landscape is shifting. Document management systems, e-discovery platforms, and financial data providers increasingly offer APIs. Some legal research providers are beginning to follow. As more systems expose MCP-compatible interfaces, the range of information sources available to agents expands.

\begin{keybox}[title={Standards Reduce Integration Costs}, breakable=false]
Without MCP, connecting 10 agents to 10 tools requires 100 custom integrations. With MCP, the same setup requires only 20 implementations; each agent and each tool learns the protocol once. Recent benchmarks found over ten thousand MCP servers in the ecosystem \parencite{livemcpbench-2025}.
\end{keybox}

\begin{cautionbox}[title={MCP Is Evolving Rapidly}]
The MCP specification has undergone significant revision since its initial release, and the ecosystem remains immature. Many third-party MCP servers do not fully implement the specification, particularly around Resources (often omitted in favor of Tools) and authentication (frequently absent or non-standard). Before deploying MCP integrations in production, verify that servers implement the primitives and security controls your use case requires. Expect breaking changes as the protocol matures.
\end{cautionbox}

% ----------------------------------------------------------------------------
% Memory as Perception
% ----------------------------------------------------------------------------

\subsection{Institutional Memory}
\label{sec:agents2-memory-perception}

Memory systems (\Cref{sec:agents2-memory}) serve as perception tools for institutional knowledge. The retrieval concepts introduced above (embeddings, vector stores, and RAG) enable agents to perceive accumulated expertise that would otherwise be inaccessible.

When an agent queries a precedent database using the RAG pattern (\Cref{sec:agents2-icl-retrieval}), it perceives institutional knowledge through in-context learning. The retrieved content appears in the agent's prompt, allowing it to reason about specific precedents, prior analyses, and established approaches. A search for ``breach of fiduciary duty'' retrieves documents about ``violation of trust obligations'' because their embeddings are similar, not because they share keywords.

This mechanism enables perception into knowledge bases far too large to fit in any model's context window. A law firm's precedent database might contain decades of work product; a financial institution's research archive might span thousands of analyst reports. No agent can hold all of this in active context. RAG allows selective retrieval: the agent perceives only the most relevant fragments, guided by semantic similarity to the current task.

Institutional memory provides access to prior work product. When drafting a new registration statement, an agent can perceive prior S-1 filings (IPO registrations), SEC comment histories, and successful disclosure language. This access allows current work to build on verified precedents rather than starting from first principles.

Memory-as-perception distinguishes experienced agents from novices. A junior associate reasons from what they learned in law school; a senior associate draws on pattern recognition from hundreds of matters. Memory provides agents with this accumulated experience, but only if the retrieval infrastructure connects them to the right knowledge at the right time. \Cref{sec:agents2-memory} develops these requirements in detail, including how memory systems must enforce the authority, temporal validity, and isolation constraints introduced above.

% ----------------------------------------------------------------------------
% Domain-Specific Perception Requirements
% ----------------------------------------------------------------------------

\subsection{Domain Requirements}
\label{sec:agents2-perception-domain}

Perception for regulated professional services requires specialized enhancements. General-purpose search is insufficient; professional agents require authority tracking, jurisdictional awareness, and confidentiality boundaries.

\paragraph{Authority and Verification}

Information varies in authority. Perception systems must track provenance to ensure reliability. Authority weighting ranks primary sources (statutes, binding precedent) above secondary sources (law reviews, news). When searching for ``insider trading liability,'' a Supreme Court opinion outranks a commentary article. Source verification confirms that retrieved information originates from the claimed source. Perception tools must return verifiable citations, not just text. Currency validation ensures the authority remains valid. Integrated citators (like Shepard's or KeyCite) verify that retrieved cases have not been overruled.

\paragraph{Jurisdiction and Temporal Scope}

Legal and regulatory information is bounded by jurisdiction. California precedent does not bind Texas courts; SEC rules differ from CFTC rules. Perception tools must filter results by relevant jurisdiction. Temporal validity is equally critical. Laws change, and financial data expires. Perception systems must track effective dates. In finance, validity varies by context: milliseconds for trading prices, quarters for compliance reporting. Identifier resolution manages the proliferation of formats. ``123 F.3d 456'' and ``123 F3d 456'' refer to the same case. Financial identifiers include tickers, CUSIPs, and LEIs (Legal Entity Identifiers). Perception must normalize these to ensure consistent retrieval.

\paragraph{Matter and Client Isolation}

Critically, perception must respect confidentiality boundaries. Whether a human or AI, an agent working on Matter A cannot perceive documents from adverse Matter B. This enforcement of \keyterm{ethical walls} arguably must occur at the perception layer. In financial contexts, an agent advising Client X cannot perceive material non-public information (MNPI) from Client Y's engagement. Every perception event must be logged, capturing the agent, the query, and the matter context. This audit trail enables compliance review and breach detection. See \Cref{sec:agents2-memory} for detailed treatment of isolation requirements; Chapter~3 addresses the professional responsibility obligations, including attorney confidentiality duties and fiduciary obligations, that mandate these controls.

% ----------------------------------------------------------------------------
% Tool Design Principles for Perception
% ----------------------------------------------------------------------------

\subsection{Tool Design Principles}
\label{sec:agents2-perception-design}

Robust perception tools follow design principles that enable reliable operation in professional environments.

\paragraph{Single Responsibility}

Each tool should perform one function well. Poorly designed tools bundle multiple functions (searching, formatting, and validation) into opaque interfaces. Untyped return values obscure what callers can expect.

\begin{listingbox}[title={Poor Design: Bundled Functions, Untyped Returns}, listing options={language=Python}]
def legal_research(query: str) -> dict:
  """Returns... something. Good luck."""
  ...
\end{listingbox}

When such a tool fails, diagnosing the error is difficult. A better approach separates tools by function with typed returns. This allows the agent to compose them and isolates failures.

\begin{listingbox}[title={Better Design: Single Responsibility, Typed Returns}, listing options={language=Python}, breakable=false]
def search_cases(query: str, jurisdiction: str) -> list[Citation]:
  """Returns matching citations from case law database."""

def retrieve_case(citation: Citation) -> CaseText:
  """Fetches full text for a specific citation."""

def shepardize(citation: Citation) -> CitatorResult:
  """Checks validity: good law, distinguished, overruled."""

def format_citation(case: CaseText, style: str) -> str:
  """Converts to Bluebook, ALWD, or other format."""
\end{listingbox}

\paragraph{Graceful Failure}

Production systems inevitably fail. Tools should return informative errors rather than generic exceptions. A poor approach raises exceptions that provide no context.

\begin{listingbox}[title={Poor: Opaque Exception}, listing options={language=Python}]
def retrieve_case(citation: str) -> dict:
  result = db.query(citation)
  return result["text"]  # raises KeyError if not found
\end{listingbox}

A better approach uses typed result objects that make success and failure explicit.

\begin{listingbox}[title={Better: Typed Result with Structured Errors}, listing options={language=Python}]
class CaseNotFoundError(BaseModel):
  citation: str
  reason: str
  suggestions: list[str]

def retrieve_case(citation: Citation) -> CaseText | CaseNotFoundError:
  """Returns case text or structured error with recovery options."""
  if not (result := db.query(citation)):
    return CaseNotFoundError(
      citation=str(citation),
      reason="Case may not be in database",
      suggestions=["Check citation format", "Try alternative reporter"]
    )
  return CaseText(...)
\end{listingbox}

In professional practice, graceful failure prevents malpractice. When an agent cannot find authority, it must report that explicitly rather than proceeding silently.

\paragraph{Least Privilege and Rate Limiting}

Perception tools should request minimum necessary permissions. A legal research tool requires read access to case databases, not write access to the document management system. If a compromised agent gains perception credentials, damage is limited to information disclosure rather than destruction. Rate limiting addresses a common failure mode: infinite search loops. Tools should track invocation frequency and refuse requests beyond reasonable thresholds. If an agent searches five times without results, the tool should force a stop and escalation (\Cref{sec:agents2-escalation}).

% ----------------------------------------------------------------------------
% Evaluating Perception Capabilities
% ----------------------------------------------------------------------------

\subsection{Evaluation}
\label{sec:agents2-perception-eval}

When evaluating agentic systems, you should assess perception against criteria that matter for professional practice.

\textbf{Coverage} determines which sources the agent can access. A litigation agent that queries Westlaw but not state-specific databases has incomplete coverage. You must map available perception tools against information needs to identify gaps.

\textbf{Retrieval quality} measures whether the agent finds relevant information. Test with known-good queries where the correct result is established. Measure both \keyterm{precision} (the fraction of retrieved documents that are actually relevant) and \keyterm{recall} (the fraction of all relevant documents that the system successfully retrieves). These metrics will be familiar to legal professionals from \keyterm{technology-assisted review} (TAR), the use of machine learning to identify relevant documents during e-discovery, where courts have recognized precision and recall as the standard measures of retrieval effectiveness \parencite{grossman2011tar}. The same framework applies to agent perception: high precision means the agent does not waste time on irrelevant results; high recall means the agent does not miss important authorities. The tradeoff between them (casting a wider net improves recall but may reduce precision) is a design decision that should be calibrated to the task's risk profile.

\textbf{Verification} confirms that the system distinguishes authoritative from secondary sources. You must ensure that retrieved information is traceable to its source and that citations are independently verifiable.

\textbf{Access controls} ensure that permissions are appropriate. The agent must access only what it should, and confidentiality boundaries must hold across matter and client lines.

\textbf{Failure handling} reveals system behavior when perception fails. Does it retry, try alternatives, or escalate? It must not crash or proceed with incomplete information.

\textbf{Audit capability} confirms that every perception event is logged. You must be able to reconstruct what information the agent accessed during a task for compliance review.

\begin{practicebox}[title={\textbf{Perception Capability Checklist}},breakable=false]
\begin{itemize}[leftmargin=1.5em,nosep]
\item[$\square$] \textbf{Coverage:} Map available perception tools against information needs and identify gaps in accessible sources
\item[$\square$] \textbf{Retrieval quality:} Test with known-good queries and measure both precision (relevance) and recall (completeness)
\item[$\square$] \textbf{Source verification:} Confirm citations are traceable, authoritative sources are distinguished from secondary ones
\item[$\square$] \textbf{Access controls:} Verify matter/client isolation holds and permissions are scoped to minimum necessary access
\item[$\square$] \textbf{Failure handling and audit:} Test failure scenarios (retry, escalation) and confirm all perception events are logged
\end{itemize}
\end{practicebox}

% ----------------------------------------------------------------------------
% Connection to Other Questions
% ----------------------------------------------------------------------------

Perception enables agents to gather information, but professional value ultimately requires effecting change: filing documents, sending communications, executing trades. This distinction between observing the world and changing it is fundamental to agent architecture, and the protocols we use reflect it. MCP, introduced earlier in this section, explicitly separates \textbf{Resources} (read-only data access) from \textbf{Tools} (operations that modify state). A single MCP server might expose both capabilities. For example, a document management system could offer read access to files alongside the ability to create, modify, or delete them. However, the protocol distinguishes these so that access control and governance can treat them differently.

The distinction matters because the consequences of failure differ fundamentally. When perception tools fail (such as when a search returns wrong results or a document fails to load), the external world remains unchanged. The agent can retry, try alternatives, or escalate to human review without having caused any harm beyond wasted time. Action tools carry different stakes entirely. They file documents that become part of court records, send emails that reach recipient inboxes, execute trades that transfer ownership at market prices. Once executed, many actions cannot be undone, or can only be undone at significant cost. The agent's mistakes become facts in the world.

\Cref{sec:agents2-action} examines action capabilities in detail, beginning with the conceptual foundations that distinguish actions from observations and developing the governance frameworks that these differences require.
