% =============================================================================
% Meta-Prompting â€” Prompt Design, Evaluation, Optimization
% Purpose: Rubrics, self-critique, declarative/programmatic assembly
% Label: sec:llmD-meta
% =============================================================================

\section{Meta-Prompting and Self-Critique}
\label{sec:llmD-meta}

Meta-prompting uses language models to improve, validate, or generate prompts themselves. Rather than manually crafting every prompt variant, we leverage the model's capabilities to systematize prompt development, implement quality gates, and bootstrap training data. This section covers three core patterns: declarative prompt programming, self-reflection loops, and synthetic data generation.

\begin{highlightbox}[title={Rubric Before Solve}]
Define the grading rubric first (criteria and weights), then ask the model to produce and self-evaluate against the rubric before finalizing output. This pattern separates quality criteria from content generation, enabling systematic improvement.
\end{highlightbox}

\subsection{Declarative and Programmatic Prompting}
\label{sec:llmD-meta-declarative}

Traditional prompting treats prompts as monolithic text strings. Declarative prompting represents prompts as structured data---specifications, schemas, and policies---that can be composed, versioned, and tested programmatically.

\paragraph{Prompts as Data Structures.}
Instead of embedding instructions in prose, declarative prompts separate concerns:

\begin{itemize}
  \item \textbf{Task specification}: What the model should accomplish, expressed as a schema or contract.
  \item \textbf{Constraints}: Boundaries on output format, content restrictions, and safety policies.
  \item \textbf{Context assembly}: Rules for which documents, examples, or metadata to include.
  \item \textbf{Quality criteria}: The rubric against which outputs will be evaluated.
\end{itemize}

This separation enables automated assembly: a prompt compiler reads specifications and generates the final prompt text, inserting appropriate examples, formatting constraints, and context based on the current request.

\paragraph{Composition and Reuse.}
Declarative prompts support composition. A base prompt for ``legal document analysis'' can be extended with task-specific overlays for ``contract review,'' ``regulatory filing,'' or ``litigation support'' without duplicating shared instructions. Changes to the base propagate automatically.

\paragraph{Version Control and Diffing.}
Structured prompts enable meaningful diffs. When a specification changes, teams can see exactly which constraints, examples, or policies were modified---unlike prose prompts where changes are difficult to track systematically.

\paragraph{Programmatic Optimization.}
With prompts as data, optimization becomes tractable. Automated systems can vary parameters (temperature, example selection, instruction phrasing), measure outcomes against the quality rubric, and converge on improved configurations without manual iteration.

\begin{keybox}[title={Declarative Prompt Benefits}]
\begin{itemize}
  \item \textbf{Testability}: Specifications can be validated before deployment.
  \item \textbf{Composability}: Shared components reduce duplication and drift.
  \item \textbf{Auditability}: Structured changes enable governance review.
  \item \textbf{Optimization}: Parameterized prompts enable automated tuning.
\end{itemize}
\end{keybox}

\subsection{Self-Reflection and Error-Driven Repair}
\label{sec:llmD-meta-reflection}

Self-reflection patterns use the model to critique and improve its own outputs. Rather than accepting the first response, we implement iterative refinement loops that catch errors, fill gaps, and improve quality.

\paragraph{The Critique-Fix Loop.}
The basic pattern involves three steps:

\begin{enumerate}
  \item \textbf{Generate}: Produce an initial response to the task.
  \item \textbf{Critique}: Ask the model to evaluate the response against explicit criteria (completeness, accuracy, format compliance, citation presence).
  \item \textbf{Revise}: If the critique identifies issues, generate a revised response incorporating the feedback.
\end{enumerate}

This loop can iterate until the critique passes or a maximum iteration count is reached.

\paragraph{Explicit Error Messages.}
Effective critique requires specific feedback. Rather than ``this response is incomplete,'' the critique should identify exactly what is missing: ``The response does not address the third contract clause. The citation for the statutory reference is missing.'' Specific feedback enables targeted revision.

\paragraph{Schema Validation as Critique.}
For structured outputs, schema validation provides automatic critique. If the model's JSON output fails validation, the error message (``missing required field `jurisdiction`'') becomes the feedback for the next iteration. This pattern is particularly effective because validation errors are unambiguous.

\paragraph{Iteration Limits and Fallbacks.}
Unbounded loops risk infinite iteration or degrading quality. Best practices include:

\begin{itemize}
  \item \textbf{Maximum iterations}: Typically 2--4 rounds; diminishing returns beyond.
  \item \textbf{Fallback on failure}: If iterations exhaust without passing critique, route to human review rather than emitting low-quality output.
  \item \textbf{Delta logging}: Record what changed between iterations for debugging and audit.
\end{itemize}

\paragraph{Multi-Perspective Critique.}
Advanced patterns use multiple critique perspectives. One pass checks factual accuracy, another checks format compliance, a third checks policy adherence. Separating concerns improves critique quality and enables parallel evaluation.

\begin{cautionbox}[title={Self-Reflection Limitations}]
Models may not reliably catch their own errors, especially for factual accuracy. Self-reflection improves format compliance and completeness but should not replace external validation for high-stakes factual claims. Ground truth verification (retrieval, calculation tools) remains essential.
\end{cautionbox}

\subsection{Bootstrapping Examples and Synthetic Data}
\label{sec:llmD-meta-bootstrap}

High-quality few-shot examples are scarce and expensive to create manually. Meta-prompting can bootstrap candidate examples, though with important guardrails to ensure quality and prevent data contamination.

\paragraph{Exemplar Generation Pipeline.}
The typical pipeline includes:

\begin{enumerate}
  \item \textbf{Seed specification}: Define the task, input format, and desired output characteristics.
  \item \textbf{Candidate generation}: Use the model to generate input-output pairs matching the specification.
  \item \textbf{Rubric filtering}: Automatically score candidates against quality criteria; reject low-scoring pairs.
  \item \textbf{Human review}: Subject matter experts validate surviving candidates, correcting errors and rejecting borderline cases.
  \item \textbf{Provenance tracking}: Record that examples are synthetic, their generation date, and the model version used.
\end{enumerate}

\paragraph{Diversity and Coverage.}
Generated examples often cluster around common patterns. Explicit diversity strategies include:

\begin{itemize}
  \item \textbf{Stratified generation}: Request examples for each category, edge case, or difficulty level.
  \item \textbf{Temperature variation}: Higher temperatures produce more varied candidates.
  \item \textbf{Negative examples}: Explicitly generate incorrect outputs to teach the model what to avoid.
\end{itemize}

\paragraph{Contamination and Leakage Risks.}
Synthetic data creates evaluation risks:

\begin{itemize}
  \item \textbf{Train-test leakage}: If generated examples inform both prompts and evaluation sets, metrics are inflated.
  \item \textbf{Model circularity}: Examples generated by the same model may reinforce its biases rather than correct them.
  \item \textbf{License ambiguity}: Synthetic data derived from copyrighted inputs may carry licensing constraints.
\end{itemize}

Mitigation requires strict separation: generation models should differ from evaluation models, and synthetic data should be clearly marked in all downstream uses.

\paragraph{Provenance and Urldate Tracking.}
For regulatory compliance, synthetic data requires provenance metadata:

\begin{itemize}
  \item Generation date and model version
  \item Seed data sources and their licenses
  \item Human review status and reviewer identity
  \item Intended use restrictions
\end{itemize}

This metadata enables audit and supports data governance requirements.

\begin{keybox}[title={Synthetic Data Best Practices}]
\begin{itemize}
  \item Always filter generated candidates through quality rubrics.
  \item Require human review for high-stakes applications.
  \item Maintain strict separation between generation and evaluation data.
  \item Track provenance, licenses, and intended use restrictions.
\end{itemize}
\end{keybox}

