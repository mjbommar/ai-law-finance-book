% =============================================================================
% Conversational Models and State â€” Conversations & Reasoning
% Purpose: Roles, memory/state, chat mechanics, guardrails
% Label: sec:llmB-convo
% =============================================================================

\section{Conversational Models and State}
\label{sec:llmB-convo}

The distinction between a text-completion engine and a conversational agent lies entirely in the management of state. Since the underlying model weights are frozen and stateless, ``memory'' is purely a function of the input context. To maintain a conversation, the orchestration layer must recursively re-inject the history of the interaction into the model's context window with each new turn. This process, often termed \keyterm{context rehydration}, is the fundamental mechanic of conversational AI.

However, treating the context window as an infinite append-only log is computationally unsustainable and algorithmically flawed. Context windows have fixed limits---ranging from 4,096 tokens in older models to 128,000+ in newer iterations---and the attention mechanism itself has limitations that affect how well the model can access different parts of a long context.

In this section, we examine the architectural components of conversational state: the role-based prompt structure that defines who is speaking and what constraints apply, the memory strategies that maintain coherence across turns, and the safety mechanisms that bound model behavior.

\subsection{Role-Based Prompt Engineering}
\label{sec:llmB-roles}

Modern conversational architectures enforce a strict separation of concerns through \keyterm{role-based prompting}. This is not merely a formatting convention but a control mechanism that delineates the model's operational constraints from user inputs. The architecture recognizes three primary roles: the \textbf{System}, the \textbf{User}, and the \textbf{Assistant}.

\subsubsection{The System Role: The Immutable Constitution}
\label{sec:llmB-system-role}

The \keyterm{system prompt} (often designated with special control tokens in the model's vocabulary) serves as the immutable ``constitution'' of the agent. It establishes the behavioral baseline, tone, output format, and, crucially, the safety boundaries of the interaction \parencite{zheng2023personas}.

Unlike user prompts, which are dynamic, untrusted, and variable, the system prompt is intended to be static and privileged. In architectures like Llama 2, the input is structured to wrap user messages in specific tags while separating the system instruction, ensuring the attention heads distinguish between instructions to be followed and text to be processed \parencite{touvron2023llama2}. This structural separation is vital for defense against ``jailbreaking'' or ``prompt injection'' attacks, where a user attempts to override the model's safety protocols by mimicking authoritative instructions.

\paragraph{Signpost to Chapter~1.} Prompt injection is not only a conversational concern but also a core LLM failure mode with security implications. Chapter~1 provides a detailed taxonomy of prompt injection and defense-in-depth mitigations.

\begin{keybox}[title={Effective System Prompt Design}]
An effective system prompt should include:
\begin{enumerate}
  \item \textbf{Identity and role}: Define who the assistant is and its primary purpose
  \item \textbf{Behavioral constraints}: Specify what the assistant should and should not do
  \item \textbf{Output format}: Establish expected structure for responses (when applicable)
  \item \textbf{Knowledge boundaries}: Define what the assistant knows and when to acknowledge uncertainty
  \item \textbf{Escalation criteria}: Specify when to refuse, ask for clarification, or defer to human judgment
\end{enumerate}
\end{keybox}

\paragraph{Example: Legal Research Assistant.} Consider a system prompt for a legal research assistant:

\begin{quote}
\texttt{You are a legal research assistant specializing in U.S. federal law. You help attorneys find relevant case law, statutes, and regulations. You always cite specific sources with proper legal citations. You acknowledge when a question falls outside your knowledge or when case law may have evolved since your training. You never provide legal advice or make predictions about case outcomes. When asked about jurisdiction-specific rules, you note that local practice may vary and recommend verifying with current local sources.}
\end{quote}

This prompt establishes identity (legal research assistant), scope (U.S. federal law), expected behavior (cite sources, acknowledge limitations), prohibitions (no legal advice or predictions), and appropriate hedging (verify with local sources).

\paragraph{Instruction Drift and Mitigation.} However, the efficacy of the system prompt is not absolute. A phenomenon known as \keyterm{instruction drift} or \keyterm{context dilution} can occur as the conversation lengthens. As the distance between the initial system prompt and the current generation increases, the model's adherence to the initial constraints can degrade, primarily due to the ``recency bias'' inherent in the attention mechanism \parencite{liu2024lostmiddle}.

To combat this, advanced prompting strategies involve:
\begin{itemize}
  \item \textbf{Repeating critical constraints}: Restate key rules near the end of the prompt, immediately before the generation trigger
  \item \textbf{Periodic reinforcement}: Insert reminder statements in long conversations (e.g., ``Remember, you should not provide legal advice'')
  \item \textbf{Sentinel phrases}: Use specific phrases that trigger compliance checks (though this requires custom orchestration logic)
\end{itemize}

\subsubsection{The User and Assistant Roles: Managing the Dialogue Loop}

The \textbf{user role} represents external input---the queries, commands, and information provided by the human interacting with the system. The \textbf{assistant role} captures the model's generated outputs. To maintain state, the application must append each user query and assistant response to a growing list of messages. This list is serialized into a single prompt string (or a structured token sequence) at every turn.

\begin{definitionbox}[title={The Dialogue Loop}]
A typical multi-turn conversation follows this pattern:
\begin{enumerate}
  \item Receive user input
  \item Construct prompt: \texttt{[System] + [History] + [User Input]}
  \item Call model, generate response
  \item Append \texttt{[User Input, Assistant Response]} to history
  \item Return response to user
  \item Repeat from step 1
\end{enumerate}
\end{definitionbox}

The integrity of this dialogue loop is maintained through strict formatting. For instance, Llama 2 uses a specific syntax where the system prompt and user prompt are enclosed in special tags, but the model's response is left outside, creating a clear demarcation in the token stream. This formatting guides the model's stop-token prediction; without it, the model might hallucinate the user's next response rather than stopping after its own turn.

\paragraph{Practical Implications.} When constructing prompts:
\begin{itemize}
  \item \textbf{Be explicit}: Don't assume the model remembers what ``it'' refers to from five turns back unless you've managed context accordingly
  \item \textbf{Reference context}: If a question relates to something said much earlier, remind the model: ``Based on our discussion about X earlier, ...''
  \item \textbf{Use retrieval}: For long conversations, retrieve and inject the exact relevant snippet rather than relying on raw history
\end{itemize}

\subsection{Memory Strategies: Maintaining Context in Dialogues}
\label{sec:llmB-memory}

One of the biggest challenges in multi-turn conversations is how the model ``remembers'' earlier parts of the dialogue. Given the finite nature of the context window, managing conversation history is fundamentally a \emph{resource allocation problem}. As the conversation exceeds the window size, the system must make active decisions about what to retain and what to discard.

\subsubsection{The ``Lost in the Middle'' Phenomenon}

A critical insight in context management is that LLMs do not access all parts of their context window with equal acuity. Extensive empirical analysis reveals a distinct U-shaped performance curve regarding information retrieval \parencite{liu2024lostmiddle}. Models excel at retrieving information located at the very beginning (primacy effect) and the very end (recency effect) of the input context. Information buried in the middle of a long sequence is significantly more likely to be ignored, hallucinated, or incorrectly associated.

\begin{keybox}[title={The U-Shaped Retrieval Curve}]
When retrieving facts from context:
\begin{itemize}
  \item \textbf{Beginning of context}: High retrieval accuracy (primacy effect)
  \item \textbf{Middle of context}: Significantly degraded accuracy
  \item \textbf{End of context}: High retrieval accuracy (recency effect)
\end{itemize}
This phenomenon is likely an emergent property of training data structure, where introductions and conclusions contain concentrated semantic signal.
\end{keybox}

\Cref{fig:llmB-lost-middle} illustrates this U-shaped retrieval curve, showing how information placement affects model attention.

\begin{figure}[htbp]
  \centering
  \resizebox{0.95\textwidth}{!}{\input{chapters/02-conversations-reasoning/figures/fig-lost-middle}}
  \caption{The ``Lost in the Middle'' phenomenon: LLMs exhibit a U-shaped retrieval curve, with high accuracy at the beginning (primacy) and end (recency) of context, but degraded performance in the middle. This has profound implications for prompt construction.}
  \label{fig:llmB-lost-middle}
\end{figure}

The implications for conversation design are profound. Simply filling the context window with relevant documents or history does not guarantee retrieval. To mitigate this:
\begin{itemize}
  \item \textbf{Prompt re-ranking}: Relocate the most critical information to the beginning or immediate end of the prompt
  \item \textbf{Chunking with overlap}: Break long content into chunks with semantic boundaries, ensuring critical information appears at chunk boundaries
  \item \textbf{Explicit signaling}: Use formatting (headers, bullet points) to make critical information more salient
\end{itemize}

\subsubsection{Sliding Windows}

The simplest memory management strategy retains only the most recent $N$ tokens or turns. This \keyterm{sliding window} approach:

\begin{itemize}
  \item \textbf{Advantages}: Computationally simple, ensures high recency, predictable token usage
  \item \textbf{Disadvantages}: Induces ``catastrophic forgetting'' of earlier context; critical constraints or facts established early are lost once they slide out of the window
\end{itemize}

\paragraph{When to Use.} Sliding windows are appropriate when:
\begin{itemize}
  \item The conversation is inherently short-term (e.g., simple Q\&A)
  \item Recent context is far more relevant than historical context
  \item You have other mechanisms (like system prompts) to maintain critical constraints
\end{itemize}

\subsubsection{Recursive Summarization}

A more sophisticated approach employs the LLM itself to periodically compress older turns into a concise narrative summary \parencite{wang2024recursivesummarizing}. For example, after 10 turns, the model might summarize the dialogue as ``User asked about Python lists; Assistant explained syntax and provided examples.'' This summary is then prepended to the active context, replacing the raw tokens.

\begin{itemize}
  \item \textbf{Advantages}: Preserves the semantic ``gist'' of the conversation, frees up token space for new content
  \item \textbf{Disadvantages}: Lossy compression; specific details (e.g., a phone number, a specific code snippet, exact client instructions) may be smoothed over
\end{itemize}

\paragraph{Implementation Considerations.}
\begin{itemize}
  \item \textbf{Summarization frequency}: Every $N$ turns, or when the history exceeds a token threshold
  \item \textbf{Preservation of critical facts}: Maintain a separate ``pinned facts'' section for information that must never be summarized away
  \item \textbf{Hierarchical summarization}: For very long conversations, summarize summaries recursively
\end{itemize}

\begin{highlightbox}[title={Legal Application: Client Interview Memory}]
In a legal intake scenario, you might configure memory as follows:
\begin{itemize}
  \item \textbf{Pinned facts}: Client name, case type, jurisdiction, key dates (never summarized)
  \item \textbf{Active window}: Last 5 turns (full detail)
  \item \textbf{Summarized history}: Earlier turns compressed to key facts and decisions
\end{itemize}
This ensures the attorney always sees critical identifying information while maintaining context efficiency.
\end{highlightbox}

\subsubsection{Vector-Enhanced Memory (Long-Term Retrieval)}

For indefinite memory, systems employ external \keyterm{vector stores}. Conversation turns are embedded into vectors and stored in a database (using indices like HNSW or FAISS). When a new user query arrives, the system retrieves semantically relevant past interactions---regardless of temporal distance---and injects them into the current context \parencite{zhang2024memory}.

\paragraph{Signpost to Chapter~1.} Chapter~1 provides the primary treatment of embeddings, similarity metrics, and hybrid retrieval. This section focuses on how retrieval is used to implement conversational memory.

\begin{itemize}
  \item \textbf{Advantages}: Approximates human episodic memory; can retrieve relevant context from arbitrarily long histories; enables cross-session memory
  \item \textbf{Disadvantages}: Introduces latency; requires embedding model and vector database infrastructure; can surface conflicting memories if user preferences changed
\end{itemize}

\paragraph{Hybrid Approaches.} In practice, the most robust systems combine strategies:
\begin{enumerate}
  \item \textbf{System prompt}: Immutable constraints and identity
  \item \textbf{Pinned facts}: Critical session-specific information that must persist
  \item \textbf{Retrieved context}: Semantically relevant prior exchanges or documents
  \item \textbf{Summarized history}: Compressed earlier conversation
  \item \textbf{Active window}: Recent turns in full detail
  \item \textbf{Current query}: The user's latest input
\end{enumerate}

This layered approach maximizes the utility of the limited context window while maintaining coherence.

\begin{highlightbox}[title={Bridge to Agentic Systems}]
These memory patterns become architectural decisions in agentic systems. Our companion volume, \textit{Agentic AI in Law and Finance}, examines Memory as one of the core design questions for agents---addressing how agents maintain state across iterations and sessions.
\end{highlightbox}

\subsection{Context Management and Instruction Placement}
\label{sec:llmB-context-mgmt}

Given the ``Lost in the Middle'' phenomenon, where you place information in the prompt matters as much as what information you include.

\subsubsection{Token Budgeting}

Before constructing any prompt, establish a token budget:

\begin{enumerate}
  \item \textbf{Determine total available tokens}: Model context limit (e.g., 128K)
  \item \textbf{Reserve for output}: Allocate space for the expected response (e.g., 2K--4K tokens for detailed answers)
  \item \textbf{Allocate to components}:
  \begin{itemize}
    \item System prompt: 500--1,000 tokens
    \item Few-shot examples (if used): 1,000--3,000 tokens
    \item Retrieved context (RAG): Variable, often 2,000--10,000 tokens
    \item Conversation history: Remainder
    \item Current query: Usually small
    \item Final instructions/reminders: 100--200 tokens
  \end{itemize}
\end{enumerate}

\subsubsection{Optimal Instruction Placement}

Because LLMs weight recent tokens more heavily, the \emph{end} of the prompt (just before generation begins) is prime real estate. Place critical instructions there:

\begin{keybox}[title={The ``Sandwich'' Pattern}]
Structure your prompt as:
\begin{enumerate}
  \item \textbf{Beginning}: System prompt with core identity and constraints
  \item \textbf{Middle}: Context, history, retrieved documents
  \item \textbf{End}: Current query + \emph{reinforcement of critical constraints}
\end{enumerate}
The final reinforcement ensures that key rules (output format, prohibited actions, required disclaimers) are fresh in the model's ``attention'' when it begins generating.
\end{keybox}

\Cref{fig:llmB-context-assembly} illustrates the recommended ordering for prompt construction, showing how each layer builds on the previous.

\begin{figure}[htbp]
  \centering
  \resizebox{0.85\textwidth}{!}{\input{chapters/02-conversations-reasoning/figures/fig-context-assembly}}
  \caption{Context assembly order for multi-turn conversations. Place the system prompt and critical constraints at the beginning (primacy zone), context and history in the middle, and the current query with reminders at the end (recency zone) for maximum retention.}
  \label{fig:llmB-context-assembly}
\end{figure}

\paragraph{Example: Financial Compliance.} Consider a financial advisory system where legal disclaimers are mandatory:

\begin{quote}
\texttt{[System: You are a financial information assistant...]}

\texttt{[History of conversation...]}

\texttt{[User: What stocks should I buy?]}

\texttt{[Reminder: You must not provide personalized investment advice. If asked, explain that you provide educational information only and recommend consulting a licensed financial advisor. Always include a disclaimer.]}
\end{quote}

The final reminder, placed immediately before generation, increases compliance probability.

\subsection{Safety, Guardrails, and Constitutional AI}
\label{sec:llmB-safety}

As conversational agents become more capable and autonomous, the risk of harmful behaviors increases. Systems may generate toxic content, facilitate harmful activities, or reveal sensitive information. Governance controls must be baked into the conversational loop, distinct from the model's raw capabilities.

\subsubsection{Guardrails: Classification-Based Defense}

\keyterm{Guardrails} are typically implemented as separate, lightweight classification models that scan both inputs and outputs \parencite{dong2024guardrails, inan2023llamaguard}. These guardrail models effectively ``wrap'' the main LLM:

\begin{enumerate}
  \item \textbf{Input check}: Before the user input reaches the LLM, the guardrail checks for malicious intent, prohibited requests, or policy violations
  \item \textbf{Generation}: If input passes, the main LLM generates a response
  \item \textbf{Output check}: Before returning to the user, the guardrail checks the response for policy violations
  \item \textbf{Intervention}: If a violation is detected at either stage, the guardrail intercepts the message and replaces it with a standard refusal or safe response
\end{enumerate}

\begin{highlightbox}[title={Defense in Depth}]
Robust systems implement multiple layers of protection:
\begin{itemize}
  \item \textbf{System prompt}: First line of defense, establishing behavioral boundaries
  \item \textbf{Input guardrails}: Catch malicious inputs before they reach the model
  \item \textbf{Output guardrails}: Catch harmful generations before they reach the user
  \item \textbf{Audit logging}: Record all interactions for review and incident response
  \item \textbf{Human escalation}: Mechanism to route sensitive queries to human reviewers
\end{itemize}
\end{highlightbox}

\begin{highlightbox}[title={From Guardrails to Governance}]
Guardrails evolve into governance surfaces in agentic systems. Our companion volume, \textit{Agentic AI in Law and Finance}, addresses how to design agents that can be audited, overridden, and controlled---extending these safety concepts to autonomous operation.
\end{highlightbox}

\subsubsection{Constitutional AI}

A more advanced and theoretically robust approach is \keyterm{Constitutional AI} (CAI), pioneered by Anthropic \parencite{bai2022constitutional}. Instead of relying solely on Reinforcement Learning from Human Feedback (RLHF)---which is hard to scale, expensive, and can encode implicit human biases---CAI uses a ``constitution'' of explicit principles (e.g., ``Please choose the response that is most helpful, honest, and harmless'').

The CAI process involves a self-supervised mechanism:

\begin{enumerate}
  \item \textbf{Generation}: The model generates responses to prompts, including potentially harmful ones
  \item \textbf{Critique}: The model critiques its own response based on the constitution (e.g., ``Did this response encourage violence?'')
  \item \textbf{Revision}: The model revises the response to be compliant
  \item \textbf{Training}: The model is fine-tuned on these revised, safe traces
\end{enumerate}

This method creates a conversational agent that is ``aligned'' via explicit rules rather than implicit human preferences, making the model's refusal behavior more explainable, consistent, and robust against adversarial attacks.

\subsubsection{Practical Safety Prompts}

For practitioners without access to fine-tuning, safety must be implemented through prompt design:

\begin{itemize}
  \item \textbf{Explicit prohibitions}: ``Do not provide medical diagnoses, legal advice, or recommendations to harm.''
  \item \textbf{Uncertainty acknowledgment}: ``When uncertain, acknowledge limitations and recommend consulting an expert.''
  \item \textbf{Refusal patterns}: ``If asked to do X, respond with Y.''
  \item \textbf{Clarification triggers}: ``If the request is ambiguous or potentially harmful, ask for clarification rather than assuming intent.''
\end{itemize}

\paragraph{Safety and Reasoning.} Note that revealing the model's reasoning (if you use techniques like chain-of-thought, discussed in \Cref{sec:llmB-reason}) can sometimes conflict with safety. Often we keep detailed reasoning hidden precisely so the model can deliberate freely---even consider and reject a potentially harmful action---without ever exposing a harmful thought to the user. For example, an LLM might internally reason ``The user is asking how to do something dangerous---I should refuse,'' but you wouldn't want it to reply with that reasoning visible. You just want it to refuse. Thus, part of safety is deciding which parts of the model's process to keep private.

\subsection{Implementation Patterns for Professional Applications}

Before synthesizing the complete architecture, it is worth examining specific implementation patterns that arise in legal and financial contexts. These domains present unique challenges that generic chatbot architectures do not address.

\subsubsection{Legal Conversation Patterns}

Legal applications require careful attention to several domain-specific constraints:

\paragraph{Privilege Protection.} Attorney-client privilege considerations must be embedded in the system architecture. The conversational system must never expose privileged information to unauthorized parties, even in error messages or debugging logs. This requires:

\begin{itemize}
  \item Separate storage for privileged and non-privileged conversation content
  \item Access controls that verify privilege status before retrieving historical context
  \item Audit trails that distinguish between privileged and non-privileged interactions
  \item Memory strategies that respect privilege boundaries when summarizing conversations
\end{itemize}

\paragraph{Jurisdictional Awareness.} Legal analysis varies significantly by jurisdiction. A conversational legal assistant must maintain awareness of:

\begin{itemize}
  \item The governing jurisdiction for the matter under discussion
  \item When the user shifts jurisdictional context (e.g., ``What about under California law?'')
  \item When an answer depends on jurisdictional variation and should note alternatives
  \item Conflict of laws considerations when multiple jurisdictions are relevant
\end{itemize}

These requirements influence memory architecture. The system should pin the primary jurisdiction in persistent memory, flag jurisdictional shifts as significant context changes, and retrieve jurisdiction-specific precedents when constructing prompts.

\paragraph{Temporal Sensitivity.} Legal rules change. A conversation about employment law may produce different answers depending on whether it occurred before or after a significant regulatory change. Conversational legal systems must:

\begin{itemize}
  \item Track the effective dates of legal rules referenced in responses
  \item Warn users when advice may be affected by recent or pending legal changes
  \item Maintain metadata about when information was last verified
  \item Consider whether to use retrieval to verify current legal status
\end{itemize}

\subsubsection{Financial Conversation Patterns}

Financial applications present their own distinct requirements:

\paragraph{Regulatory Boundaries.} Financial services are heavily regulated, and conversational AI must navigate complex compliance requirements:

\begin{itemize}
  \item Distinguishing between general information and personalized advice (which may trigger registration requirements)
  \item Ensuring disclosures are presented when required
  \item Maintaining records as required by securities, banking, or insurance regulations
  \item Avoiding forward-looking statements that could constitute market manipulation
\end{itemize}

\paragraph{Numerical Precision.} Financial conversations frequently involve numerical data where precision matters:

\begin{itemize}
  \item Currency amounts should be tracked with appropriate precision
  \item Percentages, ratios, and rates must be consistently formatted
  \item The system should clarify ambiguous numerical references (``Did you mean 3\% annually or monthly?'')
  \item Calculations should be verified, ideally through tool use rather than model inference
\end{itemize}

\paragraph{Client Risk Profiling.} Financial conversations often need to consider the client's risk profile, which should be:

\begin{itemize}
  \item Established early in the conversation or retrieved from persistent storage
  \item Updated when the client indicates changed circumstances
  \item Consulted before providing any investment-related information
  \item Protected as sensitive personal information
\end{itemize}

\subsubsection{Multi-Session Continuity}

Many professional applications span multiple conversation sessions. A client may return days or weeks later expecting the assistant to remember prior discussions. This requires:

\paragraph{Session Bridging.} When a user returns, the system must efficiently reconstruct relevant context:

\begin{itemize}
  \item Retrieve high-level summaries of prior sessions
  \item Identify topics that may be relevant to the new session (based on initial user message)
  \item Load pinned facts and persistent preferences
  \item Present a natural transition (``Welcome back. When we last spoke, we were discussing...'')
\end{itemize}

\paragraph{Context Staleness.} Prior conversation context may become stale:

\begin{itemize}
  \item Facts discussed months ago may no longer be accurate
  \item The user's situation may have changed
  \item Regulatory requirements may have evolved
  \item Market conditions discussed previously may be outdated
\end{itemize}

Systems should track context freshness and prompt users to confirm critical facts when resuming long-dormant conversations.

\paragraph{Archival and Retrieval.} Professional contexts often require conversation archival:

\begin{itemize}
  \item Legal hold requirements may prevent deletion of relevant conversations
  \item Compliance auditors may need to search historical conversations
  \item Users may need to reference prior discussions for their own records
  \item The system itself may need to retrieve historical context for current queries
\end{itemize}

These requirements influence storage architecture, retention policies, and search capabilities.

\subsection{Putting It Together: A Conversational Architecture}

To synthesize this section, consider the complete architecture of a production conversational system:

\begin{definitionbox}[title={Conversational System Components}]
\begin{enumerate}
  \item \textbf{Orchestration Layer}: Manages state, constructs prompts, handles tool calls
  \item \textbf{Memory Store}: Persists conversation history, embeddings, pinned facts
  \item \textbf{Retrieval System}: Searches memory and external knowledge for relevant context
  \item \textbf{Input Guardrails}: Classifies user input for policy compliance
  \item \textbf{LLM}: Generates responses given the constructed prompt
  \item \textbf{Output Guardrails}: Classifies model output for policy compliance
  \item \textbf{Audit System}: Logs all interactions for compliance and debugging
\end{enumerate}
\end{definitionbox}

Each component plays a role in maintaining the illusion of a coherent, stateful, and safe conversational agent built on a fundamentally stateless foundation. The choices you make at each layer---what to remember, where to place instructions, how to structure guardrails---determine the reliability and safety of your system.

With the mechanics of conversational state established, we now turn to the second major challenge: how to elicit structured reasoning from these systems.
