% ============================================================================
% GLOSSARY ENTRIES - LLM Essentials for Law and Finance
% ============================================================================
% This file defines key terms for the mini-book glossary.
% Terms are automatically tracked when referenced via \gls{key} or \glsadd{key}.
%
% Usage in text:
%   \gls{llm}             - lowercase with automatic pluralization
%   \Gls{llm}             - capitalized
%   \glspl{token}         - plural
%   \glsadd{embedding}    - add page reference without printing term
%
% In definitionbox environments, use glsadd to register the definition page:
%   \begin{definitionbox}[title={Token}]
%     \glsadd{token}
%     Definition text...
%   \end{definitionbox}
% ============================================================================

% ----------------------------------------------------------------------------
% CORE LLM CONCEPTS
% ----------------------------------------------------------------------------

\newglossaryentry{llm}{
  name={Large Language Model (LLM)},
  description={A neural network trained on massive text corpora to predict and generate text. Modern LLMs like GPT-4, Claude, and Gemini power most conversational AI applications. The ``large'' refers to billions of parameters that encode learned patterns from training data}
}

\newglossaryentry{token}{
  name={Token},
  description={The atomic unit of text processing in an LLM---typically a subword piece, punctuation mark, or character sequence. Tokenization affects pricing (models charge per token), context limits, and subtle behaviors like handling of numbers and proper nouns}
}

\newglossaryentry{context-window}{
  name={Context Window},
  description={The maximum number of tokens (input plus output) that a model can process in a single request. Larger windows enable longer documents but increase cost and latency. Current models range from 4K to 2M tokens}
}

\newglossaryentry{prompt}{
  name={Prompt},
  description={The input text provided to an LLM to guide its response. Prompt design---the craft of constructing effective prompts---significantly influences output quality, reliability, and compliance with requirements}
}

\newglossaryentry{completion}{
  name={Completion},
  description={The text generated by an LLM in response to a prompt. Also called the response or output. The model generates completions token by token based on probability distributions}
}

\newglossaryentry{hallucination}{
  name={Hallucination},
  description={Model-generated content that is factually incorrect, ungrounded, or fabricated. Includes invented citations, false claims, and plausible-sounding but wrong analysis. A primary risk in legal and financial applications}
}

% ----------------------------------------------------------------------------
% EMBEDDINGS AND RETRIEVAL
% ----------------------------------------------------------------------------

\newglossaryentry{embedding}{
  name={Embedding},
  description={A high-dimensional vector representation of text that captures semantic meaning. Similar concepts have vectors that are geometrically close. Embeddings enable semantic search, clustering, and retrieval-augmented generation}
}

\newglossaryentry{rag}{
  name={Retrieval-Augmented Generation (RAG)},
  description={An architecture that retrieves relevant documents before generation, grounding LLM responses in specific source material. Critical for legal and financial applications where accuracy and citation are required}
}

\newglossaryentry{vector-database}{
  name={Vector Database},
  description={A specialized database optimized for storing and querying embedding vectors. Enables efficient semantic search across large document collections. Examples include Pinecone, Weaviate, and Chroma}
}

\newglossaryentry{semantic-search}{
  name={Semantic Search},
  description={Search that matches based on meaning rather than exact keywords. Uses embeddings to find conceptually similar content even when different words are used. Complements traditional keyword search}
}

\newglossaryentry{hybrid-search}{
  name={Hybrid Search},
  description={A retrieval strategy combining semantic search (embeddings) with keyword search (BM25) to capture both conceptual similarity and exact term matches. Often outperforms either approach alone}
}

% ----------------------------------------------------------------------------
% SAMPLING AND GENERATION
% ----------------------------------------------------------------------------

\newglossaryentry{sampling}{
  name={Sampling},
  description={The process by which a model selects the next token from its predicted probability distribution. Sampling parameters (temperature, top-p) control the randomness versus determinism of outputs}
}

\newglossaryentry{temperature}{
  name={Temperature},
  description={A sampling parameter that controls output randomness. Lower values (0.0--0.3) produce more deterministic, focused outputs; higher values (0.7--1.0) increase creativity and variation. Critical for reproducible legal and financial outputs}
}

\newglossaryentry{top-p}{
  name={Top-p (Nucleus Sampling)},
  description={A sampling parameter that limits selection to tokens whose cumulative probability reaches a threshold (e.g., 0.95). Provides dynamic vocabulary restriction while maintaining diversity}
}

\newglossaryentry{max-tokens}{
  name={Max Tokens},
  description={A parameter limiting the length of generated output. Prevents runaway generation and controls costs. Must be set carefully---too low truncates responses; too high wastes budget}
}

% ----------------------------------------------------------------------------
% REASONING PATTERNS
% ----------------------------------------------------------------------------

\newglossaryentry{chain-of-thought}{
  name={Chain-of-Thought (CoT)},
  description={A prompting technique that instructs the model to show its reasoning step by step before providing a final answer. Improves accuracy on complex tasks by making intermediate reasoning explicit and auditable}
}

\newglossaryentry{few-shot-prompting}{
  name={Few-Shot Prompting},
  description={Providing examples of desired input-output pairs within the prompt to guide model behavior. The model learns the pattern from examples without any training. Typically uses 2--5 examples}
}

\newglossaryentry{zero-shot-prompting}{
  name={Zero-Shot Prompting},
  description={Prompting without examples, relying on instructions alone. Modern instruction-tuned models perform well zero-shot on many tasks. Simpler to implement but may be less reliable than few-shot approaches}
}

\newglossaryentry{react}{
  name={ReAct (Reasoning and Acting)},
  description={A prompting pattern that interleaves reasoning traces with actions (tool calls). The model thinks, acts, observes results, and continues reasoning. Foundation for many agent architectures}
}

\newglossaryentry{self-consistency}{
  name={Self-Consistency},
  description={A technique that samples multiple reasoning paths and selects the most common answer. Improves reliability by averaging out individual errors, at the cost of increased token usage}
}

% ----------------------------------------------------------------------------
% STRUCTURED OUTPUTS AND TOOLS
% ----------------------------------------------------------------------------

\newglossaryentry{structured-output}{
  name={Structured Output},
  description={Model output formatted according to a defined schema (JSON, XML, CSV). Enables reliable parsing and integration with downstream systems. Critical for automated workflows in regulated environments}
}

\newglossaryentry{function-calling}{
  name={Function Calling},
  description={A capability where the model generates structured calls to predefined functions rather than free text. Enables reliable tool integration with typed parameters and return values}
}

\newglossaryentry{tool-use}{
  name={Tool Use},
  description={The ability of an LLM to invoke external tools (APIs, databases, calculators) during generation. Extends model capabilities beyond pure text generation to actions in the real world}
}

\newglossaryentry{constrained-decoding}{
  name={Constrained Decoding},
  description={A generation technique that restricts token selection to those valid under a grammar or schema. Guarantees syntactically valid output (e.g., well-formed JSON) at the cost of some expressiveness}
}

% ----------------------------------------------------------------------------
% EVALUATION AND QUALITY
% ----------------------------------------------------------------------------

\newglossaryentry{evaluation-harness}{
  name={Evaluation Harness},
  description={A systematic framework for testing prompt performance across multiple dimensions (accuracy, latency, cost, safety). Enables objective comparison and regression detection during prompt iteration}
}

\newglossaryentry{gold-standard-test-set}{
  name={Gold Standard Test Set},
  description={A curated collection of inputs with verified correct outputs, used to evaluate and compare prompt performance. Essential for measuring accuracy in legal and financial applications}
}

\newglossaryentry{prompt-injection}{
  name={Prompt Injection},
  description={An attack where malicious content in user input attempts to override or modify the system prompt's instructions. A significant security concern when processing untrusted documents}
}

\newglossaryentry{indirect-prompt-injection}{
  name={Indirect Prompt Injection},
  description={Prompt injection that occurs through retrieved content rather than direct user input. Particularly relevant for RAG systems processing external documents that may contain adversarial content}
}

% ----------------------------------------------------------------------------
% MULTIMODAL PROCESSING
% ----------------------------------------------------------------------------

\newglossaryentry{multimodal}{
  name={Multimodal},
  description={Relating to models or systems that process multiple input types (text, images, audio, video). Modern LLMs increasingly support direct processing of documents, charts, and recordings}
}

\newglossaryentry{ocr}{
  name={Optical Character Recognition (OCR)},
  description={The process of extracting text from images of documents. Traditional OCR extracts raw text; modern layout-aware approaches preserve document structure for better LLM processing}
}

\newglossaryentry{layout-analysis}{
  name={Layout Analysis},
  description={Detecting and preserving document structure (tables, sections, headers) during processing. Critical for financial statements, contracts, and regulatory filings where layout carries meaning}
}

\newglossaryentry{speaker-diarization}{
  name={Speaker Diarization},
  description={Identifying and labeling different speakers in audio recordings. Essential for processing depositions, earnings calls, and meetings where speaker identity matters}
}

% ----------------------------------------------------------------------------
% TRAINING AND MODELS
% ----------------------------------------------------------------------------

\newglossaryentry{transformer}{
  name={Transformer},
  description={The neural network architecture underlying modern LLMs, introduced in 2017. Uses self-attention to process sequences in parallel, enabling efficient training on massive text corpora}
}

\newglossaryentry{fine-tuning}{
  name={Fine-Tuning},
  description={Additional training of a pre-trained model on task-specific data. Adapts general capabilities to specific domains or formats. Requires significant data and compute resources}
}

\newglossaryentry{instruction-tuning}{
  name={Instruction Tuning},
  description={Training a model to follow explicit instructions rather than just completing text. Makes models more useful for practical applications. Most modern LLMs are instruction-tuned}
}

\newglossaryentry{rlhf}{
  name={Reinforcement Learning from Human Feedback (RLHF)},
  description={A training technique that uses human preferences to align model behavior with user intent. Responsible for much of modern LLMs' helpfulness and safety properties}
}

\newglossaryentry{knowledge-cutoff}{
  name={Knowledge Cutoff},
  description={The date after which a model has no training data. Events, regulations, and developments after this date are unknown to the model. Critical for legal and financial applications involving recent changes}
}

% ----------------------------------------------------------------------------
% GOVERNANCE AND COMPLIANCE
% ----------------------------------------------------------------------------

\newglossaryentry{guardrails}{
  name={Guardrails},
  description={Controls that constrain model behavior to acceptable bounds. Include input validation, output filtering, and runtime monitoring. Essential for deploying LLMs in regulated environments}
}

\newglossaryentry{audit-trail}{
  name={Audit Trail},
  description={A complete record of inputs, outputs, and decision processes for LLM interactions. Required for compliance in legal and financial contexts. The Evidence Record pattern formalizes this concept}
}

\newglossaryentry{evidence-record}{
  name={Evidence Record},
  description={A structured, tamper-evident log capturing all aspects of an LLM interaction: inputs, outputs, model version, timestamps, and provenance. Supports regulatory compliance and dispute resolution}
}

\newglossaryentry{human-in-the-loop}{
  name={Human-in-the-Loop},
  description={A workflow pattern where humans review or approve LLM outputs before action. Provides a safety net for high-stakes decisions. Common in legal review and financial compliance}
}

% ----------------------------------------------------------------------------
% OPERATIONAL TERMS
% ----------------------------------------------------------------------------

\newglossaryentry{latency}{
  name={Latency},
  description={The time delay between sending a request and receiving a response. Includes network time, queue time, and generation time. Critical for user experience and system design}
}

\newglossaryentry{ttft}{
  name={Time-to-First-Token (TTFT)},
  description={The time from request submission until the first token of the response appears. Important for perceived responsiveness in streaming applications}
}

\newglossaryentry{rate-limiting}{
  name={Rate Limiting},
  description={Controls on request frequency to prevent system overload or abuse. APIs impose rate limits; applications must implement retry logic and backoff strategies}
}

\newglossaryentry{model-card}{
  name={Model Card},
  description={A standardized document describing a model's capabilities, limitations, training data, and intended uses. Supports informed deployment decisions and risk assessment}
}
